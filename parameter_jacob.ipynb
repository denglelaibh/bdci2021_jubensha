{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5ca8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv('data/train_dataset_v2.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "716b6033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0,0,0,0,0,0\n",
      "1        0,0,0,0,0,0\n",
      "2        0,0,0,0,0,0\n",
      "3        0,0,0,0,0,0\n",
      "4        0,0,0,0,0,0\n",
      "            ...     \n",
      "42785    0,0,0,0,0,0\n",
      "42786    0,3,0,0,0,0\n",
      "42787    2,3,0,0,0,0\n",
      "42788    2,3,0,0,0,0\n",
      "42789    0,0,0,0,0,0\n",
      "Name: emotions, Length: 42790, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train['emotions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1122e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = {}\n",
    "for index in range(len(train['content'])):\n",
    "    currentindex = train['id'][index]\n",
    "    currentindex = currentindex.split('_')\n",
    "    str1 = currentindex[0]\n",
    "    while len(str1) < 5:\n",
    "        str1 = '0'+str1\n",
    "    str4 = currentindex[3]\n",
    "    while len(str4) < 4:\n",
    "        str4 = '0'+str4\n",
    "    resultindex = str1+'_'+currentindex[1]+'_'+currentindex[2]+'_'+str4\n",
    "    content_dict[resultindex] = train['content'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3141791a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = \n",
      "['01171_0001_A_0001', '01171_0001_A_0002', '01171_0001_A_0003', '01171_0001_A_0004', '01171_0001_A_0005', '01171_0001_A_0006', '01171_0001_A_0007', '01171_0001_A_0008', '01171_0001_A_0009', '01171_0001_A_0010']\n"
     ]
    }
   ],
   "source": [
    "def sortedDictValues(adict): \n",
    "    keys = adict.keys() \n",
    "    keys = sorted(keys)\n",
    "    print('keys = ')\n",
    "    print(keys[0:10])\n",
    "    new_content_dict = {}\n",
    "    for data in keys:\n",
    "        new_content_dict[data] = content_dict[data]\n",
    "    return new_content_dict \n",
    "new_content_dict = sortedDictValues(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a326a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_new_content_dict = {}\n",
    "for data in new_content_dict:\n",
    "    inv_new_content_dict[new_content_dict[data]] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "137f25ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = new_content_dict.keys()\n",
    "keys = list(keys)\n",
    "dict_keys_index = {}\n",
    "for index in range(len(keys)):\n",
    "    #dict_keys_index[index] = keys[index]\n",
    "    dict_keys_index[keys[index]] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc511fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "content,emotions = [],[]\n",
    "dicts = {}\n",
    "inv_dicts = {}\n",
    "label_num = 0\n",
    "characters = []\n",
    "label1,label2,label3,label4,label5,label6 = [],[],[],[],[],[]\n",
    "for index in range(len(train['content'])):\n",
    "    if pd.isna(train['emotions'][index]) == False:\n",
    "        content.append(train['content'][index])\n",
    "        emotions.append(train['emotions'][index])\n",
    "        characters.append(train['character'][index])\n",
    "        current_emotion = train['emotions'][index].split(',')\n",
    "        label1.append(int(current_emotion[0]))\n",
    "        label2.append(int(current_emotion[1]))\n",
    "        label3.append(int(current_emotion[2]))\n",
    "        label4.append(int(current_emotion[3]))\n",
    "        label5.append(int(current_emotion[4]))\n",
    "        label6.append(int(current_emotion[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5428e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = set(label1)\n",
    "dict1,dict2,dict3,dict4,dict5,dict6 = {},{},{},{},{},{}\n",
    "for item in set1:\n",
    "    dict1.update({item:label1.count(item)})\n",
    "set2 = set(label2)\n",
    "for item in set2:\n",
    "    dict2.update({item:label2.count(item)})\n",
    "set3 = set(label3)\n",
    "for item in set3:\n",
    "    dict3.update({item:label3.count(item)})\n",
    "set4 = set(label4)\n",
    "for item in set4:\n",
    "    dict4.update({item:label4.count(item)})\n",
    "set5 = set(label5)\n",
    "for item in set5:\n",
    "    dict5.update({item:label5.count(item)})\n",
    "set6 = set(label6)\n",
    "for item in set6:\n",
    "    dict6.update({item:label6.count(item)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16734219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---__init__ Nezha\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from bertmodels import Config\n",
    "from nezha import Bert\n",
    "base_dir = r'nezha-base'\n",
    "\n",
    "vocab_file = base_dir + r'/vocab.txt'\n",
    "vocab_size = len(open(vocab_file,'r').readlines()) \n",
    "with open(base_dir+ '/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "json_data['vocab_size'] = vocab_size\n",
    "config = Config(**json_data)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tokenization import FullTokenizer\n",
    "import numpy as np\n",
    "tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from tqdm import tqdm\n",
    "config.with_mlm = False\n",
    "#config.with_pooler = True\n",
    "bertmodel = Bert(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df8c3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!load Pytorch model!!!\n",
      "Done loading 196 NEZHA weights from: nezha-base/pytorch_model.bin. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert.pooler.dense.weight\n",
      "\tcls.predictions.decoder.weight\n",
      "\tcls.predictions.transform.dense.weight\n",
      "\tbert.pooler.dense.bias\n",
      "\tcls.predictions.bias\n",
      "\tcls.predictions.transform.LayerNorm.bias\n",
      "\tcls.predictions.transform.LayerNorm.weight\n",
      "\tcls.seq_relationship.bias\n",
      "\tcls.predictions.transform.dense.bias\n",
      "\tcls.seq_relationship.weight\n"
     ]
    }
   ],
   "source": [
    "from loader_nezha import load_bert_data\n",
    "bertmodel = load_bert_data(bertmodel,base_dir + '/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8dd4bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text,characters,label1,label2,label3,label4,label5,label6,maxlen,flag):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        self.characters = characters\n",
    "        #self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            while current_key_index != -1 and new_content_dict[keys[current_key_index]] == current_text:\n",
    "                current_key_index = current_key_index-1\n",
    "            #找寻前面语句的过程之中去重\n",
    "            if current_key_index == -1:\n",
    "                pre_content = '无'\n",
    "            else:\n",
    "                pre_content = new_content_dict[keys[current_key_index]]\n",
    "            #找到前面去重之后的语句内容\n",
    "            new_str_index = keys[current_key_index]\n",
    "            current_str_index = current_str_index.split('_')\n",
    "            new_str_index = new_str_index.split('_')\n",
    "            if current_str_index[0] != new_str_index[0]:\n",
    "                pre_content = '无'\n",
    "            if current_str_index[1] != new_str_index[1]:\n",
    "                pre_content = '无'\n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            current_character = tokenizer.tokenize(current_character)\n",
    "            \n",
    "            #不属于同一个剧本杀的剧本\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]+ current_character +[\"[SEP]\"]\n",
    "#             if str(current_character) != 'nan' and pre_content != '':\n",
    "#                 pre_token = tokenizer.tokenize(pre_content)\n",
    "#                 current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]+[current_character]+[\"[SEP]\"]\n",
    "#             elif str(current_character) != 'nan' and pre_content == '':\n",
    "#                 current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+[current_character]+[\"[SEP]\"]\n",
    "#             elif str(current_character) == 'nan' and pre_content != '':\n",
    "#                 pre_token = tokenizer.tokenize(pre_content)\n",
    "#                 current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "#             elif str(current_character) == 'nan' and pre_content == '':\n",
    "#                 current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]\n",
    "                \n",
    "            r\"\"\"\n",
    "            if pre_content == '':\n",
    "                pre_content = '无上文'\n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无角色'\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            #current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]+[current_character]+[\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+pre_token+[\"[SEP]\"]+[current_character]+[\"[SEP]\"]\n",
    "            \"\"\"\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        self.tensors = [torch.tensor(self.token_id,dtype=torch.long),\n",
    "                 torch.tensor(self.label[0],dtype=torch.long),\n",
    "                 torch.tensor(self.label[1],dtype=torch.long),\n",
    "                 torch.tensor(self.label[2],dtype=torch.long),\n",
    "                 torch.tensor(self.label[3],dtype=torch.long),\n",
    "                 torch.tensor(self.label[4],dtype=torch.long),\n",
    "                 torch.tensor(self.label[5],dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "\n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length]\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7397662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29426/29426 [00:09<00:00, 3238.18it/s]\n",
      "100%|██████████| 7356/7356 [00:02<00:00, 3196.17it/s]\n"
     ]
    }
   ],
   "source": [
    "text = content\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "ss = StratifiedKFold(n_splits=5,shuffle=True,random_state=2)\n",
    "#建立4折交叉验证方法 查一下KFold函数的参数\n",
    "text = np.array(text)\n",
    "label1 = np.array(label1)\n",
    "label2 = np.array(label2)\n",
    "label3 = np.array(label3)\n",
    "label4 = np.array(label4)\n",
    "label5 = np.array(label5)\n",
    "label6 = np.array(label6)\n",
    "characters = np.array(characters)\n",
    "for train_index,test_index in ss.split(text,label1):\n",
    "    train_text = text[np.array(train_index)]\n",
    "    test_text = text[test_index]\n",
    "    train_characters = characters[train_index]\n",
    "    test_characters = characters[test_index]\n",
    "    train_label1 = label1[train_index]\n",
    "    test_label1 = label1[test_index]\n",
    "    train_label2 = label2[train_index]\n",
    "    test_label2 = label2[test_index]\n",
    "    train_label3 = label3[train_index]\n",
    "    test_label3 = label3[test_index]\n",
    "    train_label4 = label4[train_index]\n",
    "    test_label4 = label4[test_index]\n",
    "    train_label5 = label5[train_index]\n",
    "    test_label5 = label5[test_index]\n",
    "    train_label6 = label6[train_index]\n",
    "    test_label6 = label6[test_index]\n",
    "train_dataset = ClassificationDataset(train_text,train_characters,train_label1,train_label2,train_label3,train_label4,train_label5,train_label6,maxlen=350,flag=True)\n",
    "test_dataset = ClassificationDataset(test_text,test_characters,test_label1,test_label2,test_label3,test_label4,test_label5,test_label6,maxlen=350,flag=False)\n",
    "#到里面的classificationdataset才进行字符的切割以及划分\n",
    "train_loader = DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a19aa676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, model, config, n_classes):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        #self.embedding = nn.Embedding(30522,768)\n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(config.embedding_size,128)\n",
    "        self.activation = F.relu\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #self.activation = F.tanh\n",
    "        self.out = nn.Linear(128, 6)\n",
    "        self.sorrow = nn.Linear(6, 1)\n",
    "        \n",
    "    def forward(self,input_ids,segment_ids,input_mask):\n",
    "        #outputs = self.embedding(input_ids)\n",
    "        output = self.model(input_ids)\n",
    "        #[64,128,768]\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.fc1(output)\n",
    "        output = self.activation(output)\n",
    "        logits = self.dropout(output)\n",
    "        pooled_output = self.out(logits)\n",
    "        tmp_out = torch.transpose(pooled_output, 0, 1)\n",
    "        love = tmp_out[0]\n",
    "        joy = tmp_out[1]\n",
    "        fright = tmp_out[2]\n",
    "        anger = tmp_out[3]\n",
    "        fear = tmp_out[4]\n",
    "        sorrow = tmp_out[5]\n",
    "\n",
    "#         sorrow = self.sorrow(torch.cat((love.unsqueeze(-1), joy.unsqueeze(-1), fright.unsqueeze(-1), anger.unsqueeze(-1),\n",
    "#                                         fear.unsqueeze(-1), sorrow.unsqueeze(-1)), dim=-1))\n",
    "        return {\n",
    "            'love': love, 'joy': joy, 'fright': fright,\n",
    "            'anger': anger, 'fear': fear, 'sorrow': sorrow,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8875a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model = ClassificationModel(bertmodel,config,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78de9dc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 5e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1840/1840 [11:26<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000005\n",
      "Train Loss: 373.943848\n",
      "love [0.13889859567624743] joy [0.12266608508747912] fright[0.13859603939123663] anger[0.23516315863298193] fear[0.21413416567129784] sorrow[0.36992442339668863]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460/460 [01:06<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.135675\n",
      "index = 1,current_loss = 0.130319\n",
      "index = 2,current_loss = 0.132856\n",
      "index = 3,current_loss = 0.214879\n",
      "index = 4,current_loss = 0.188604\n",
      "index = 5,current_loss = 0.339719\n",
      "totalloss = \n",
      "1.1420523524284363\n",
      "current_point = \n",
      "0.696242061088765\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1840/1840 [11:28<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000003\n",
      "Train Loss: 310.135345\n",
      "love [0.12491315075239981] joy [0.10305933437984609] fright[0.11894654501123549] anger[0.1829495683766689] fear[0.19063080303277946] sorrow[0.2908111947464615]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460/460 [01:07<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.133989\n",
      "index = 1,current_loss = 0.120819\n",
      "index = 2,current_loss = 0.122795\n",
      "index = 3,current_loss = 0.184906\n",
      "index = 4,current_loss = 0.179621\n",
      "index = 5,current_loss = 0.367014\n",
      "totalloss = \n",
      "1.1091422885656357\n",
      "current_point = \n",
      "0.6993251252979845\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1840/1840 [11:29<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000003\n",
      "Train Loss: 268.003235\n",
      "love [0.11108653076403045] joy [0.09162304146479786] fright[0.10800269181155549] anger[0.14944265286932298] fear[0.1714246566172021] sorrow[0.24234504702187185]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460/460 [01:07<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.130745\n",
      "index = 1,current_loss = 0.118485\n",
      "index = 2,current_loss = 0.120656\n",
      "index = 3,current_loss = 0.192175\n",
      "index = 4,current_loss = 0.176814\n",
      "index = 5,current_loss = 0.314319\n",
      "totalloss = \n",
      "1.0531932935118675\n",
      "current_point = \n",
      "0.704738694871001\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1840/1840 [11:30<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000002\n",
      "Train Loss: 238.423233\n",
      "love [0.10067160031071778] joy [0.08405863974722982] fright[0.09989094531477388] anger[0.130707657312577] fear[0.15434200903018] sorrow[0.20779572621888845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1840/1840 [11:32<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第4个epoch的学习率：0.000002\n",
      "Train Loss: 219.782715\n",
      "love [0.0950666013864975] joy [0.07848784204306077] fright[0.0928781414622632] anger[0.11739948739464719] fear[0.1429339919125398] sorrow[0.18991721007699633]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460/460 [01:08<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.129815\n",
      "index = 1,current_loss = 0.120264\n",
      "index = 2,current_loss = 0.122694\n",
      "index = 3,current_loss = 0.185075\n",
      "index = 4,current_loss = 0.175544\n",
      "index = 5,current_loss = 0.325849\n",
      "totalloss = \n",
      "1.0592405647039413\n",
      "current_point = \n",
      "0.7041426671132069\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1840/1840 [11:33<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第5个epoch的学习率：0.000005\n",
      "Train Loss: 203.138031\n",
      "love [0.08502204840197405] joy [0.07416027937943] fright[0.08911099212858498] anger[0.10815696045231206] fear[0.13297365983592538] sorrow[0.17298269465606173]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460/460 [01:08<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.131805\n",
      "index = 1,current_loss = 0.119775\n",
      "index = 2,current_loss = 0.126433\n",
      "index = 3,current_loss = 0.188586\n",
      "index = 4,current_loss = 0.181214\n",
      "index = 5,current_loss = 0.331785\n",
      "totalloss = \n",
      "1.0795970112085342\n",
      "current_point = \n",
      "0.7021560158294828\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1493/1840 [09:24<02:10,  2.65it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1840/1840 [11:34<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第6个epoch的学习率：0.000005\n",
      "Train Loss: 204.005371\n",
      "love [0.08367581428678735] joy [0.07459838000074302] fright[0.08912716014878559] anger[0.10893094860170412] fear[0.13090301773412175] sorrow[0.17799984923710682]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460/460 [01:08<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.126461\n",
      "index = 1,current_loss = 0.120272\n",
      "index = 2,current_loss = 0.126757\n",
      "index = 3,current_loss = 0.194248\n",
      "index = 4,current_loss = 0.186428\n",
      "index = 5,current_loss = 0.328298\n",
      "totalloss = \n",
      "1.0824644938111305\n",
      "current_point = \n",
      "0.7018785740584265\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1840/1840 [11:36<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第7个epoch的学习率：0.000005\n",
      "Train Loss: 185.842468\n",
      "love [0.07502996432740475] joy [0.06910596572978767] fright[0.08267994023054084] anger[0.0988749516689416] fear[0.11818932908348] sorrow[0.16212733584461209]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460/460 [01:09<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.134968\n",
      "index = 1,current_loss = 0.119331\n",
      "index = 2,current_loss = 0.129308\n",
      "index = 3,current_loss = 0.181074\n",
      "index = 4,current_loss = 0.213539\n",
      "index = 5,current_loss = 0.371446\n",
      "totalloss = \n",
      "1.149665616452694\n",
      "current_point = \n",
      "0.6955390178495878\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1840/1840 [11:38<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第8个epoch的学习率：0.000005\n",
      "Train Loss: 167.730270\n",
      "love [0.06379092007036492] joy [0.06563506410612203] fright[0.07469871707051168] anger[0.09104915521545437] fear[0.10547446977908966] sorrow[0.14629840620530202]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460/460 [01:09<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.133456\n",
      "index = 1,current_loss = 0.116998\n",
      "index = 2,current_loss = 0.129051\n",
      "index = 3,current_loss = 0.193720\n",
      "index = 4,current_loss = 0.199007\n",
      "index = 5,current_loss = 0.351470\n",
      "totalloss = \n",
      "1.12370203435421\n",
      "current_point = \n",
      "0.6979522184250958\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1840/1840 [11:40<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第9个epoch的学习率：0.000005\n",
      "Train Loss: 153.911484\n",
      "love [0.05923651377131097] joy [0.06193704367460273] fright[0.06883532854367133] anger[0.08352352738451294] fear[0.09482368895098815] sorrow[0.133529397457175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460/460 [01:10<00:00,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.128778\n",
      "index = 1,current_loss = 0.112035\n",
      "index = 2,current_loss = 0.128064\n",
      "index = 3,current_loss = 0.181980\n",
      "index = 4,current_loss = 0.175611\n",
      "index = 5,current_loss = 0.346924\n",
      "totalloss = \n",
      "1.073391169309616\n",
      "current_point = \n",
      "0.702758477507092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "#不动态调整学习率的时候，lr=2e-5的时候最好值0.420293\n",
    "def lr_lambda(epoch):\n",
    "    if epoch > 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2/(epoch+1)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "bestpoint = 0.0\n",
    "for epoch in range(10):\n",
    "    print('epoch {}'.format(epoch))\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True,size_average=True)\n",
    "\n",
    "    losses_love = []\n",
    "    losses_joy = []\n",
    "    losses_fright = []\n",
    "    losses_anger = []\n",
    "    losses_fear = []\n",
    "    losses_sorrow = []\n",
    "\n",
    "    for batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(train_loader):\n",
    "        torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "        #print('batch_token_ids')\n",
    "        #print(batch_token_ids)\n",
    "        batch_token_ids = batch_token_ids.to(device)\n",
    "        batch_label1 = torch.tensor([batch_label1.numpy()], dtype=torch.float)\n",
    "        batch_label2 = torch.tensor([batch_label2.numpy()], dtype=torch.float)\n",
    "        batch_label3 = torch.tensor([batch_label3.numpy()], dtype=torch.float)\n",
    "        batch_label4 = torch.tensor([batch_label4.numpy()], dtype=torch.float)\n",
    "        batch_label5 = torch.tensor([batch_label5.numpy()], dtype=torch.float)\n",
    "        batch_label6 = torch.tensor([batch_label6.numpy()], dtype=torch.float)\n",
    "        logits = model(batch_token_ids,None,None)\n",
    "        optimizer.zero_grad()\n",
    "        pred1 = logits['love'].unsqueeze(-1)\n",
    "        pred2 = logits['joy'].unsqueeze(-1)\n",
    "        pred3 = logits['fright'].unsqueeze(-1)\n",
    "        pred4 = logits['anger'].unsqueeze(-1)\n",
    "        pred5 = logits['fear'].unsqueeze(-1)\n",
    "        pred6 = logits['sorrow'].unsqueeze(-1)\n",
    "\n",
    "\n",
    "        loss_love = loss_fn(torch.transpose(pred1, 0, 1), batch_label1.to(device))\n",
    "        loss_joy = loss_fn(torch.transpose(pred2, 0, 1), batch_label2.to(device))\n",
    "        loss_fright = loss_fn(torch.transpose(pred3, 0, 1), batch_label3.to(device))\n",
    "        loss_anger = loss_fn(torch.transpose(pred4, 0, 1), batch_label4.to(device))\n",
    "        loss_fear = loss_fn(torch.transpose(pred5, 0, 1), batch_label5.to(device))\n",
    "        loss_sorrow = loss_fn(torch.transpose(pred6, 0, 1), batch_label6.to(device))\n",
    "\n",
    "        #for index in range(len(batch_labels)):\n",
    "        #    batch_labels[index] = batch_labels[index].to(device)\n",
    "        loss = loss_love + loss_joy + loss_fright + loss_anger + loss_fear + loss_sorrow\n",
    "        losses_love.append(loss_love.item())\n",
    "        losses_joy.append(loss_joy.item())\n",
    "        losses_fright.append(loss_fright.item())\n",
    "        losses_anger.append(loss_anger.item())\n",
    "        losses_fear.append(loss_fear.item())\n",
    "        losses_sorrow.append(loss_sorrow.item())\n",
    "        train_loss = train_loss + loss / 6.0\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "    #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "    print('Train Loss: {:.6f}'.format(train_loss))\n",
    "    print('love [%s] joy [%s] fright[%s] anger[%s] fear[%s] sorrow[%s]' % (np.mean(losses_love), np.mean(losses_joy), np.mean(losses_fright), np.mean(losses_anger),\n",
    "                                                                          np.mean(losses_fear), np.mean(losses_sorrow)))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    eval_true_label = [[],[],[],[],[],[]]\n",
    "    eval_predict_label = [[],[],[],[],[],[]]\n",
    "    \n",
    "    #for batch_token_ids,batch_labels in tqdm(test_loader,bar_format='{l_bar}%s{bar}%s{r_bar}' % (Fore.BLUE, Fore.RESET)):\n",
    "    for batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(test_loader):\n",
    "        batch_token_ids = batch_token_ids.to(device)\n",
    "        batch_labels = [batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6]\n",
    "        with torch.no_grad():\n",
    "            output = model(batch_token_ids,None,None)\n",
    "        lst = ['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']\n",
    "        for index in lst:\n",
    "            for index1 in range(len(output[index])):\n",
    "                \n",
    "                abs0 = abs(output[index][index1]-0)\n",
    "                abs1 = abs(output[index][index1]-1)\n",
    "                abs2 = abs(output[index][index1]-2)\n",
    "                abs3 = abs(output[index][index1]-3)\n",
    "                currentdata = [abs0,abs1,abs2,abs3]\n",
    "                current_label = currentdata.index(min(currentdata))\n",
    "                #eval_predict_label[index1].append(current_label)\n",
    "                current_predict = output[index][index1].item()\n",
    "                #当前类别的分类结果,这里append(output[index][index1])\n",
    "                #直接放入对应概率值时效果最好,这里直接放入0,1,2,3对应的数值\n",
    "                eval_predict_label[lst.index(index)].append(current_predict)\n",
    "        for index in range(6):\n",
    "            if index == 0:\n",
    "                current_label = np.array(batch_label1.cpu()).tolist()\n",
    "            if index == 1:\n",
    "                current_label = np.array(batch_label2.cpu()).tolist()\n",
    "            if index == 2:\n",
    "                current_label = np.array(batch_label3.cpu()).tolist()\n",
    "            if index == 3:\n",
    "                current_label = np.array(batch_label4.cpu()).tolist()\n",
    "            if index == 4:\n",
    "                current_label = np.array(batch_label5.cpu()).tolist()\n",
    "            if index == 5:\n",
    "                current_label = np.array(batch_label6.cpu()).tolist()\n",
    "            eval_true_label[index].extend(current_label)\n",
    "    criterion = nn.MSELoss()\n",
    "    totalloss = 0\n",
    "\n",
    "    for index in range(len(eval_true_label)):\n",
    "#         if index == 5:\n",
    "#             for j in range(len(eval_predict_label[index])):\n",
    "#                 eval_predict_label[index][j] *= 0.67\n",
    "        inputs = torch.autograd.Variable(torch.from_numpy(np.array(eval_predict_label[index])))\n",
    "        target = torch.autograd.Variable(torch.from_numpy(np.array(eval_true_label[index])))\n",
    "        current_loss = criterion(inputs.float(),target.float())\n",
    "        current_loss = current_loss.item()\n",
    "        print('index = %d,current_loss = %f'% (index,current_loss))\n",
    "        totalloss = totalloss + current_loss\n",
    "    \n",
    "    #totalloss = totalloss/len(eval_predict_label)\n",
    "    print('totalloss = ')\n",
    "    print(totalloss)\n",
    "    totalloss = totalloss/6\n",
    "    totalloss = math.sqrt(totalloss)\n",
    "    currentpoint = 1/(1+totalloss)\n",
    "    #currentpoint = 1/(1+current_loss)\n",
    "    print('current_point = ')\n",
    "    print(currentpoint)\n",
    "    if currentpoint > bestpoint:\n",
    "        bestpoint = currentpoint\n",
    "        torch.save(model,'best_score'+str(bestpoint)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ebb3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model = torch.load('best_score'+str(bestpoint)+'.pth')\n",
    "test=pd.read_csv('data/test_dataset.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd28149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('best_score0.7111817192757776.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd38a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = {}\n",
    "for index in range(len(test['content'])):\n",
    "    currentindex = test['id'][index]\n",
    "    currentindex = currentindex.split('_')\n",
    "    str1 = currentindex[0]\n",
    "    while len(str1) < 5:\n",
    "        str1 = '0'+str1\n",
    "    str4 = currentindex[3]\n",
    "    while len(str4) < 4:\n",
    "        str4 = '0'+str4\n",
    "    resultindex = str1+'_'+currentindex[1]+'_'+currentindex[2]+'_'+str4\n",
    "    content_dict[resultindex] = test['content'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0773ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = \n",
      "['01597_0001_A_0001', '01597_0001_A_0002', '01597_0001_A_0003', '01597_0001_A_0004', '01597_0001_A_0005', '01597_0001_A_0006', '01597_0001_A_0007', '01597_0001_A_0008', '01597_0001_A_0009', '01597_0001_A_0010']\n"
     ]
    }
   ],
   "source": [
    "def sortedDictValues(adict): \n",
    "    keys = adict.keys() \n",
    "    keys = sorted(keys)\n",
    "    print('keys = ')\n",
    "    print(keys[0:10])\n",
    "    new_content_dict = {}\n",
    "    for data in keys:\n",
    "        new_content_dict[data] = content_dict[data]\n",
    "    return new_content_dict \n",
    "new_content_dict = sortedDictValues(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c44e0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_new_content_dict = {}\n",
    "for data in new_content_dict:\n",
    "    inv_new_content_dict[new_content_dict[data]] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "465f5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = new_content_dict.keys()\n",
    "keys = list(keys)\n",
    "dict_keys_index = {}\n",
    "for index in range(len(keys)):\n",
    "    #dict_keys_index[index] = keys[index]\n",
    "    dict_keys_index[keys[index]] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9c07940",
   "metadata": {},
   "outputs": [],
   "source": [
    "content,emotions = [],[]\n",
    "dicts = {}\n",
    "inv_dicts = {}\n",
    "label_num = 0\n",
    "characters = []\n",
    "for index in range(len(test['content'])):\n",
    "        content.append(test['content'][index])\n",
    "        characters.append(test['character'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d55b1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext = test['content']\n",
    "testid = test['id']\n",
    "testcharacter = test['character']\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,character,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            while current_key_index != -1 and new_content_dict[keys[current_key_index]] == current_text:\n",
    "                current_key_index = current_key_index-1\n",
    "            #找寻前面语句的过程之中去重\n",
    "            if current_key_index == -1:\n",
    "                pre_content = ''\n",
    "            else:\n",
    "                pre_content = new_content_dict[keys[current_key_index]]\n",
    "            \n",
    "            new_str_index = keys[current_key_index]\n",
    "            current_str_index = current_str_index.split('_')\n",
    "            new_str_index = new_str_index.split('_')\n",
    "            if current_str_index[0] != new_str_index[0]:\n",
    "                pre_content = '无'\n",
    "            if current_str_index[1] != new_str_index[1]:\n",
    "                pre_content = '无'\n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            pre_token = tokenizer.tokenize(current_character)\n",
    "            #不属于同一个剧本杀的剧本\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]+[current_character]+[\"[SEP]\"]\n",
    "            \n",
    "#             if str(current_character) != 'nan' and pre_content != '':\n",
    "#                 pre_token = tokenizer.tokenize(pre_content)\n",
    "#                 current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]+[current_character]+[\"[SEP]\"]\n",
    "#             elif str(current_character) != 'nan' and pre_content == '':\n",
    "#                 current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+[current_character]+[\"[SEP]\"]\n",
    "#             elif str(current_character) == 'nan' and pre_content != '':\n",
    "#                 pre_token = tokenizer.tokenize(pre_content)\n",
    "#                 current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "#             elif str(current_character) == 'nan' and pre_content == '':\n",
    "#                 current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]\n",
    "            \n",
    "            r\"\"\"\n",
    "            if pre_content == '':\n",
    "                pre_content = '无上文'\n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无角色'\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            #current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]+[current_character]+[\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+pre_token+[\"[SEP]\"]+[current_character]+[\"[SEP]\"]\n",
    "            \"\"\"\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        #self.segment_id = sequence_padding(self.segment_id,maxlen)\n",
    "        #self.mask_id = sequence_padding(self.mask_id,maxlen)\n",
    "        self.tensors = [torch.tensor(self.token_id,dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length]\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26f63c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21376/21376 [00:06<00:00, 3182.85it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(testtext,testcharacter,maxlen=200)\n",
    "test_loader = DataLoader(test_dataset,batch_size=10,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60092d83",
   "metadata": {},
   "source": [
    "## 修改测试集的评分标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b81b7a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2138/2138 [01:53<00:00, 18.80it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "eval_loss = 0.\n",
    "eval_acc = 0.\n",
    "eval_predict_label = []\n",
    "index = []\n",
    "pred = [[],[],[],[],[],[]]\n",
    "current_index = 0\n",
    "for batch_token_ids in tqdm(test_loader):\n",
    "    batch_token_ids = batch_token_ids[0].to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(batch_token_ids,None,None)\n",
    "        lst = ['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']\n",
    "\n",
    "        for index in lst:\n",
    "            for index1 in range(len(output[index])):\n",
    "                #对应的类别概率0~6\n",
    "                abs0 = abs(output[index][index1]-0)\n",
    "                abs1 = abs(output[index][index1]-1)\n",
    "                abs2 = abs(output[index][index1]-2)\n",
    "                abs3 = abs(output[index][index1]-3)\n",
    "                currentdata = [abs0,abs1,abs2,abs3]\n",
    "                current_label = currentdata.index(min(currentdata))\n",
    "                #eval_predict_label[index1].append(current_label)\n",
    "                current_predict = output[index][index1].item()\n",
    "                pred[lst.index(index)].append(current_predict)\n",
    "                #eval_predict_label[index1].append(current_predict)\n",
    "                #当前类别的分类结果,这里append(output[index][index1])\n",
    "                #直接放入对应概率值时效果最好,这里直接放入0,1,2,3对应的数值\n",
    "for index in range(len(pred[0])):\n",
    "    for i in range(6):\n",
    "        if pred[i][index] <= 0.0001:\n",
    "            pred[i][index] = 0.0\n",
    "        if pred[i][index] >= 3.0:\n",
    "            pred[i][index] = 3.0\n",
    "#         if i == 5:\n",
    "#             pred[i][index] *= 0.78\n",
    "    eval_predict_label.append(str(pred[0][index])+','+str(pred[1][index])+','+str(pred[2][index])+','+str(pred[3][index])+','+str(pred[4][index])+','+str(pred[5][index]))\n",
    "result_data = []\n",
    "for index in range(len(testid)):\n",
    "    result_data.append([testid[index],eval_predict_label[index]])\n",
    "#pd.DataFrame({\"id\":testid,\"label\":eval_predict_label}).to_csv(\"/home/xiaoguzai/代码/剧本角色情感识别/数据集/crossentropy\"+str(bestpoint)+\"result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfedb1ca",
   "metadata": {},
   "source": [
    "a=np.array([[1,2],[3,4]])\n",
    "b=np.array([[2,3],[4,5]])\n",
    "loss_fn = torch.nn.MSELoss(reduce=False, size_average=False)\n",
    "input = torch.autograd.Variable(torch.from_numpy(a))\n",
    "target = torch.autograd.Variable(torch.from_numpy(b))\n",
    "loss = loss_fn(input.float(),target.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bf8ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(f'outputs/nezha_%.4f_result.csv' % bestpoint, 'w') as f:\n",
    "    tsv_w = csv.writer(f, delimiter='\\t')\n",
    "    tsv_w.writerow(['id', 'label'])  # 单行写入\n",
    "    tsv_w.writerows(result_data)  # 多行写入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5dbc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
