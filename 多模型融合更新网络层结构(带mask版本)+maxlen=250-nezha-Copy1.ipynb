{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e614aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = \n",
      "['01171_0001_A_0001', '01171_0001_A_0002', '01171_0001_A_0003', '01171_0001_A_0004', '01171_0001_A_0005', '01171_0001_A_0006', '01171_0001_A_0007', '01171_0001_A_0008', '01171_0001_A_0009', '01171_0001_A_0010']\n",
      "---__init__ Nezha\n",
      "test_str = \n",
      "['02721', '34173', '34940']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34632/34632 [00:11<00:00, 3005.58it/s]\n",
      "100%|█████████████████████████████████████| 2150/2150 [00:00<00:00, 3214.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoguzai/.local/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "100%|███████████████████████████████████████| 2165/2165 [05:57<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 392.749878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 135/135 [00:10<00:00, 13.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.049040\n",
      "index = 1,current_loss = 0.072006\n",
      "index = 2,current_loss = 0.139737\n",
      "index = 3,current_loss = 0.208176\n",
      "index = 4,current_loss = 0.632007\n",
      "index = 5,current_loss = 0.371120\n",
      "totalloss = \n",
      "1.472085490822792\n",
      "current_point = \n",
      "0.6687506125988172\n",
      "eval_label_loss = \n",
      "[[0, 180, 14, 0], [566, 0, 26, 2], [328, 80, 0, 1], [113, 70, 33, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2165/2165 [05:57<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 255.254501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 135/135 [00:10<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.041775\n",
      "index = 1,current_loss = 0.068353\n",
      "index = 2,current_loss = 0.133459\n",
      "index = 3,current_loss = 0.175889\n",
      "index = 4,current_loss = 0.670176\n",
      "index = 5,current_loss = 0.360336\n",
      "totalloss = \n",
      "1.44998749345541\n",
      "current_point = \n",
      "0.6704237571295506\n",
      "eval_label_loss = \n",
      "[[0, 163, 15, 0], [578, 0, 18, 3], [324, 80, 0, 4], [112, 60, 39, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2165/2165 [06:00<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 187.663406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 135/135 [00:10<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.049885\n",
      "index = 1,current_loss = 0.068109\n",
      "index = 2,current_loss = 0.139760\n",
      "index = 3,current_loss = 0.176431\n",
      "index = 4,current_loss = 0.597453\n",
      "index = 5,current_loss = 0.352828\n",
      "totalloss = \n",
      "1.3844658620655537\n",
      "current_point = \n",
      "0.6755120362102971\n",
      "eval_label_loss = \n",
      "[[0, 246, 27, 1], [552, 0, 26, 3], [293, 103, 0, 7], [85, 69, 44, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2165/2165 [06:03<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 150.050674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 135/135 [00:10<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.040706\n",
      "index = 1,current_loss = 0.066764\n",
      "index = 2,current_loss = 0.145839\n",
      "index = 3,current_loss = 0.175956\n",
      "index = 4,current_loss = 0.639554\n",
      "index = 5,current_loss = 0.374348\n",
      "totalloss = \n",
      "1.4431664645671844\n",
      "current_point = \n",
      "0.670944483746519\n",
      "eval_label_loss = \n",
      "[[0, 210, 31, 2], [562, 0, 26, 5], [303, 92, 0, 10], [96, 64, 34, 0]]\n",
      "test_str = \n",
      "['34911', '34899', '02369']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33595/33595 [00:10<00:00, 3078.12it/s]\n",
      "100%|█████████████████████████████████████| 3187/3187 [00:00<00:00, 3399.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2100/2100 [05:46<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 399.523346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [00:15<00:00, 13.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.080206\n",
      "index = 1,current_loss = 0.175903\n",
      "index = 2,current_loss = 0.081092\n",
      "index = 3,current_loss = 0.139568\n",
      "index = 4,current_loss = 0.047637\n",
      "index = 5,current_loss = 0.435321\n",
      "totalloss = \n",
      "0.9597278013825417\n",
      "current_point = \n",
      "0.7143146501852282\n",
      "eval_label_loss = \n",
      "[[0, 503, 46, 0], [1659, 0, 111, 0], [143, 56, 0, 0], [37, 12, 18, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2100/2100 [05:47<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 258.559937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [00:15<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.054573\n",
      "index = 1,current_loss = 0.150644\n",
      "index = 2,current_loss = 0.089947\n",
      "index = 3,current_loss = 0.137140\n",
      "index = 4,current_loss = 0.043775\n",
      "index = 5,current_loss = 0.400289\n",
      "totalloss = \n",
      "0.8763669654726982\n",
      "current_point = \n",
      "0.7234950372296107\n",
      "eval_label_loss = \n",
      "[[0, 447, 50, 1], [1593, 0, 114, 6], [119, 62, 0, 10], [28, 18, 13, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2100/2100 [05:50<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 188.190048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [00:15<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.053956\n",
      "index = 1,current_loss = 0.171759\n",
      "index = 2,current_loss = 0.110741\n",
      "index = 3,current_loss = 0.147933\n",
      "index = 4,current_loss = 0.068514\n",
      "index = 5,current_loss = 0.465287\n",
      "totalloss = \n",
      "1.0181912668049335\n",
      "current_point = \n",
      "0.7082429691727529\n",
      "eval_label_loss = \n",
      "[[0, 550, 114, 5], [1493, 0, 176, 31], [104, 72, 0, 19], [31, 15, 17, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2100/2100 [05:52<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 149.981857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [00:15<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.089030\n",
      "index = 1,current_loss = 0.171029\n",
      "index = 2,current_loss = 0.100189\n",
      "index = 3,current_loss = 0.145089\n",
      "index = 4,current_loss = 0.055225\n",
      "index = 5,current_loss = 0.441392\n",
      "totalloss = \n",
      "1.0019531100988388\n",
      "current_point = \n",
      "0.7099011755291598\n",
      "eval_label_loss = \n",
      "[[0, 521, 112, 9], [1497, 0, 170, 24], [99, 74, 0, 19], [28, 16, 14, 0]]\n",
      "test_str = \n",
      "['32505', '02388', '34945', '34126']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33948/33948 [00:11<00:00, 3040.04it/s]\n",
      "100%|█████████████████████████████████████| 2834/2834 [00:00<00:00, 3277.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2122/2122 [05:49<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 393.379333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 178/178 [00:13<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.242959\n",
      "index = 1,current_loss = 0.161748\n",
      "index = 2,current_loss = 0.126211\n",
      "index = 3,current_loss = 0.257013\n",
      "index = 4,current_loss = 0.178727\n",
      "index = 5,current_loss = 0.264448\n",
      "totalloss = \n",
      "1.2311058789491653\n",
      "current_point = \n",
      "0.688244152758128\n",
      "eval_label_loss = \n",
      "[[0, 536, 45, 0], [431, 0, 30, 0], [236, 111, 0, 0], [130, 102, 54, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2122/2122 [05:51<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 256.153503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 178/178 [00:13<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.262234\n",
      "index = 1,current_loss = 0.162116\n",
      "index = 2,current_loss = 0.124132\n",
      "index = 3,current_loss = 0.260123\n",
      "index = 4,current_loss = 0.187997\n",
      "index = 5,current_loss = 0.247012\n",
      "totalloss = \n",
      "1.243613950908184\n",
      "current_point = \n",
      "0.6871586331984979\n",
      "eval_label_loss = \n",
      "[[0, 478, 67, 0], [424, 0, 39, 0], [221, 110, 0, 2], [139, 89, 48, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2122/2122 [05:53<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 186.780106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 178/178 [00:13<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.249976\n",
      "index = 1,current_loss = 0.161174\n",
      "index = 2,current_loss = 0.128114\n",
      "index = 3,current_loss = 0.250092\n",
      "index = 4,current_loss = 0.188179\n",
      "index = 5,current_loss = 0.271021\n",
      "totalloss = \n",
      "1.2485574930906296\n",
      "current_point = \n",
      "0.6867320505353763\n",
      "eval_label_loss = \n",
      "[[0, 536, 100, 5], [408, 0, 50, 5], [193, 113, 0, 10], [112, 91, 68, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2122/2122 [05:56<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 149.059311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 178/178 [00:13<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.250917\n",
      "index = 1,current_loss = 0.165580\n",
      "index = 2,current_loss = 0.127599\n",
      "index = 3,current_loss = 0.262224\n",
      "index = 4,current_loss = 0.271156\n",
      "index = 5,current_loss = 0.266334\n",
      "totalloss = \n",
      "1.3438105136156082\n",
      "current_point = \n",
      "0.6787700332944501\n",
      "eval_label_loss = \n",
      "[[0, 500, 138, 15], [419, 0, 57, 10], [204, 102, 0, 19], [121, 79, 69, 0]]\n",
      "test_str = \n",
      "['34121', '34314', '32845']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 32476/32476 [00:10<00:00, 2991.60it/s]\n",
      "100%|█████████████████████████████████████| 4306/4306 [00:01<00:00, 3486.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2030/2030 [05:34<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 377.845001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 270/270 [00:20<00:00, 13.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.167807\n",
      "index = 1,current_loss = 0.077543\n",
      "index = 2,current_loss = 0.119813\n",
      "index = 3,current_loss = 0.180972\n",
      "index = 4,current_loss = 0.267917\n",
      "index = 5,current_loss = 0.335500\n",
      "totalloss = \n",
      "1.149550162255764\n",
      "current_point = \n",
      "0.6955496514141277\n",
      "eval_label_loss = \n",
      "[[0, 630, 26, 0], [527, 0, 43, 0], [482, 171, 0, 0], [170, 115, 54, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2030/2030 [05:35<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 244.018555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 270/270 [00:20<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.161419\n",
      "index = 1,current_loss = 0.079517\n",
      "index = 2,current_loss = 0.131105\n",
      "index = 3,current_loss = 0.173484\n",
      "index = 4,current_loss = 0.290687\n",
      "index = 5,current_loss = 0.357447\n",
      "totalloss = \n",
      "1.1936582326889038\n",
      "current_point = \n",
      "0.6915484357717482\n",
      "eval_label_loss = \n",
      "[[0, 950, 118, 8], [453, 0, 71, 13], [404, 206, 0, 10], [137, 106, 74, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2030/2030 [05:38<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 179.076889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 270/270 [00:20<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.165490\n",
      "index = 1,current_loss = 0.083281\n",
      "index = 2,current_loss = 0.138489\n",
      "index = 3,current_loss = 0.171267\n",
      "index = 4,current_loss = 0.302364\n",
      "index = 5,current_loss = 0.383707\n",
      "totalloss = \n",
      "1.2445983961224556\n",
      "current_point = \n",
      "0.6870735747474125\n",
      "eval_label_loss = \n",
      "[[0, 819, 149, 13], [484, 0, 76, 19], [445, 168, 0, 14], [155, 82, 72, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2030/2030 [05:40<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 143.425629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 270/270 [00:20<00:00, 12.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.169667\n",
      "index = 1,current_loss = 0.087828\n",
      "index = 2,current_loss = 0.154350\n",
      "index = 3,current_loss = 0.171901\n",
      "index = 4,current_loss = 0.310635\n",
      "index = 5,current_loss = 0.355955\n",
      "totalloss = \n",
      "1.250336840748787\n",
      "current_point = \n",
      "0.6865788451596928\n",
      "eval_label_loss = \n",
      "[[0, 761, 137, 17], [487, 0, 55, 22], [439, 181, 0, 11], [161, 77, 68, 0]]\n",
      "test_str = \n",
      "['02996', '34527', '34311']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34512/34512 [00:11<00:00, 3012.57it/s]\n",
      "100%|█████████████████████████████████████| 2270/2270 [00:00<00:00, 3731.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2157/2157 [05:55<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 393.178131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 142/142 [00:10<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.183734\n",
      "index = 1,current_loss = 0.109531\n",
      "index = 2,current_loss = 0.096454\n",
      "index = 3,current_loss = 0.348266\n",
      "index = 4,current_loss = 0.220986\n",
      "index = 5,current_loss = 0.467110\n",
      "totalloss = \n",
      "1.4260796681046486\n",
      "current_point = \n",
      "0.6722579247285322\n",
      "eval_label_loss = \n",
      "[[0, 608, 33, 0], [322, 0, 58, 8], [182, 142, 0, 12], [116, 97, 50, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2157/2157 [05:57<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 256.661957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 142/142 [00:11<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.188346\n",
      "index = 1,current_loss = 0.104291\n",
      "index = 2,current_loss = 0.091341\n",
      "index = 3,current_loss = 0.321749\n",
      "index = 4,current_loss = 0.225383\n",
      "index = 5,current_loss = 0.449817\n",
      "totalloss = \n",
      "1.3809274435043335\n",
      "current_point = \n",
      "0.6757924417818474\n",
      "eval_label_loss = \n",
      "[[0, 556, 45, 3], [330, 0, 69, 5], [176, 133, 0, 14], [120, 78, 60, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2157/2157 [06:00<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 186.967468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 142/142 [00:11<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.194993\n",
      "index = 1,current_loss = 0.115878\n",
      "index = 2,current_loss = 0.107524\n",
      "index = 3,current_loss = 0.322714\n",
      "index = 4,current_loss = 0.208962\n",
      "index = 5,current_loss = 0.454063\n",
      "totalloss = \n",
      "1.404135674238205\n",
      "current_point = \n",
      "0.6739639724900807\n",
      "eval_label_loss = \n",
      "[[0, 445, 57, 4], [354, 0, 73, 9], [199, 104, 0, 32], [120, 69, 56, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2157/2157 [06:02<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 148.154114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 142/142 [00:11<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.195161\n",
      "index = 1,current_loss = 0.117370\n",
      "index = 2,current_loss = 0.114502\n",
      "index = 3,current_loss = 0.329312\n",
      "index = 4,current_loss = 0.231932\n",
      "index = 5,current_loss = 0.472744\n",
      "totalloss = \n",
      "1.4610218182206154\n",
      "current_point = \n",
      "0.6695856690428013\n",
      "eval_label_loss = \n",
      "[[0, 436, 66, 6], [338, 0, 86, 8], [200, 110, 0, 33], [128, 67, 52, 0]]\n",
      "test_str = \n",
      "['01171', '32899', '34162']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34495/34495 [00:11<00:00, 3111.44it/s]\n",
      "100%|█████████████████████████████████████| 2287/2287 [00:00<00:00, 2695.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2156/2156 [05:55<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 380.182861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 143/143 [00:11<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.221107\n",
      "index = 1,current_loss = 0.170140\n",
      "index = 2,current_loss = 0.212152\n",
      "index = 3,current_loss = 0.367599\n",
      "index = 4,current_loss = 0.328933\n",
      "index = 5,current_loss = 0.617003\n",
      "totalloss = \n",
      "1.916933313012123\n",
      "current_point = \n",
      "0.6388823223979103\n",
      "eval_label_loss = \n",
      "[[0, 587, 44, 0], [247, 0, 18, 0], [324, 180, 0, 0], [172, 154, 93, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2156/2156 [05:57<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 249.445389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 143/143 [00:10<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.220738\n",
      "index = 1,current_loss = 0.141355\n",
      "index = 2,current_loss = 0.177195\n",
      "index = 3,current_loss = 0.326788\n",
      "index = 4,current_loss = 0.324883\n",
      "index = 5,current_loss = 0.569985\n",
      "totalloss = \n",
      "1.7609434723854065\n",
      "current_point = \n",
      "0.6486145528519083\n",
      "eval_label_loss = \n",
      "[[0, 621, 71, 3], [234, 0, 29, 1], [279, 181, 0, 12], [136, 141, 128, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2156/2156 [06:00<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 182.342468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 143/143 [00:11<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.217007\n",
      "index = 1,current_loss = 0.144261\n",
      "index = 2,current_loss = 0.187265\n",
      "index = 3,current_loss = 0.350300\n",
      "index = 4,current_loss = 0.328014\n",
      "index = 5,current_loss = 0.584644\n",
      "totalloss = \n",
      "1.81149061024189\n",
      "current_point = \n",
      "0.6453827952741614\n",
      "eval_label_loss = \n",
      "[[0, 617, 101, 10], [226, 0, 38, 3], [277, 163, 0, 41], [145, 104, 114, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2156/2156 [06:02<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 145.235931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 143/143 [00:11<00:00, 12.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.226515\n",
      "index = 1,current_loss = 0.145434\n",
      "index = 2,current_loss = 0.195086\n",
      "index = 3,current_loss = 0.342434\n",
      "index = 4,current_loss = 0.336304\n",
      "index = 5,current_loss = 0.587542\n",
      "totalloss = \n",
      "1.8333151042461395\n",
      "current_point = \n",
      "0.6440111901070129\n",
      "eval_label_loss = \n",
      "[[0, 684, 113, 13], [225, 0, 36, 4], [267, 158, 0, 48], [141, 104, 112, 0]]\n",
      "test_str = \n",
      "['34946', '32795', '34313']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34188/34188 [00:11<00:00, 3041.76it/s]\n",
      "100%|█████████████████████████████████████| 2594/2594 [00:00<00:00, 2901.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2137/2137 [05:52<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 395.716003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 163/163 [00:12<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.092967\n",
      "index = 1,current_loss = 0.106414\n",
      "index = 2,current_loss = 0.151728\n",
      "index = 3,current_loss = 0.271921\n",
      "index = 4,current_loss = 0.115977\n",
      "index = 5,current_loss = 0.317884\n",
      "totalloss = \n",
      "1.0568904876708984\n",
      "current_point = \n",
      "0.7043739716879776\n",
      "eval_label_loss = \n",
      "[[0, 517, 44, 0], [469, 0, 32, 0], [171, 156, 0, 1], [70, 64, 59, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2137/2137 [05:54<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 254.697479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 163/163 [00:12<00:00, 13.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.103668\n",
      "index = 1,current_loss = 0.102311\n",
      "index = 2,current_loss = 0.139145\n",
      "index = 3,current_loss = 0.277101\n",
      "index = 4,current_loss = 0.126587\n",
      "index = 5,current_loss = 0.308665\n",
      "totalloss = \n",
      "1.0574776753783226\n",
      "current_point = \n",
      "0.7043161398532034\n",
      "eval_label_loss = \n",
      "[[0, 470, 67, 1], [461, 0, 52, 2], [176, 126, 0, 11], [65, 55, 55, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2137/2137 [05:56<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 187.652145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 163/163 [00:12<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.097271\n",
      "index = 1,current_loss = 0.108204\n",
      "index = 2,current_loss = 0.138307\n",
      "index = 3,current_loss = 0.294322\n",
      "index = 4,current_loss = 0.137114\n",
      "index = 5,current_loss = 0.331719\n",
      "totalloss = \n",
      "1.1069376170635223\n",
      "current_point = \n",
      "0.6995342709011456\n",
      "eval_label_loss = \n",
      "[[0, 514, 87, 6], [453, 0, 49, 8], [166, 115, 0, 25], [73, 47, 46, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2137/2137 [05:57<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 149.852448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 163/163 [00:12<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.123880\n",
      "index = 1,current_loss = 0.112130\n",
      "index = 2,current_loss = 0.141736\n",
      "index = 3,current_loss = 0.294861\n",
      "index = 4,current_loss = 0.106492\n",
      "index = 5,current_loss = 0.328863\n",
      "totalloss = \n",
      "1.1079631224274635\n",
      "current_point = \n",
      "0.6994369451520008\n",
      "eval_label_loss = \n",
      "[[0, 421, 92, 13], [462, 0, 48, 7], [173, 107, 0, 28], [69, 42, 48, 0]]\n",
      "test_str = \n",
      "['32504', '34949', '02930']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33952/33952 [00:10<00:00, 3099.10it/s]\n",
      "100%|█████████████████████████████████████| 2830/2830 [00:00<00:00, 2986.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2122/2122 [05:49<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 403.470184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 177/177 [00:13<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.056842\n",
      "index = 1,current_loss = 0.107180\n",
      "index = 2,current_loss = 0.079313\n",
      "index = 3,current_loss = 0.093882\n",
      "index = 4,current_loss = 0.250031\n",
      "index = 5,current_loss = 0.138193\n",
      "totalloss = \n",
      "0.7254424802958965\n",
      "current_point = \n",
      "0.7419955630038406\n",
      "eval_label_loss = \n",
      "[[0, 278, 15, 0], [804, 0, 19, 0], [182, 65, 0, 0], [28, 17, 15, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2122/2122 [05:51<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 263.121552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 177/177 [00:13<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.074470\n",
      "index = 1,current_loss = 0.107833\n",
      "index = 2,current_loss = 0.090928\n",
      "index = 3,current_loss = 0.121411\n",
      "index = 4,current_loss = 0.298589\n",
      "index = 5,current_loss = 0.175456\n",
      "totalloss = \n",
      "0.8686861544847488\n",
      "current_point = \n",
      "0.7243746914803356\n",
      "eval_label_loss = \n",
      "[[0, 568, 104, 5], [739, 0, 59, 1], [149, 57, 0, 6], [18, 14, 28, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2122/2122 [05:54<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 193.541336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 177/177 [00:13<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.068060\n",
      "index = 1,current_loss = 0.108782\n",
      "index = 2,current_loss = 0.092599\n",
      "index = 3,current_loss = 0.111402\n",
      "index = 4,current_loss = 0.308826\n",
      "index = 5,current_loss = 0.154469\n",
      "totalloss = \n",
      "0.8441384881734848\n",
      "current_point = \n",
      "0.7272270728719783\n",
      "eval_label_loss = \n",
      "[[0, 404, 94, 9], [745, 0, 56, 9], [156, 48, 0, 14], [21, 10, 19, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2122/2122 [05:56<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 154.221176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 177/177 [00:13<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.063304\n",
      "index = 1,current_loss = 0.108557\n",
      "index = 2,current_loss = 0.100163\n",
      "index = 3,current_loss = 0.126920\n",
      "index = 4,current_loss = 0.325882\n",
      "index = 5,current_loss = 0.153184\n",
      "totalloss = \n",
      "0.8780092224478722\n",
      "current_point = \n",
      "0.723307732888833\n",
      "eval_label_loss = \n",
      "[[0, 397, 113, 10], [742, 0, 62, 8], [158, 45, 0, 17], [20, 9, 19, 0]]\n",
      "test_str = \n",
      "['01460', '34842', '32798']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34069/34069 [00:11<00:00, 3069.40it/s]\n",
      "100%|█████████████████████████████████████| 2713/2713 [00:01<00:00, 2586.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2130/2130 [05:50<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 407.631195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 170/170 [00:13<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.081217\n",
      "index = 1,current_loss = 0.152314\n",
      "index = 2,current_loss = 0.101633\n",
      "index = 3,current_loss = 0.138134\n",
      "index = 4,current_loss = 0.240984\n",
      "index = 5,current_loss = 0.350193\n",
      "totalloss = \n",
      "1.064474456012249\n",
      "current_point = \n",
      "0.7036289894612195\n",
      "eval_label_loss = \n",
      "[[0, 1109, 162, 0], [839, 0, 97, 1], [51, 53, 0, 0], [21, 22, 40, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2130/2130 [05:52<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 263.554291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 170/170 [00:13<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.064737\n",
      "index = 1,current_loss = 0.143806\n",
      "index = 2,current_loss = 0.068382\n",
      "index = 3,current_loss = 0.131684\n",
      "index = 4,current_loss = 0.264616\n",
      "index = 5,current_loss = 0.270344\n",
      "totalloss = \n",
      "0.9435673505067825\n",
      "current_point = \n",
      "0.7160442399491639\n",
      "eval_label_loss = \n",
      "[[0, 706, 130, 4], [914, 0, 83, 4], [69, 46, 0, 4], [24, 22, 21, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2130/2130 [05:55<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 194.328751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 170/170 [00:13<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.082162\n",
      "index = 1,current_loss = 0.151959\n",
      "index = 2,current_loss = 0.089869\n",
      "index = 3,current_loss = 0.146611\n",
      "index = 4,current_loss = 0.298955\n",
      "index = 5,current_loss = 0.285507\n",
      "totalloss = \n",
      "1.0550637543201447\n",
      "current_point = \n",
      "0.7045540493891126\n",
      "eval_label_loss = \n",
      "[[0, 667, 190, 11], [828, 0, 111, 23], [65, 38, 0, 11], [24, 23, 18, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2130/2130 [05:53<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 154.615372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 170/170 [00:13<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.085256\n",
      "index = 1,current_loss = 0.146885\n",
      "index = 2,current_loss = 0.084832\n",
      "index = 3,current_loss = 0.148244\n",
      "index = 4,current_loss = 0.156474\n",
      "index = 5,current_loss = 0.322778\n",
      "totalloss = \n",
      "0.9444688111543655\n",
      "current_point = \n",
      "0.715947150546639\n",
      "eval_label_loss = \n",
      "[[0, 556, 131, 17], [846, 0, 87, 21], [55, 51, 0, 14], [24, 20, 20, 0]]\n",
      "test_str = \n",
      "['32812', '34135', '34913']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 32852/32852 [00:10<00:00, 3095.09it/s]\n",
      "100%|█████████████████████████████████████| 3930/3930 [00:01<00:00, 3230.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2054/2054 [05:38<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 369.587250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 246/246 [00:18<00:00, 13.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.131183\n",
      "index = 1,current_loss = 0.102623\n",
      "index = 2,current_loss = 0.162827\n",
      "index = 3,current_loss = 0.323960\n",
      "index = 4,current_loss = 0.179903\n",
      "index = 5,current_loss = 0.472201\n",
      "totalloss = \n",
      "1.3726966306567192\n",
      "current_point = \n",
      "0.6764469986259533\n",
      "eval_label_loss = \n",
      "[[0, 639, 76, 3], [747, 0, 39, 0], [441, 229, 0, 2], [185, 106, 97, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2054/2054 [05:40<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 238.315918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 246/246 [00:18<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.139691\n",
      "index = 1,current_loss = 0.104261\n",
      "index = 2,current_loss = 0.158086\n",
      "index = 3,current_loss = 0.330053\n",
      "index = 4,current_loss = 0.185509\n",
      "index = 5,current_loss = 0.475323\n",
      "totalloss = \n",
      "1.3929226249456406\n",
      "current_point = \n",
      "0.6748442578108395\n",
      "eval_label_loss = \n",
      "[[0, 854, 155, 14], [682, 0, 77, 4], [384, 249, 0, 22], [143, 118, 96, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2054/2054 [05:42<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 175.840485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 246/246 [00:18<00:00, 12.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.135040\n",
      "index = 1,current_loss = 0.109085\n",
      "index = 2,current_loss = 0.184467\n",
      "index = 3,current_loss = 0.336621\n",
      "index = 4,current_loss = 0.203900\n",
      "index = 5,current_loss = 0.526363\n",
      "totalloss = \n",
      "1.4954756796360016\n",
      "current_point = \n",
      "0.6670022234123673\n",
      "eval_label_loss = \n",
      "[[0, 926, 219, 37], [647, 0, 109, 15], [360, 227, 0, 55], [142, 89, 104, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2054/2054 [05:44<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 139.650818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 246/246 [00:19<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.140972\n",
      "index = 1,current_loss = 0.113069\n",
      "index = 2,current_loss = 0.184338\n",
      "index = 3,current_loss = 0.343062\n",
      "index = 4,current_loss = 0.206929\n",
      "index = 5,current_loss = 0.492826\n",
      "totalloss = \n",
      "1.481196328997612\n",
      "current_point = \n",
      "0.6680668585654392\n",
      "eval_label_loss = \n",
      "[[0, 727, 180, 28], [693, 0, 92, 14], [420, 194, 0, 42], [165, 80, 99, 0]]\n",
      "keys = \n",
      "['01597_0001_A_0001', '01597_0001_A_0002', '01597_0001_A_0003', '01597_0001_A_0004', '01597_0001_A_0005', '01597_0001_A_0006', '01597_0001_A_0007', '01597_0001_A_0008', '01597_0001_A_0009', '01597_0001_A_0010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 21376/21376 [00:07<00:00, 3042.66it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:42<00:00, 13.03it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:42<00:00, 13.06it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:41<00:00, 13.13it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:41<00:00, 13.12it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:42<00:00, 13.05it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:42<00:00, 13.05it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:41<00:00, 13.12it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:41<00:00, 13.12it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:42<00:00, 13.08it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:41<00:00, 13.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/数据/train_dataset_v2.tsv', sep='\\t')\n",
    "seed = 1\n",
    "split_n = 10\n",
    "#split_n = 2\n",
    "\n",
    "content_dict = {}\n",
    "play_dict = {}\n",
    "for index in range(len(train['content'])):\n",
    "    currentindex = train['id'][index]\n",
    "    currentindex = currentindex.split('_')\n",
    "    str1 = currentindex[0]\n",
    "    while len(str1) < 5:\n",
    "        str1 = '0'+str1\n",
    "    str4 = currentindex[3]\n",
    "    while len(str4) < 4:\n",
    "        str4 = '0'+str4\n",
    "    resultindex = str1+'_'+currentindex[1]+'_'+currentindex[2]+'_'+str4\n",
    "    train['id'][index] = currentindex\n",
    "    if str1 not in play_dict:\n",
    "        play_dict[str1] = 0\n",
    "    else:\n",
    "        play_dict[str1] = play_dict[str1]+1\n",
    "    content_dict[resultindex] = train['content'][index]\n",
    "    \n",
    "    \n",
    "def sortedDictValues(adict): \n",
    "    keys = adict.keys() \n",
    "    keys = sorted(keys)\n",
    "    print('keys = ')\n",
    "    print(keys[0:10])\n",
    "    new_content_dict = {}\n",
    "    for data in keys:\n",
    "        new_content_dict[data] = content_dict[data]\n",
    "    return new_content_dict \n",
    "new_content_dict = sortedDictValues(content_dict)\n",
    "\n",
    "inv_new_content_dict = {}\n",
    "for data in new_content_dict:\n",
    "    inv_new_content_dict[new_content_dict[data]] = data\n",
    "\n",
    "keys = new_content_dict.keys()\n",
    "keys = list(keys)\n",
    "dict_keys_index = {}\n",
    "for index in range(len(keys)):\n",
    "    #dict_keys_index[index] = keys[index]\n",
    "    dict_keys_index[keys[index]] = index\n",
    "\n",
    "content,emotions = [],[]\n",
    "dicts = {}\n",
    "inv_dicts = {}\n",
    "label_num = 0\n",
    "characters = []\n",
    "ids = []\n",
    "label1,label2,label3,label4,label5,label6 = [],[],[],[],[],[]\n",
    "for index in range(len(train['content'])):\n",
    "    if pd.isna(train['emotions'][index]) == False:\n",
    "        ids.append(train['id'][index])\n",
    "        content.append(train['content'][index])\n",
    "        emotions.append(train['emotions'][index])\n",
    "        characters.append(train['character'][index])\n",
    "        current_emotion = train['emotions'][index].split(',')\n",
    "        label1.append(int(current_emotion[0]))\n",
    "        label2.append(int(current_emotion[1]))\n",
    "        label3.append(int(current_emotion[2]))\n",
    "        label4.append(int(current_emotion[3]))\n",
    "        label5.append(int(current_emotion[4]))\n",
    "        label6.append(int(current_emotion[5]))\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from nezha import Config\n",
    "from nezha import Bert\n",
    "vocab_file = r'/home/xiaoguzai/模型/nezha-base/vocab.txt'\n",
    "vocab_size = len(open(vocab_file,'r').readlines()) \n",
    "with open('/home/xiaoguzai/模型/nezha-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "json_data['vocab_size'] = vocab_size\n",
    "config = Config(**json_data)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tokenization import FullTokenizer\n",
    "import numpy as np\n",
    "tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from tqdm import tqdm\n",
    "config.with_mlm = False\n",
    "#config.with_pooler = True\n",
    "bertmodel = Bert(config)\n",
    "import os\n",
    "import random\n",
    "from scipy import stats, integrate\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(seed)\n",
    "\n",
    "play_name = []\n",
    "for data in play_dict.keys():\n",
    "    if data != '34945':\n",
    "        play_name.append(data)\n",
    "        \n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text,characters,label1,label2,label3,label4,label5,label6,maxlen,flag):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        self.characters = characters\n",
    "        #self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            #print('current_text:\\t', current_text, len(current_text))\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "            current_str_index_bak = inv_new_content_dict[current_text]\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            current_key_index_bak = dict_keys_index[current_str_index]\n",
    "            # 同一幕中所有上文拼在一起\n",
    "            lcs_lst = []\n",
    "\n",
    "            pre_current_key_index = current_key_index\n",
    "            pre_current_key_text = current_text\n",
    "            pre_content = ''\n",
    "            #前面一个语句对应的索引以及文本内容\n",
    "            num = 0\n",
    "            while True:\n",
    "                new_current_key_index = pre_current_key_index\n",
    "                new_current_key_text = pre_current_key_text\n",
    "                while new_current_key_index != -1 and new_content_dict[keys[new_current_key_index]] == new_current_key_text:\n",
    "                    new_current_key_index = new_current_key_index-1\n",
    "                if new_current_key_index == -1:\n",
    "                    pre_content = ''\n",
    "                    break\n",
    "                # 只在同一幕中找\n",
    "                new_str_index = keys[new_current_key_index]\n",
    "                current_str_indexs = current_str_index.split('_')\n",
    "                new_str_indexs = new_str_index.split('_')\n",
    "                # 跨剧本，停止寻找\n",
    "                if current_str_indexs[0] != new_str_indexs[0]:\n",
    "                    break\n",
    "                # 跨幕，停止寻找\n",
    "                if current_str_indexs[1] != new_str_indexs[1]:\n",
    "                    break\n",
    "                new_pre_content = new_content_dict[keys[new_current_key_index]]\n",
    "                if str(new_pre_content) == 'nan':\n",
    "                    new_pre_content = '无'\n",
    "                if str(current_character) == 'nan':\n",
    "                    break\n",
    "\n",
    "                lcs_lst.append(new_pre_content)\n",
    "                num = num+1\n",
    "                \n",
    "                pre_current_key_index = new_current_key_index\n",
    "                pre_current_key_text = new_pre_content\n",
    "                if num == 5:\n",
    "                    break\n",
    "            lcs_lst.reverse()\n",
    "            if not lcs_lst:\n",
    "                pre_content = '无'\n",
    "            else:\n",
    "                pre_content = ''.join(lcs_lst)\n",
    "            # 统计有效上文长度\n",
    "            \n",
    "            #print('pre_content = ')\n",
    "            #print(pre_content)\n",
    "            #print('current_text = ')\n",
    "            #print(current_text)\n",
    "            \n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            current_character_token = tokenizer.tokenize(current_character)\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            \n",
    "            #current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #for data1 in lcs_lst:\n",
    "            #    new_token = tokenizer.tokenize(data1)\n",
    "            #    current_token = current_token+new_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"] + current_token + [\"[SEP]\"] + current_character_token + [\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+current_character_token+[\"[SEP]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #if len(current_token) > 1:\n",
    "                #print('pre_text:\\t', pre_content, len(pre_content))\n",
    "                #pre_content_lengths.append(len(current_token))\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        \n",
    "        self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        self.tensors = [torch.tensor(self.token_id,dtype=torch.long),\n",
    "                 torch.tensor(self.label[0],dtype=torch.long),\n",
    "                 torch.tensor(self.label[1],dtype=torch.long),\n",
    "                 torch.tensor(self.label[2],dtype=torch.long),\n",
    "                 torch.tensor(self.label[3],dtype=torch.long),\n",
    "                 torch.tensor(self.label[4],dtype=torch.long),\n",
    "                 torch.tensor(self.label[5],dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "\n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            abondon = len(inputs)-length\n",
    "            inputs = inputs[0:2]+inputs[3+abondon-1:]\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "\n",
    "text = content\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        #self.embedding = nn.Embedding(30522,768)\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        #self.fc1 = nn.Linear(config.embedding_size,128)\n",
    "        #self.activation = F.relu\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        #self.activation = F.tanh\n",
    "        #self.fc2 = nn.Linear(128,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        #outputs = self.embedding(input_ids)\n",
    "        mask_ids = torch.not_equal(input_ids,0)\n",
    "        output = self.model(input_ids)\n",
    "        #[64,128,768]\n",
    "        if mask_ids is not None:\n",
    "            mask_ids = mask_ids[:,:,None].float()\n",
    "            output -= 1e-12*(1.0-mask_ids)\n",
    "        #output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        #output = self.fc1(output)\n",
    "        #output = self.activation(output)\n",
    "        #output = self.dropout(output)\n",
    "        #output = self.fc2(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    #之前这里少量return outputs返回值为None\n",
    "\n",
    "def compute_multilabel_loss(x,model,label):\n",
    "    logit = model(x)\n",
    "    mseloss = 0\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    logit = torch.transpose(logit, 0, 1)\n",
    "    mseloss = loss_fn(logit,label)\n",
    "    return mseloss\n",
    "\n",
    "from loader_bert import load_bert_data\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore\n",
    "n_label = 6\n",
    "model = ClassificationModel(bertmodel,config,n_label)\n",
    "\n",
    "bestpoint = []\n",
    "for currentsplit in range(split_n):\n",
    "    bestpoint.append(0.0)\n",
    "needcount = 30//split_n\n",
    "#random.seed(seed)\n",
    "current_list = []\n",
    "for index in range(30):\n",
    "    current_list.append(index)\n",
    "test_index = random.sample(current_list,30)\n",
    "for current_split in range(split_n):\n",
    "    train_text,test_text,train_ids,test_ids = [],[],[],[]\n",
    "    train_characters,test_characters = [],[]\n",
    "    train_label1,train_label2,train_label3,train_label4,train_label5,train_label6 = [],[],[],[],[],[]\n",
    "    test_label1,test_label2,test_label3,test_label4,test_label5,test_label6 = [],[],[],[],[],[]\n",
    "    needcount = 30//split_n\n",
    "    \n",
    "    test_str = []\n",
    "    for index in test_index[needcount*current_split:min(needcount*(current_split+1),30)]:\n",
    "        test_str.append(play_name[index])\n",
    "        if play_name[index] == '02388':\n",
    "            test_str.append('34945')\n",
    "    print('test_str = ')\n",
    "    print(test_str)\n",
    "    for index in range(len(ids)):\n",
    "        current_id = ids[index]\n",
    "        current_str = ''\n",
    "        for data1 in current_id:\n",
    "            current_str = current_str+data1\n",
    "        if  current_id[0] in test_str:\n",
    "            test_ids.append(current_str)\n",
    "            test_text.append(text[index])\n",
    "            test_characters.append(characters[index])\n",
    "            test_label1.append(label1[index])\n",
    "            test_label2.append(label2[index])\n",
    "            test_label3.append(label3[index])\n",
    "            test_label4.append(label4[index])\n",
    "            test_label5.append(label5[index])\n",
    "            test_label6.append(label6[index])\n",
    "        else:\n",
    "            train_ids.append(current_str)\n",
    "            train_text.append(text[index])\n",
    "            train_characters.append(characters[index])\n",
    "            train_label1.append(label1[index])\n",
    "            train_label2.append(label2[index])\n",
    "            train_label3.append(label3[index])\n",
    "            train_label4.append(label4[index])\n",
    "            train_label5.append(label5[index])\n",
    "            train_label6.append(label6[index])\n",
    "\n",
    "    train_dataset = ClassificationDataset(train_text,train_characters,train_label1,train_label2,train_label3,train_label4,train_label5,train_label6,maxlen=250,flag=True)\n",
    "    test_dataset = ClassificationDataset(test_text,test_characters,test_label1,test_label2,test_label3,test_label4,test_label5,test_label6,maxlen=250,flag=False)\n",
    "    #到里面的classificationdataset才进行字符的切割以及划分\n",
    "    train_loader = DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "    \n",
    "    from loader_pretrain_weights import load_bert_data\n",
    "    bertmodel = load_bert_data(bertmodel,'/home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth')\n",
    "    model = ClassificationModel(bertmodel,config,n_label)\n",
    "    #初始化模型:易漏\n",
    "    \n",
    "    import torch.nn.functional as F\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model = model.to(device)\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=9e-6)\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=9e-6)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.MSELoss(reduce=True,size_average=True)\n",
    "\n",
    "        for batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            #print('batch_token_ids')\n",
    "            #print(batch_token_ids)\n",
    "            batch_token_ids = batch_token_ids.to(device)\n",
    "            batch_labels = [batch_label1.numpy(),batch_label2.numpy(),batch_label3.numpy(),\\\n",
    "                            batch_label4.numpy(),batch_label5.numpy(),batch_label6.numpy()]\n",
    "            batch_labels = torch.tensor(batch_labels,dtype=torch.float)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            #for index in range(len(batch_labels)):\n",
    "            #    batch_labels[index] = batch_labels[index].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_multilabel_loss(batch_token_ids,model,batch_labels)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        eval_true_label = [[],[],[],[],[],[]]\n",
    "        eval_predict_label = [[],[],[],[],[],[]]\n",
    "\n",
    "        eval_label_loss = [[0,0,0,0],\\\n",
    "                           [0,0,0,0],\\\n",
    "                           [0,0,0,0],\\\n",
    "                           [0,0,0,0]]\n",
    "        #for batch_token_ids,batch_labels in tqdm(test_loader,bar_format='{l_bar}%s{bar}%s{r_bar}' % (Fore.BLUE, Fore.RESET)):\n",
    "        for batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(test_loader):\n",
    "            batch_token_ids = batch_token_ids.to(device)\n",
    "            batch_labels = [batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6]\n",
    "            with torch.no_grad():\n",
    "                output = model(batch_token_ids)\n",
    "            for index in range(len(output)):\n",
    "                #总的数据\n",
    "                for index1 in range(len(output[index])):\n",
    "                    #对应的类别概率0~6\n",
    "                    abs0 = abs(output[index][index1]-0)\n",
    "                    abs1 = abs(output[index][index1]-1)\n",
    "                    abs2 = abs(output[index][index1]-2)\n",
    "                    abs3 = abs(output[index][index1]-3)\n",
    "                    currentdata = [abs0,abs1,abs2,abs3]\n",
    "                    current_label = currentdata.index(min(currentdata))\n",
    "                    #eval_predict_label[index1].append(current_label)\n",
    "                    current_predict = output[index][index1].item()\n",
    "                    if current_predict < 0.00:\n",
    "                        current_predict = 0\n",
    "                    elif current_predict > 3:\n",
    "                        current_predict = 3\n",
    "                    #当前类别的分类结果,这里append(output[index][index1])\n",
    "                    #直接放入对应概率值时效果最好,这里直接放入0,1,2,3对应的数值\n",
    "                    eval_predict_label[index1].append(current_predict)\n",
    "            for index in range(len(batch_labels)):\n",
    "                current_label = np.array(batch_labels[index].cpu()).tolist()\n",
    "                eval_true_label[index].extend(current_label)\n",
    "        criterion = nn.MSELoss()\n",
    "        totalloss = 0\n",
    "\n",
    "        for index in range(len(eval_true_label)):\n",
    "            inputs = torch.autograd.Variable(torch.from_numpy(np.array(eval_predict_label[index])))\n",
    "            target = torch.autograd.Variable(torch.from_numpy(np.array(eval_true_label[index])))\n",
    "            for index1 in range(len(inputs)):\n",
    "                abs0 = abs(inputs[index1]-0)\n",
    "                abs1 = abs(inputs[index1]-1)\n",
    "                abs2 = abs(inputs[index1]-2)\n",
    "                abs3 = abs(inputs[index1]-3)\n",
    "                currentdata = [abs0,abs1,abs2,abs3]\n",
    "                current_label = currentdata.index(min(currentdata))\n",
    "                true_label = target[index1].item()\n",
    "                if current_label != true_label:\n",
    "                    eval_label_loss[true_label][current_label] = eval_label_loss[true_label][current_label]+1\n",
    "                    #对的预测为错误的\n",
    "            current_loss = criterion(inputs.float(),target.float())\n",
    "            current_loss = current_loss.item()\n",
    "            print('index = %d,current_loss = %f'%(index,current_loss))\n",
    "            totalloss = totalloss+current_loss\n",
    "\n",
    "        #totalloss = totalloss/len(eval_predict_label)\n",
    "        print('totalloss = ')\n",
    "        print(totalloss)\n",
    "        totalloss = totalloss/6\n",
    "        totalloss = math.sqrt(totalloss)\n",
    "        currentpoint = 1/(1+totalloss)\n",
    "        #currentpoint = 1/(1+current_loss)\n",
    "        print('current_point = ')\n",
    "        print(currentpoint)\n",
    "        if currentpoint >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = currentpoint\n",
    "            torch.save(model,'best_score='+str(bestpoint[current_split])+'split='+str(current_split)+'.pth')\n",
    "        print('eval_label_loss = ')\n",
    "        print(eval_label_loss)\n",
    "\n",
    "test=pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/数据/test_dataset.tsv', sep='\\t')\n",
    "\n",
    "content_dict = {}\n",
    "for index in range(len(test['content'])):\n",
    "    currentindex = test['id'][index]\n",
    "    currentindex = currentindex.split('_')\n",
    "    str1 = currentindex[0]\n",
    "    while len(str1) < 5:\n",
    "        str1 = '0'+str1\n",
    "    str4 = currentindex[3]\n",
    "    while len(str4) < 4:\n",
    "        str4 = '0'+str4\n",
    "    resultindex = str1+'_'+currentindex[1]+'_'+currentindex[2]+'_'+str4\n",
    "    content_dict[resultindex] = test['content'][index]\n",
    "\t\n",
    "def sortedDictValues(adict): \n",
    "    keys = adict.keys() \n",
    "    keys = sorted(keys)\n",
    "    print('keys = ')\n",
    "    print(keys[0:10])\n",
    "    new_content_dict = {}\n",
    "    for data in keys:\n",
    "        new_content_dict[data] = content_dict[data]\n",
    "    return new_content_dict \n",
    "new_content_dict = sortedDictValues(content_dict)\n",
    "\n",
    "inv_new_content_dict = {}\n",
    "for data in new_content_dict:\n",
    "    inv_new_content_dict[new_content_dict[data]] = data\n",
    "\t\n",
    "keys = new_content_dict.keys()\n",
    "keys = list(keys)\n",
    "dict_keys_index = {}\n",
    "for index in range(len(keys)):\n",
    "    #dict_keys_index[index] = keys[index]\n",
    "    dict_keys_index[keys[index]] = index\n",
    "\t\n",
    "content,emotions = [],[]\n",
    "dicts = {}\n",
    "inv_dicts = {}\n",
    "label_num = 0\n",
    "characters = []\n",
    "for index in range(len(test['content'])):\n",
    "        content.append(test['content'][index])\n",
    "        characters.append(test['character'][index])\n",
    "\t\t\n",
    "testtext = test['content']\n",
    "testid = test['id']\n",
    "testcharacter = test['character']\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,character,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            #print('current_text:\\t', current_text, len(current_text))\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "            current_str_index_bak = inv_new_content_dict[current_text]\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            current_key_index_bak = dict_keys_index[current_str_index]\n",
    "            # 同一幕中所有上文拼在一起\n",
    "            lcs_lst = []\n",
    "\n",
    "            pre_current_key_index = current_key_index\n",
    "            pre_current_key_text = current_text\n",
    "            pre_content = ''\n",
    "            #前面一个语句对应的索引以及文本内容\n",
    "            num = 0\n",
    "            while True:\n",
    "                new_current_key_index = pre_current_key_index\n",
    "                new_current_key_text = pre_current_key_text\n",
    "                while new_current_key_index != -1 and new_content_dict[keys[new_current_key_index]] == new_current_key_text:\n",
    "                    new_current_key_index = new_current_key_index-1\n",
    "                if new_current_key_index == -1:\n",
    "                    pre_content = ''\n",
    "                    break\n",
    "                # 只在同一幕中找\n",
    "                new_str_index = keys[new_current_key_index]\n",
    "                current_str_indexs = current_str_index.split('_')\n",
    "                new_str_indexs = new_str_index.split('_')\n",
    "                # 跨剧本，停止寻找\n",
    "                if current_str_indexs[0] != new_str_indexs[0]:\n",
    "                    break\n",
    "                # 跨幕，停止寻找\n",
    "                if current_str_indexs[1] != new_str_indexs[1]:\n",
    "                    break\n",
    "                new_pre_content = new_content_dict[keys[new_current_key_index]]\n",
    "                if str(new_pre_content) == 'nan':\n",
    "                    new_pre_content = '无'\n",
    "                if str(current_character) == 'nan':\n",
    "                    break\n",
    "                #if num == 0 or current_character in new_pre_content:\n",
    "                lcs_lst.append(new_pre_content)\n",
    "                num = num+1\n",
    "                pre_current_key_index = new_current_key_index\n",
    "                pre_current_key_text = new_pre_content\n",
    "                if num == 5:\n",
    "                    break\n",
    "            lcs_lst.reverse()\n",
    "            if not lcs_lst:\n",
    "                pre_content = '无'\n",
    "            else:\n",
    "                pre_content = ''.join(lcs_lst)\n",
    "            # 统计有效上文长度\n",
    "            \n",
    "            #print('pre_content = ')\n",
    "            #print(pre_content)\n",
    "            #print('current_text = ')\n",
    "            #print(current_text)\n",
    "            \n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            current_character_token = tokenizer.tokenize(current_character)\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            \n",
    "            #current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #for data1 in lcs_lst:\n",
    "            #    new_token = tokenizer.tokenize(data1)\n",
    "            #    current_token = current_token+new_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"] + current_token + [\"[SEP]\"] + current_character_token + [\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+current_character_token+[\"[SEP]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #if len(current_token) > 1:\n",
    "                #print('pre_text:\\t', pre_content, len(pre_content))\n",
    "                #pre_content_lengths.append(len(current_token))\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        #self.segment_id = sequence_padding(self.segment_id,maxlen)\n",
    "        #self.mask_id = sequence_padding(self.mask_id,maxlen)\n",
    "        self.tensors = [torch.tensor(self.token_id,dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            #print('inputs = ')\n",
    "            #print(len(inputs))\n",
    "            abondon = len(inputs)-length\n",
    "            inputs = inputs[0:2]+inputs[3+abondon-1:]\n",
    "            #print('now current_token = ')\n",
    "            #print(len(inputs))\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "\t\t\n",
    "test_dataset = TestDataset(testtext,testcharacter,maxlen=250)\n",
    "test_loader = DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
    "\n",
    "for current_split in range(split_n):\n",
    "    model = torch.load('best_score='+str(bestpoint[current_split])+'split='+str(current_split)+'.pth')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    eval_predict_label = []\n",
    "    index = []\n",
    "    pred = [[],[],[],[],[],[]]\n",
    "    current_index = 0\n",
    "    for batch_token_ids in tqdm(test_loader):\n",
    "        batch_token_ids = batch_token_ids[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(batch_token_ids)\n",
    "            for index in range(len(output)):\n",
    "                #总的数据\n",
    "                for index1 in range(len(output[index])):\n",
    "                    #对应的类别概率0~6\n",
    "                    abs0 = abs(output[index][index1]-0)\n",
    "                    abs1 = abs(output[index][index1]-1)\n",
    "                    abs2 = abs(output[index][index1]-2)\n",
    "                    abs3 = abs(output[index][index1]-3)\n",
    "                    currentdata = [abs0,abs1,abs2,abs3]\n",
    "                    current_label = currentdata.index(min(currentdata))\n",
    "                    #eval_predict_label[index1].append(current_label)\n",
    "                    current_predict = output[index][index1].item()\n",
    "\n",
    "                    if current_predict < 0.00:\n",
    "                        current_predict = 0.0\n",
    "                    elif current_predict > 3:\n",
    "                        current_predict = 3.0\n",
    "\n",
    "                    pred[index1].append(current_predict)\n",
    "                    #eval_predict_label[index1].append(current_predict)\n",
    "                    #当前类别的分类结果,这里append(output[index][index1])\n",
    "                    #直接放入对应概率值时效果最好,这里直接放入0,1,2,3对应的数值\n",
    "    for index in range(len(pred[0])):\n",
    "        eval_predict_label.append(str(pred[0][index])+','+str(pred[1][index])+','+str(pred[2][index])+','+str(pred[3][index])+','+str(pred[4][index])+','+str(pred[5][index]))\n",
    "    result_data = []\n",
    "    for index in range(len(testid)):\n",
    "        result_data.append([testid[index],eval_predict_label[index]])\n",
    "    #pd.DataFrame({\"id\":testid,\"label\":eval_predict_label}).to_csv(\"/home/xiaoguzai/代码/剧本角色情感识别/数据集/crossentropy\"+str(bestpoint)+\"result.csv\",index=False)\n",
    "    import csv\n",
    "    with open(r'/home/xiaoguzai/程序/剧本角色情感识别/训练数据/多折叠_上文+同一人物说过的两句话_nezha'+str(seed)+'_best_point='+str(bestpoint[current_split])+\"result.csv\", 'w') as f:\n",
    "        tsv_w = csv.writer(f, delimiter='\\t')\n",
    "        tsv_w.writerow(['id', 'emotion'])  # 单行写入\n",
    "        tsv_w.writerows(result_data)  # 多行写入\n",
    "\n",
    "#模型融合\n",
    "import pandas as pd\n",
    "result = []\n",
    "for current_split in range(split_n):\n",
    "    current_result = pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/训练数据/多折叠_上文+同一人物说过的两句话_nezha'+str(seed)+'_best_point='+str(bestpoint[current_split])+\"result.csv\",sep='\\t')\n",
    "    result.append(current_result)\n",
    "\n",
    "final_result = [[],[],[],[],[],[]]\n",
    "test_id = []\n",
    "for index in range(len(result[0]['emotion'])):\n",
    "    test_id.append(result[0]['id'][index])\n",
    "    currentdata = [0,0,0,0,0,0]\n",
    "    for index1 in range(split_n):\n",
    "        newdata = result[index1]['emotion'][index].split(',')\n",
    "        currentdata[0] = currentdata[0]+float(newdata[0])\n",
    "        currentdata[1] = currentdata[1]+float(newdata[1])\n",
    "        currentdata[2] = currentdata[2]+float(newdata[2])\n",
    "        currentdata[3] = currentdata[3]+float(newdata[3])\n",
    "        currentdata[4] = currentdata[4]+float(newdata[4])\n",
    "        currentdata[5] = currentdata[5]+float(newdata[5])\n",
    "    currentdata[0] = currentdata[0]/split_n\n",
    "    currentdata[1] = currentdata[1]/split_n\n",
    "    currentdata[2] = currentdata[2]/split_n\n",
    "    currentdata[3] = currentdata[3]/split_n\n",
    "    currentdata[4] = currentdata[4]/split_n\n",
    "    currentdata[5] = currentdata[5]/split_n\n",
    "    #final_result.append(currentdata)\n",
    "    final_result[0].append(currentdata[0])\n",
    "    final_result[1].append(currentdata[1])\n",
    "    final_result[2].append(currentdata[2])\n",
    "    final_result[3].append(currentdata[3])\n",
    "    final_result[4].append(currentdata[4])\n",
    "    final_result[5].append(currentdata[5])\n",
    "eval_predict_label = []\n",
    "for index in range(len(pred[0])):\n",
    "    eval_predict_label.append(str(final_result[0][index])+','+str(final_result[1][index])+','+str(final_result[2][index]) \\\n",
    "                               +','+str(final_result[3][index])+','+str(final_result[4][index])+','+str(final_result[5][index]))\n",
    "result = []\n",
    "for index in range(len(test_id)):\n",
    "    result.append([test_id[index],eval_predict_label[index]])\n",
    "with open(r'/home/xiaoguzai/程序/剧本角色情感识别/训练数据/多折去掉主语加上一句'+str(split_n)+'折融合nezha+seed='+str(seed)+'.csv','w') as f:\n",
    "    csv_w = csv.writer(f,delimiter='\\t')\n",
    "    csv_w.writerow(['id','emotion'])\n",
    "    csv_w.writerows(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d18f9939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = \n",
      "['01171_0001_A_0001', '01171_0001_A_0002', '01171_0001_A_0003', '01171_0001_A_0004', '01171_0001_A_0005', '01171_0001_A_0006', '01171_0001_A_0007', '01171_0001_A_0008', '01171_0001_A_0009', '01171_0001_A_0010']\n",
      "---__init__ Nezha\n",
      "test_str = \n",
      "['34940', '34949', '01460']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34745/34745 [00:11<00:00, 3108.93it/s]\n",
      "100%|█████████████████████████████████████| 2037/2037 [00:00<00:00, 2843.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2172/2172 [05:57<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 401.612335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 128/128 [00:09<00:00, 13.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.064075\n",
      "index = 1,current_loss = 0.160081\n",
      "index = 2,current_loss = 0.099410\n",
      "index = 3,current_loss = 0.147764\n",
      "index = 4,current_loss = 0.362859\n",
      "index = 5,current_loss = 0.279520\n",
      "totalloss = \n",
      "1.1137089878320694\n",
      "current_point = \n",
      "0.6988929632596649\n",
      "eval_label_loss = \n",
      "[[0, 344, 38, 2], [961, 0, 58, 8], [178, 89, 0, 9], [20, 20, 20, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2172/2172 [05:58<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 257.820038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 128/128 [00:09<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.053707\n",
      "index = 1,current_loss = 0.158524\n",
      "index = 2,current_loss = 0.098802\n",
      "index = 3,current_loss = 0.141530\n",
      "index = 4,current_loss = 0.383458\n",
      "index = 5,current_loss = 0.266793\n",
      "totalloss = \n",
      "1.1028140112757683\n",
      "current_point = \n",
      "0.6999263523697368\n",
      "eval_label_loss = \n",
      "[[0, 189, 22, 3], [1026, 0, 42, 6], [209, 58, 0, 14], [23, 17, 19, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2172/2172 [06:02<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 188.373016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 128/128 [00:09<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.062538\n",
      "index = 1,current_loss = 0.167387\n",
      "index = 2,current_loss = 0.101319\n",
      "index = 3,current_loss = 0.141195\n",
      "index = 4,current_loss = 0.395204\n",
      "index = 5,current_loss = 0.262289\n",
      "totalloss = \n",
      "1.1299324482679367\n",
      "current_point = \n",
      "0.6973690777566774\n",
      "eval_label_loss = \n",
      "[[0, 168, 25, 4], [1059, 0, 35, 8], [215, 53, 0, 16], [25, 13, 17, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2172/2172 [06:05<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 150.061035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 128/128 [00:09<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.072112\n",
      "index = 1,current_loss = 0.161318\n",
      "index = 2,current_loss = 0.112449\n",
      "index = 3,current_loss = 0.137367\n",
      "index = 4,current_loss = 0.400321\n",
      "index = 5,current_loss = 0.271044\n",
      "totalloss = \n",
      "1.1546100601553917\n",
      "current_point = \n",
      "0.695084428870153\n",
      "eval_label_loss = \n",
      "[[0, 206, 36, 3], [1037, 0, 42, 11], [210, 55, 0, 21], [23, 12, 19, 0]]\n",
      "test_str = \n",
      "['02369', '34913', '32812']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34280/34280 [00:11<00:00, 3046.03it/s]\n",
      "100%|█████████████████████████████████████| 2502/2502 [00:00<00:00, 3148.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2143/2143 [05:53<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 395.834869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 157/157 [00:12<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.127114\n",
      "index = 1,current_loss = 0.146448\n",
      "index = 2,current_loss = 0.187584\n",
      "index = 3,current_loss = 0.214223\n",
      "index = 4,current_loss = 0.171314\n",
      "index = 5,current_loss = 0.386398\n",
      "totalloss = \n",
      "1.2330796122550964\n",
      "current_point = \n",
      "0.6880722678413792\n",
      "eval_label_loss = \n",
      "[[0, 623, 89, 1], [531, 0, 67, 0], [198, 162, 0, 2], [62, 59, 59, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2143/2143 [05:55<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 257.347992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 157/157 [00:12<00:00, 13.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.131380\n",
      "index = 1,current_loss = 0.137468\n",
      "index = 2,current_loss = 0.205199\n",
      "index = 3,current_loss = 0.215085\n",
      "index = 4,current_loss = 0.220656\n",
      "index = 5,current_loss = 0.380929\n",
      "totalloss = \n",
      "1.2907164841890335\n",
      "current_point = \n",
      "0.6831489212803942\n",
      "eval_label_loss = \n",
      "[[0, 715, 129, 4], [510, 0, 86, 6], [188, 137, 0, 16], [57, 51, 57, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2143/2143 [05:56<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 187.167770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 157/157 [00:12<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.128079\n",
      "index = 1,current_loss = 0.142969\n",
      "index = 2,current_loss = 0.221159\n",
      "index = 3,current_loss = 0.268951\n",
      "index = 4,current_loss = 0.167533\n",
      "index = 5,current_loss = 0.427226\n",
      "totalloss = \n",
      "1.3559177219867706\n",
      "current_point = \n",
      "0.6777914153254457\n",
      "eval_label_loss = \n",
      "[[0, 614, 141, 17], [523, 0, 85, 23], [204, 105, 0, 34], [63, 40, 51, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2143/2143 [06:00<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 149.685791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 157/157 [00:12<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.132407\n",
      "index = 1,current_loss = 0.150511\n",
      "index = 2,current_loss = 0.229052\n",
      "index = 3,current_loss = 0.237288\n",
      "index = 4,current_loss = 0.207624\n",
      "index = 5,current_loss = 0.420205\n",
      "totalloss = \n",
      "1.3770883530378342\n",
      "current_point = \n",
      "0.6760973450043587\n",
      "eval_label_loss = \n",
      "[[0, 559, 142, 20], [539, 0, 94, 23], [206, 101, 0, 38], [62, 41, 49, 0]]\n",
      "test_str = \n",
      "['02930', '34314', '32795']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33794/33794 [00:10<00:00, 3078.05it/s]\n",
      "100%|█████████████████████████████████████| 2988/2988 [00:00<00:00, 3277.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2113/2113 [05:48<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 387.673492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 187/187 [00:14<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.196972\n",
      "index = 1,current_loss = 0.099504\n",
      "index = 2,current_loss = 0.136920\n",
      "index = 3,current_loss = 0.185120\n",
      "index = 4,current_loss = 0.240459\n",
      "index = 5,current_loss = 0.328463\n",
      "totalloss = \n",
      "1.1874378100037575\n",
      "current_point = \n",
      "0.6921054111991926\n",
      "eval_label_loss = \n",
      "[[0, 560, 37, 0], [387, 0, 14, 0], [334, 158, 0, 0], [94, 109, 42, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2113/2113 [05:50<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 253.548096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 187/187 [00:14<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.174611\n",
      "index = 1,current_loss = 0.094855\n",
      "index = 2,current_loss = 0.131497\n",
      "index = 3,current_loss = 0.174590\n",
      "index = 4,current_loss = 0.247500\n",
      "index = 5,current_loss = 0.354414\n",
      "totalloss = \n",
      "1.1774681955575943\n",
      "current_point = \n",
      "0.6930030242068687\n",
      "eval_label_loss = \n",
      "[[0, 464, 38, 3], [373, 0, 30, 0], [322, 148, 0, 2], [119, 72, 46, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2113/2113 [05:53<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 183.011688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 187/187 [00:14<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.178703\n",
      "index = 1,current_loss = 0.107128\n",
      "index = 2,current_loss = 0.134385\n",
      "index = 3,current_loss = 0.175924\n",
      "index = 4,current_loss = 0.274917\n",
      "index = 5,current_loss = 0.353767\n",
      "totalloss = \n",
      "1.2248236536979675\n",
      "current_point = \n",
      "0.6887927408514615\n",
      "eval_label_loss = \n",
      "[[0, 474, 78, 9], [369, 0, 33, 1], [324, 123, 0, 12], [100, 70, 57, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2113/2113 [05:55<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 147.450089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 187/187 [00:14<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.181392\n",
      "index = 1,current_loss = 0.106644\n",
      "index = 2,current_loss = 0.146426\n",
      "index = 3,current_loss = 0.176155\n",
      "index = 4,current_loss = 0.267803\n",
      "index = 5,current_loss = 0.347810\n",
      "totalloss = \n",
      "1.2262296229600906\n",
      "current_point = \n",
      "0.6886697682317139\n",
      "eval_label_loss = \n",
      "[[0, 509, 75, 10], [365, 0, 34, 3], [316, 128, 0, 14], [107, 62, 56, 0]]\n",
      "test_str = \n",
      "['32505', '34311', '02996']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34830/34830 [00:11<00:00, 3004.07it/s]\n",
      "100%|█████████████████████████████████████| 1952/1952 [00:00<00:00, 3631.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2177/2177 [05:59<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 397.318115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 122/122 [00:09<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.172857\n",
      "index = 1,current_loss = 0.107993\n",
      "index = 2,current_loss = 0.110480\n",
      "index = 3,current_loss = 0.355496\n",
      "index = 4,current_loss = 0.227437\n",
      "index = 5,current_loss = 0.428802\n",
      "totalloss = \n",
      "1.4030649289488792\n",
      "current_point = \n",
      "0.6740477806291801\n",
      "eval_label_loss = \n",
      "[[0, 463, 39, 0], [160, 0, 14, 0], [161, 110, 0, 0], [113, 89, 54, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2177/2177 [06:00<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 258.851105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 122/122 [00:09<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.169129\n",
      "index = 1,current_loss = 0.110852\n",
      "index = 2,current_loss = 0.105237\n",
      "index = 3,current_loss = 0.377956\n",
      "index = 4,current_loss = 0.249754\n",
      "index = 5,current_loss = 0.432460\n",
      "totalloss = \n",
      "1.4453874751925468\n",
      "current_point = \n",
      "0.6707747050996126\n",
      "eval_label_loss = \n",
      "[[0, 380, 41, 1], [179, 0, 20, 0], [176, 97, 0, 3], [129, 67, 55, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2177/2177 [06:04<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 190.049927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 122/122 [00:09<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.170299\n",
      "index = 1,current_loss = 0.116589\n",
      "index = 2,current_loss = 0.120498\n",
      "index = 3,current_loss = 0.369410\n",
      "index = 4,current_loss = 0.319032\n",
      "index = 5,current_loss = 0.428703\n",
      "totalloss = \n",
      "1.5245318189263344\n",
      "current_point = \n",
      "0.6648617642823629\n",
      "eval_label_loss = \n",
      "[[0, 461, 117, 7], [150, 0, 41, 4], [154, 84, 0, 18], [114, 57, 65, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2177/2177 [06:06<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 151.605850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 122/122 [00:09<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.168460\n",
      "index = 1,current_loss = 0.113318\n",
      "index = 2,current_loss = 0.125634\n",
      "index = 3,current_loss = 0.377750\n",
      "index = 4,current_loss = 0.298987\n",
      "index = 5,current_loss = 0.442553\n",
      "totalloss = \n",
      "1.5267019495368004\n",
      "current_point = \n",
      "0.6647032688654786\n",
      "eval_label_loss = \n",
      "[[0, 375, 85, 6], [170, 0, 31, 4], [179, 76, 0, 15], [125, 57, 57, 0]]\n",
      "test_str = \n",
      "['34946', '34842', '32899']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33498/33498 [00:10<00:00, 3096.40it/s]\n",
      "100%|█████████████████████████████████████| 3284/3284 [00:01<00:00, 2900.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2094/2094 [05:45<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 383.534851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 206/206 [00:15<00:00, 13.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.044639\n",
      "index = 1,current_loss = 0.152370\n",
      "index = 2,current_loss = 0.169189\n",
      "index = 3,current_loss = 0.327275\n",
      "index = 4,current_loss = 0.205054\n",
      "index = 5,current_loss = 0.340090\n",
      "totalloss = \n",
      "1.23861650750041\n",
      "current_point = \n",
      "0.6875912703325646\n",
      "eval_label_loss = \n",
      "[[0, 447, 33, 0], [1236, 0, 36, 1], [301, 168, 0, 2], [96, 87, 65, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2094/2094 [05:47<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 247.724731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 206/206 [00:15<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.049018\n",
      "index = 1,current_loss = 0.143563\n",
      "index = 2,current_loss = 0.164167\n",
      "index = 3,current_loss = 0.293260\n",
      "index = 4,current_loss = 0.200428\n",
      "index = 5,current_loss = 0.315050\n",
      "totalloss = \n",
      "1.1654856503009796\n",
      "current_point = \n",
      "0.694090022693849\n",
      "eval_label_loss = \n",
      "[[0, 582, 67, 3], [1164, 0, 75, 6], [238, 168, 0, 18], [70, 81, 82, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2094/2094 [05:49<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 180.937943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 206/206 [00:15<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.050358\n",
      "index = 1,current_loss = 0.145502\n",
      "index = 2,current_loss = 0.170201\n",
      "index = 3,current_loss = 0.338625\n",
      "index = 4,current_loss = 0.207882\n",
      "index = 5,current_loss = 0.333192\n",
      "totalloss = \n",
      "1.2457594387233257\n",
      "current_point = \n",
      "0.6869733281159665\n",
      "eval_label_loss = \n",
      "[[0, 658, 132, 13], [1096, 0, 103, 19], [220, 136, 0, 61], [65, 73, 68, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2094/2094 [05:51<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 144.932220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 206/206 [00:16<00:00, 12.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.044187\n",
      "index = 1,current_loss = 0.143330\n",
      "index = 2,current_loss = 0.188435\n",
      "index = 3,current_loss = 0.315092\n",
      "index = 4,current_loss = 0.211051\n",
      "index = 5,current_loss = 0.323431\n",
      "totalloss = \n",
      "1.2255262807011604\n",
      "current_point = \n",
      "0.6887312715147931\n",
      "eval_label_loss = \n",
      "[[0, 605, 113, 15], [1112, 0, 90, 15], [235, 151, 0, 45], [63, 73, 67, 0]]\n",
      "test_str = \n",
      "['32798', '34173', '32845']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33025/33025 [00:10<00:00, 3054.24it/s]\n",
      "100%|█████████████████████████████████████| 3757/3757 [00:01<00:00, 2885.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2065/2065 [05:40<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 385.258209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 235/235 [00:18<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.103288\n",
      "index = 1,current_loss = 0.041027\n",
      "index = 2,current_loss = 0.099694\n",
      "index = 3,current_loss = 0.132380\n",
      "index = 4,current_loss = 0.474773\n",
      "index = 5,current_loss = 0.376976\n",
      "totalloss = \n",
      "1.2281375005841255\n",
      "current_point = \n",
      "0.6885030791833581\n",
      "eval_label_loss = \n",
      "[[0, 827, 69, 0], [291, 0, 47, 1], [365, 144, 0, 0], [172, 115, 82, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2065/2065 [05:42<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 249.371323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 235/235 [00:18<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.099573\n",
      "index = 1,current_loss = 0.042476\n",
      "index = 2,current_loss = 0.099584\n",
      "index = 3,current_loss = 0.130635\n",
      "index = 4,current_loss = 0.500384\n",
      "index = 5,current_loss = 0.406143\n",
      "totalloss = \n",
      "1.2787949480116367\n",
      "current_point = \n",
      "0.6841523480850691\n",
      "eval_label_loss = \n",
      "[[0, 779, 139, 0], [287, 0, 73, 3], [357, 138, 0, 2], [172, 94, 92, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2065/2065 [05:44<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 183.552277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 235/235 [00:18<00:00, 12.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.112916\n",
      "index = 1,current_loss = 0.045792\n",
      "index = 2,current_loss = 0.126615\n",
      "index = 3,current_loss = 0.139689\n",
      "index = 4,current_loss = 0.592026\n",
      "index = 5,current_loss = 0.377754\n",
      "totalloss = \n",
      "1.3947911597788334\n",
      "current_point = \n",
      "0.6746971624591425\n",
      "eval_label_loss = \n",
      "[[0, 845, 222, 9], [260, 0, 92, 16], [349, 136, 0, 12], [160, 93, 90, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2065/2065 [05:47<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 147.029678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 235/235 [00:18<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.110755\n",
      "index = 1,current_loss = 0.042523\n",
      "index = 2,current_loss = 0.114140\n",
      "index = 3,current_loss = 0.145272\n",
      "index = 4,current_loss = 0.556288\n",
      "index = 5,current_loss = 0.393336\n",
      "totalloss = \n",
      "1.362312901765108\n",
      "current_point = \n",
      "0.6772773930639139\n",
      "eval_label_loss = \n",
      "[[0, 807, 190, 14], [263, 0, 90, 16], [351, 130, 0, 16], [165, 90, 82, 0]]\n",
      "test_str = \n",
      "['34899', '34313', '34135']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 32971/32971 [00:10<00:00, 3049.12it/s]\n",
      "100%|█████████████████████████████████████| 3811/3811 [00:01<00:00, 3580.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2061/2061 [05:40<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 375.168732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 239/239 [00:18<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.123477\n",
      "index = 1,current_loss = 0.113516\n",
      "index = 2,current_loss = 0.121489\n",
      "index = 3,current_loss = 0.312131\n",
      "index = 4,current_loss = 0.152533\n",
      "index = 5,current_loss = 0.536963\n",
      "totalloss = \n",
      "1.3601090461015701\n",
      "current_point = \n",
      "0.677454306749206\n",
      "eval_label_loss = \n",
      "[[0, 733, 90, 0], [1013, 0, 31, 0], [318, 225, 0, 3], [171, 113, 116, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2061/2061 [05:42<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 243.099243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 239/239 [00:18<00:00, 13.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.122927\n",
      "index = 1,current_loss = 0.119356\n",
      "index = 2,current_loss = 0.117400\n",
      "index = 3,current_loss = 0.324627\n",
      "index = 4,current_loss = 0.153009\n",
      "index = 5,current_loss = 0.542645\n",
      "totalloss = \n",
      "1.379963107407093\n",
      "current_point = \n",
      "0.6758689642636031\n",
      "eval_label_loss = \n",
      "[[0, 583, 80, 5], [1029, 0, 30, 1], [354, 201, 0, 17], [175, 108, 86, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2061/2061 [05:44<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 178.184128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 239/239 [00:18<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.117436\n",
      "index = 1,current_loss = 0.126676\n",
      "index = 2,current_loss = 0.118563\n",
      "index = 3,current_loss = 0.324551\n",
      "index = 4,current_loss = 0.149189\n",
      "index = 5,current_loss = 0.545132\n",
      "totalloss = \n",
      "1.38154736161232\n",
      "current_point = \n",
      "0.6757432729828334\n",
      "eval_label_loss = \n",
      "[[0, 521, 79, 7], [1026, 0, 37, 2], [347, 187, 0, 15], [180, 112, 80, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2061/2061 [05:45<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 142.221405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 239/239 [00:18<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.138429\n",
      "index = 1,current_loss = 0.126253\n",
      "index = 2,current_loss = 0.117876\n",
      "index = 3,current_loss = 0.326104\n",
      "index = 4,current_loss = 0.152921\n",
      "index = 5,current_loss = 0.552658\n",
      "totalloss = \n",
      "1.4142408594489098\n",
      "current_point = \n",
      "0.673175620974893\n",
      "eval_label_loss = \n",
      "[[0, 563, 120, 19], [1009, 0, 43, 3], [343, 195, 0, 25], [172, 100, 91, 0]]\n",
      "test_str = \n",
      "['34121', '32504', '02721']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34021/34021 [00:10<00:00, 3092.88it/s]\n",
      "100%|█████████████████████████████████████| 2761/2761 [00:00<00:00, 3306.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2127/2127 [05:51<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 409.259064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 173/173 [00:13<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.081927\n",
      "index = 1,current_loss = 0.057609\n",
      "index = 2,current_loss = 0.097828\n",
      "index = 3,current_loss = 0.126345\n",
      "index = 4,current_loss = 0.150140\n",
      "index = 5,current_loss = 0.151362\n",
      "totalloss = \n",
      "0.6652120240032673\n",
      "current_point = \n",
      "0.7502047267003841\n",
      "eval_label_loss = \n",
      "[[0, 897, 70, 0], [291, 0, 78, 1], [66, 58, 0, 1], [9, 10, 17, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2127/2127 [05:52<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 266.815063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 173/173 [00:13<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.076018\n",
      "index = 1,current_loss = 0.063192\n",
      "index = 2,current_loss = 0.114053\n",
      "index = 3,current_loss = 0.129282\n",
      "index = 4,current_loss = 0.177842\n",
      "index = 5,current_loss = 0.191525\n",
      "totalloss = \n",
      "0.7519115284085274\n",
      "current_point = \n",
      "0.7385504433307412\n",
      "eval_label_loss = \n",
      "[[0, 790, 138, 6], [276, 0, 96, 13], [70, 38, 0, 11], [10, 6, 17, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2127/2127 [05:54<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 197.800995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 173/173 [00:13<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.125875\n",
      "index = 1,current_loss = 0.071655\n",
      "index = 2,current_loss = 0.143786\n",
      "index = 3,current_loss = 0.140665\n",
      "index = 4,current_loss = 0.175954\n",
      "index = 5,current_loss = 0.212524\n",
      "totalloss = \n",
      "0.8704593032598495\n",
      "current_point = \n",
      "0.7241710852304312\n",
      "eval_label_loss = \n",
      "[[0, 801, 183, 22], [267, 0, 96, 28], [66, 39, 0, 20], [9, 7, 15, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2127/2127 [05:57<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 158.598022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 173/173 [00:13<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.100039\n",
      "index = 1,current_loss = 0.079381\n",
      "index = 2,current_loss = 0.161707\n",
      "index = 3,current_loss = 0.144675\n",
      "index = 4,current_loss = 0.192165\n",
      "index = 5,current_loss = 0.239220\n",
      "totalloss = \n",
      "0.9171875789761543\n",
      "current_point = \n",
      "0.7189181025741833\n",
      "eval_label_loss = \n",
      "[[0, 789, 204, 17], [271, 0, 108, 33], [66, 40, 0, 25], [7, 8, 12, 0]]\n",
      "test_str = \n",
      "['34911', '01171', '34126']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33909/33909 [00:11<00:00, 3041.35it/s]\n",
      "100%|█████████████████████████████████████| 2873/2873 [00:00<00:00, 3093.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2120/2120 [05:49<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 393.263397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 180/180 [00:13<00:00, 13.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.226721\n",
      "index = 1,current_loss = 0.177410\n",
      "index = 2,current_loss = 0.146239\n",
      "index = 3,current_loss = 0.263259\n",
      "index = 4,current_loss = 0.123147\n",
      "index = 5,current_loss = 0.282687\n",
      "totalloss = \n",
      "1.2194613888859749\n",
      "current_point = \n",
      "0.6892628031003838\n",
      "eval_label_loss = \n",
      "[[0, 533, 47, 0], [961, 0, 102, 0], [153, 79, 0, 0], [124, 72, 46, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2120/2120 [05:51<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 255.083374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 180/180 [00:13<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.263169\n",
      "index = 1,current_loss = 0.180193\n",
      "index = 2,current_loss = 0.131351\n",
      "index = 3,current_loss = 0.220678\n",
      "index = 4,current_loss = 0.101497\n",
      "index = 5,current_loss = 0.375288\n",
      "totalloss = \n",
      "1.2721757516264915\n",
      "current_point = \n",
      "0.6847127808657938\n",
      "eval_label_loss = \n",
      "[[0, 841, 116, 4], [824, 0, 145, 12], [124, 91, 0, 10], [81, 86, 68, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2120/2120 [05:54<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 186.465607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 180/180 [00:13<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.241140\n",
      "index = 1,current_loss = 0.177859\n",
      "index = 2,current_loss = 0.135253\n",
      "index = 3,current_loss = 0.224019\n",
      "index = 4,current_loss = 0.099441\n",
      "index = 5,current_loss = 0.321415\n",
      "totalloss = \n",
      "1.1991272494196892\n",
      "current_point = \n",
      "0.6910606761849248\n",
      "eval_label_loss = \n",
      "[[0, 502, 71, 2], [918, 0, 112, 11], [135, 85, 0, 7], [103, 80, 51, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2120/2120 [05:56<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 150.468079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 180/180 [00:14<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.258899\n",
      "index = 1,current_loss = 0.185488\n",
      "index = 2,current_loss = 0.147706\n",
      "index = 3,current_loss = 0.229753\n",
      "index = 4,current_loss = 0.108576\n",
      "index = 5,current_loss = 0.385457\n",
      "totalloss = \n",
      "1.3158778995275497\n",
      "current_point = \n",
      "0.681055720890461\n",
      "eval_label_loss = \n",
      "[[0, 701, 137, 14], [836, 0, 160, 33], [120, 84, 0, 16], [86, 76, 60, 0]]\n",
      "test_str = \n",
      "['34162', '34527', '02388', '34945']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33646/33646 [00:10<00:00, 3090.75it/s]\n",
      "100%|█████████████████████████████████████| 3136/3136 [00:00<00:00, 3198.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2103/2103 [05:47<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 391.181610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 196/196 [00:15<00:00, 12.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.185986\n",
      "index = 1,current_loss = 0.155246\n",
      "index = 2,current_loss = 0.088834\n",
      "index = 3,current_loss = 0.240578\n",
      "index = 4,current_loss = 0.170631\n",
      "index = 5,current_loss = 0.428314\n",
      "totalloss = \n",
      "1.269588716328144\n",
      "current_point = \n",
      "0.684932465824196\n",
      "eval_label_loss = \n",
      "[[0, 887, 59, 0], [520, 0, 86, 0], [286, 146, 0, 0], [98, 103, 69, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2103/2103 [05:49<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 253.942551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 196/196 [00:15<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.187949\n",
      "index = 1,current_loss = 0.141271\n",
      "index = 2,current_loss = 0.085649\n",
      "index = 3,current_loss = 0.210078\n",
      "index = 4,current_loss = 0.159725\n",
      "index = 5,current_loss = 0.449154\n",
      "totalloss = \n",
      "1.23382618278265\n",
      "current_point = \n",
      "0.6880073100802926\n",
      "eval_label_loss = \n",
      "[[0, 550, 67, 2], [556, 0, 92, 4], [302, 127, 0, 19], [119, 79, 61, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2103/2103 [05:50<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 185.231415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 196/196 [00:15<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.205142\n",
      "index = 1,current_loss = 0.152393\n",
      "index = 2,current_loss = 0.102580\n",
      "index = 3,current_loss = 0.225415\n",
      "index = 4,current_loss = 0.192022\n",
      "index = 5,current_loss = 0.476521\n",
      "totalloss = \n",
      "1.3540734127163887\n",
      "current_point = \n",
      "0.6779400248193287\n",
      "eval_label_loss = \n",
      "[[0, 763, 126, 9], [503, 0, 122, 21], [281, 124, 0, 38], [112, 73, 64, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2103/2103 [05:52<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 145.750443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 196/196 [00:15<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.195897\n",
      "index = 1,current_loss = 0.149736\n",
      "index = 2,current_loss = 0.093990\n",
      "index = 3,current_loss = 0.237518\n",
      "index = 4,current_loss = 0.211407\n",
      "index = 5,current_loss = 0.495434\n",
      "totalloss = \n",
      "1.3839819580316544\n",
      "current_point = \n",
      "0.6755503487784252\n",
      "eval_label_loss = \n",
      "[[0, 640, 131, 15], [514, 0, 130, 21], [296, 110, 0, 41], [121, 62, 67, 0]]\n",
      "keys = \n",
      "['01597_0001_A_0001', '01597_0001_A_0002', '01597_0001_A_0003', '01597_0001_A_0004', '01597_0001_A_0005', '01597_0001_A_0006', '01597_0001_A_0007', '01597_0001_A_0008', '01597_0001_A_0009', '01597_0001_A_0010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 21376/21376 [00:07<00:00, 2928.98it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:42<00:00, 13.08it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:41<00:00, 13.12it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:42<00:00, 13.06it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:41<00:00, 13.11it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:42<00:00, 13.09it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:41<00:00, 13.16it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:41<00:00, 13.16it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:41<00:00, 13.14it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:42<00:00, 13.04it/s]\n",
      "100%|███████████████████████████████████████| 1336/1336 [01:42<00:00, 13.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/数据/train_dataset_v2.tsv', sep='\\t')\n",
    "seed = 2\n",
    "split_n = 10\n",
    "#split_n = 2\n",
    "\n",
    "content_dict = {}\n",
    "play_dict = {}\n",
    "for index in range(len(train['content'])):\n",
    "    currentindex = train['id'][index]\n",
    "    currentindex = currentindex.split('_')\n",
    "    str1 = currentindex[0]\n",
    "    while len(str1) < 5:\n",
    "        str1 = '0'+str1\n",
    "    str4 = currentindex[3]\n",
    "    while len(str4) < 4:\n",
    "        str4 = '0'+str4\n",
    "    resultindex = str1+'_'+currentindex[1]+'_'+currentindex[2]+'_'+str4\n",
    "    train['id'][index] = currentindex\n",
    "    if str1 not in play_dict:\n",
    "        play_dict[str1] = 0\n",
    "    else:\n",
    "        play_dict[str1] = play_dict[str1]+1\n",
    "    content_dict[resultindex] = train['content'][index]\n",
    "    \n",
    "    \n",
    "def sortedDictValues(adict): \n",
    "    keys = adict.keys() \n",
    "    keys = sorted(keys)\n",
    "    print('keys = ')\n",
    "    print(keys[0:10])\n",
    "    new_content_dict = {}\n",
    "    for data in keys:\n",
    "        new_content_dict[data] = content_dict[data]\n",
    "    return new_content_dict \n",
    "new_content_dict = sortedDictValues(content_dict)\n",
    "\n",
    "inv_new_content_dict = {}\n",
    "for data in new_content_dict:\n",
    "    inv_new_content_dict[new_content_dict[data]] = data\n",
    "\n",
    "keys = new_content_dict.keys()\n",
    "keys = list(keys)\n",
    "dict_keys_index = {}\n",
    "for index in range(len(keys)):\n",
    "    #dict_keys_index[index] = keys[index]\n",
    "    dict_keys_index[keys[index]] = index\n",
    "\n",
    "content,emotions = [],[]\n",
    "dicts = {}\n",
    "inv_dicts = {}\n",
    "label_num = 0\n",
    "characters = []\n",
    "ids = []\n",
    "label1,label2,label3,label4,label5,label6 = [],[],[],[],[],[]\n",
    "for index in range(len(train['content'])):\n",
    "    if pd.isna(train['emotions'][index]) == False:\n",
    "        ids.append(train['id'][index])\n",
    "        content.append(train['content'][index])\n",
    "        emotions.append(train['emotions'][index])\n",
    "        characters.append(train['character'][index])\n",
    "        current_emotion = train['emotions'][index].split(',')\n",
    "        label1.append(int(current_emotion[0]))\n",
    "        label2.append(int(current_emotion[1]))\n",
    "        label3.append(int(current_emotion[2]))\n",
    "        label4.append(int(current_emotion[3]))\n",
    "        label5.append(int(current_emotion[4]))\n",
    "        label6.append(int(current_emotion[5]))\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from nezha import Config\n",
    "from nezha import Bert\n",
    "vocab_file = r'/home/xiaoguzai/模型/nezha-base/vocab.txt'\n",
    "vocab_size = len(open(vocab_file,'r').readlines()) \n",
    "with open('/home/xiaoguzai/模型/nezha-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "json_data['vocab_size'] = vocab_size\n",
    "config = Config(**json_data)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tokenization import FullTokenizer\n",
    "import numpy as np\n",
    "tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from tqdm import tqdm\n",
    "config.with_mlm = False\n",
    "#config.with_pooler = True\n",
    "bertmodel = Bert(config)\n",
    "import os\n",
    "import random\n",
    "from scipy import stats, integrate\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(seed)\n",
    "\n",
    "play_name = []\n",
    "for data in play_dict.keys():\n",
    "    if data != '34945':\n",
    "        play_name.append(data)\n",
    "        \n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text,characters,label1,label2,label3,label4,label5,label6,maxlen,flag):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        self.characters = characters\n",
    "        #self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            #print('current_text:\\t', current_text, len(current_text))\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "            current_str_index_bak = inv_new_content_dict[current_text]\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            current_key_index_bak = dict_keys_index[current_str_index]\n",
    "            # 同一幕中所有上文拼在一起\n",
    "            lcs_lst = []\n",
    "\n",
    "            pre_current_key_index = current_key_index\n",
    "            pre_current_key_text = current_text\n",
    "            pre_content = ''\n",
    "            #前面一个语句对应的索引以及文本内容\n",
    "            num = 0\n",
    "            while True:\n",
    "                new_current_key_index = pre_current_key_index\n",
    "                new_current_key_text = pre_current_key_text\n",
    "                while new_current_key_index != -1 and new_content_dict[keys[new_current_key_index]] == new_current_key_text:\n",
    "                    new_current_key_index = new_current_key_index-1\n",
    "                if new_current_key_index == -1:\n",
    "                    pre_content = ''\n",
    "                    break\n",
    "                # 只在同一幕中找\n",
    "                new_str_index = keys[new_current_key_index]\n",
    "                current_str_indexs = current_str_index.split('_')\n",
    "                new_str_indexs = new_str_index.split('_')\n",
    "                # 跨剧本，停止寻找\n",
    "                if current_str_indexs[0] != new_str_indexs[0]:\n",
    "                    break\n",
    "                # 跨幕，停止寻找\n",
    "                if current_str_indexs[1] != new_str_indexs[1]:\n",
    "                    break\n",
    "                new_pre_content = new_content_dict[keys[new_current_key_index]]\n",
    "                if str(new_pre_content) == 'nan':\n",
    "                    new_pre_content = '无'\n",
    "                if str(current_character) == 'nan':\n",
    "                    break\n",
    "\n",
    "                lcs_lst.append(new_pre_content)\n",
    "                num = num+1\n",
    "                \n",
    "                pre_current_key_index = new_current_key_index\n",
    "                pre_current_key_text = new_pre_content\n",
    "                if num == 5:\n",
    "                    break\n",
    "            lcs_lst.reverse()\n",
    "            if not lcs_lst:\n",
    "                pre_content = '无'\n",
    "            else:\n",
    "                pre_content = ''.join(lcs_lst)\n",
    "            # 统计有效上文长度\n",
    "            \n",
    "            #print('pre_content = ')\n",
    "            #print(pre_content)\n",
    "            #print('current_text = ')\n",
    "            #print(current_text)\n",
    "            \n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            current_character_token = tokenizer.tokenize(current_character)\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            \n",
    "            #current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #for data1 in lcs_lst:\n",
    "            #    new_token = tokenizer.tokenize(data1)\n",
    "            #    current_token = current_token+new_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"] + current_token + [\"[SEP]\"] + current_character_token + [\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+current_character_token+[\"[SEP]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #if len(current_token) > 1:\n",
    "                #print('pre_text:\\t', pre_content, len(pre_content))\n",
    "                #pre_content_lengths.append(len(current_token))\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        \n",
    "        self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        self.tensors = [torch.tensor(self.token_id,dtype=torch.long),\n",
    "                 torch.tensor(self.label[0],dtype=torch.long),\n",
    "                 torch.tensor(self.label[1],dtype=torch.long),\n",
    "                 torch.tensor(self.label[2],dtype=torch.long),\n",
    "                 torch.tensor(self.label[3],dtype=torch.long),\n",
    "                 torch.tensor(self.label[4],dtype=torch.long),\n",
    "                 torch.tensor(self.label[5],dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "\n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            abondon = len(inputs)-length\n",
    "            inputs = inputs[0:2]+inputs[3+abondon-1:]\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "\n",
    "text = content\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        #self.embedding = nn.Embedding(30522,768)\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        #self.fc1 = nn.Linear(config.embedding_size,128)\n",
    "        #self.activation = F.relu\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        #self.activation = F.tanh\n",
    "        #self.fc2 = nn.Linear(128,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        #outputs = self.embedding(input_ids)\n",
    "        mask_ids = torch.not_equal(input_ids,0)\n",
    "        output = self.model(input_ids)\n",
    "        #[64,128,768]\n",
    "        if mask_ids is not None:\n",
    "            mask_ids = mask_ids[:,:,None].float()\n",
    "            output -= 1e-12*(1.0-mask_ids)\n",
    "        #output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        #output = self.fc1(output)\n",
    "        #output = self.activation(output)\n",
    "        #output = self.dropout(output)\n",
    "        #output = self.fc2(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    #之前这里少量return outputs返回值为None\n",
    "\n",
    "def compute_multilabel_loss(x,model,label):\n",
    "    logit = model(x)\n",
    "    mseloss = 0\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    logit = torch.transpose(logit, 0, 1)\n",
    "    mseloss = loss_fn(logit,label)\n",
    "    return mseloss\n",
    "\n",
    "from loader_bert import load_bert_data\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore\n",
    "n_label = 6\n",
    "model = ClassificationModel(bertmodel,config,n_label)\n",
    "\n",
    "bestpoint = []\n",
    "for currentsplit in range(split_n):\n",
    "    bestpoint.append(0.0)\n",
    "needcount = 30//split_n\n",
    "#random.seed(seed)\n",
    "current_list = []\n",
    "for index in range(30):\n",
    "    current_list.append(index)\n",
    "test_index = random.sample(current_list,30)\n",
    "for current_split in range(split_n):\n",
    "    train_text,test_text,train_ids,test_ids = [],[],[],[]\n",
    "    train_characters,test_characters = [],[]\n",
    "    train_label1,train_label2,train_label3,train_label4,train_label5,train_label6 = [],[],[],[],[],[]\n",
    "    test_label1,test_label2,test_label3,test_label4,test_label5,test_label6 = [],[],[],[],[],[]\n",
    "    needcount = 30//split_n\n",
    "    \n",
    "    test_str = []\n",
    "    for index in test_index[needcount*current_split:min(needcount*(current_split+1),30)]:\n",
    "        test_str.append(play_name[index])\n",
    "        if play_name[index] == '02388':\n",
    "            test_str.append('34945')\n",
    "    print('test_str = ')\n",
    "    print(test_str)\n",
    "    for index in range(len(ids)):\n",
    "        current_id = ids[index]\n",
    "        current_str = ''\n",
    "        for data1 in current_id:\n",
    "            current_str = current_str+data1\n",
    "        if  current_id[0] in test_str:\n",
    "            test_ids.append(current_str)\n",
    "            test_text.append(text[index])\n",
    "            test_characters.append(characters[index])\n",
    "            test_label1.append(label1[index])\n",
    "            test_label2.append(label2[index])\n",
    "            test_label3.append(label3[index])\n",
    "            test_label4.append(label4[index])\n",
    "            test_label5.append(label5[index])\n",
    "            test_label6.append(label6[index])\n",
    "        else:\n",
    "            train_ids.append(current_str)\n",
    "            train_text.append(text[index])\n",
    "            train_characters.append(characters[index])\n",
    "            train_label1.append(label1[index])\n",
    "            train_label2.append(label2[index])\n",
    "            train_label3.append(label3[index])\n",
    "            train_label4.append(label4[index])\n",
    "            train_label5.append(label5[index])\n",
    "            train_label6.append(label6[index])\n",
    "\n",
    "    train_dataset = ClassificationDataset(train_text,train_characters,train_label1,train_label2,train_label3,train_label4,train_label5,train_label6,maxlen=250,flag=True)\n",
    "    test_dataset = ClassificationDataset(test_text,test_characters,test_label1,test_label2,test_label3,test_label4,test_label5,test_label6,maxlen=250,flag=False)\n",
    "    #到里面的classificationdataset才进行字符的切割以及划分\n",
    "    train_loader = DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "    \n",
    "    from loader_pretrain_weights import load_bert_data\n",
    "    bertmodel = load_bert_data(bertmodel,'/home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth')\n",
    "    model = ClassificationModel(bertmodel,config,n_label)\n",
    "    #初始化模型:易漏\n",
    "    \n",
    "    import torch.nn.functional as F\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model = model.to(device)\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=9e-6)\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=9e-6)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.MSELoss(reduce=True,size_average=True)\n",
    "\n",
    "        for batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            #print('batch_token_ids')\n",
    "            #print(batch_token_ids)\n",
    "            batch_token_ids = batch_token_ids.to(device)\n",
    "            batch_labels = [batch_label1.numpy(),batch_label2.numpy(),batch_label3.numpy(),\\\n",
    "                            batch_label4.numpy(),batch_label5.numpy(),batch_label6.numpy()]\n",
    "            batch_labels = torch.tensor(batch_labels,dtype=torch.float)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            #for index in range(len(batch_labels)):\n",
    "            #    batch_labels[index] = batch_labels[index].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_multilabel_loss(batch_token_ids,model,batch_labels)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        eval_true_label = [[],[],[],[],[],[]]\n",
    "        eval_predict_label = [[],[],[],[],[],[]]\n",
    "\n",
    "        eval_label_loss = [[0,0,0,0],\\\n",
    "                           [0,0,0,0],\\\n",
    "                           [0,0,0,0],\\\n",
    "                           [0,0,0,0]]\n",
    "        #for batch_token_ids,batch_labels in tqdm(test_loader,bar_format='{l_bar}%s{bar}%s{r_bar}' % (Fore.BLUE, Fore.RESET)):\n",
    "        for batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(test_loader):\n",
    "            batch_token_ids = batch_token_ids.to(device)\n",
    "            batch_labels = [batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6]\n",
    "            with torch.no_grad():\n",
    "                output = model(batch_token_ids)\n",
    "            for index in range(len(output)):\n",
    "                #总的数据\n",
    "                for index1 in range(len(output[index])):\n",
    "                    #对应的类别概率0~6\n",
    "                    abs0 = abs(output[index][index1]-0)\n",
    "                    abs1 = abs(output[index][index1]-1)\n",
    "                    abs2 = abs(output[index][index1]-2)\n",
    "                    abs3 = abs(output[index][index1]-3)\n",
    "                    currentdata = [abs0,abs1,abs2,abs3]\n",
    "                    current_label = currentdata.index(min(currentdata))\n",
    "                    #eval_predict_label[index1].append(current_label)\n",
    "                    current_predict = output[index][index1].item()\n",
    "                    if current_predict < 0.00:\n",
    "                        current_predict = 0\n",
    "                    elif current_predict > 3:\n",
    "                        current_predict = 3\n",
    "                    #当前类别的分类结果,这里append(output[index][index1])\n",
    "                    #直接放入对应概率值时效果最好,这里直接放入0,1,2,3对应的数值\n",
    "                    eval_predict_label[index1].append(current_predict)\n",
    "            for index in range(len(batch_labels)):\n",
    "                current_label = np.array(batch_labels[index].cpu()).tolist()\n",
    "                eval_true_label[index].extend(current_label)\n",
    "        criterion = nn.MSELoss()\n",
    "        totalloss = 0\n",
    "\n",
    "        for index in range(len(eval_true_label)):\n",
    "            inputs = torch.autograd.Variable(torch.from_numpy(np.array(eval_predict_label[index])))\n",
    "            target = torch.autograd.Variable(torch.from_numpy(np.array(eval_true_label[index])))\n",
    "            for index1 in range(len(inputs)):\n",
    "                abs0 = abs(inputs[index1]-0)\n",
    "                abs1 = abs(inputs[index1]-1)\n",
    "                abs2 = abs(inputs[index1]-2)\n",
    "                abs3 = abs(inputs[index1]-3)\n",
    "                currentdata = [abs0,abs1,abs2,abs3]\n",
    "                current_label = currentdata.index(min(currentdata))\n",
    "                true_label = target[index1].item()\n",
    "                if current_label != true_label:\n",
    "                    eval_label_loss[true_label][current_label] = eval_label_loss[true_label][current_label]+1\n",
    "                    #对的预测为错误的\n",
    "            current_loss = criterion(inputs.float(),target.float())\n",
    "            current_loss = current_loss.item()\n",
    "            print('index = %d,current_loss = %f'%(index,current_loss))\n",
    "            totalloss = totalloss+current_loss\n",
    "\n",
    "        #totalloss = totalloss/len(eval_predict_label)\n",
    "        print('totalloss = ')\n",
    "        print(totalloss)\n",
    "        totalloss = totalloss/6\n",
    "        totalloss = math.sqrt(totalloss)\n",
    "        currentpoint = 1/(1+totalloss)\n",
    "        #currentpoint = 1/(1+current_loss)\n",
    "        print('current_point = ')\n",
    "        print(currentpoint)\n",
    "        if currentpoint >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = currentpoint\n",
    "            torch.save(model,'best_score='+str(bestpoint[current_split])+'split='+str(current_split)+'.pth')\n",
    "        print('eval_label_loss = ')\n",
    "        print(eval_label_loss)\n",
    "\n",
    "test=pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/数据/test_dataset.tsv', sep='\\t')\n",
    "\n",
    "content_dict = {}\n",
    "for index in range(len(test['content'])):\n",
    "    currentindex = test['id'][index]\n",
    "    currentindex = currentindex.split('_')\n",
    "    str1 = currentindex[0]\n",
    "    while len(str1) < 5:\n",
    "        str1 = '0'+str1\n",
    "    str4 = currentindex[3]\n",
    "    while len(str4) < 4:\n",
    "        str4 = '0'+str4\n",
    "    resultindex = str1+'_'+currentindex[1]+'_'+currentindex[2]+'_'+str4\n",
    "    content_dict[resultindex] = test['content'][index]\n",
    "\t\n",
    "def sortedDictValues(adict): \n",
    "    keys = adict.keys() \n",
    "    keys = sorted(keys)\n",
    "    print('keys = ')\n",
    "    print(keys[0:10])\n",
    "    new_content_dict = {}\n",
    "    for data in keys:\n",
    "        new_content_dict[data] = content_dict[data]\n",
    "    return new_content_dict \n",
    "new_content_dict = sortedDictValues(content_dict)\n",
    "\n",
    "inv_new_content_dict = {}\n",
    "for data in new_content_dict:\n",
    "    inv_new_content_dict[new_content_dict[data]] = data\n",
    "\t\n",
    "keys = new_content_dict.keys()\n",
    "keys = list(keys)\n",
    "dict_keys_index = {}\n",
    "for index in range(len(keys)):\n",
    "    #dict_keys_index[index] = keys[index]\n",
    "    dict_keys_index[keys[index]] = index\n",
    "\t\n",
    "content,emotions = [],[]\n",
    "dicts = {}\n",
    "inv_dicts = {}\n",
    "label_num = 0\n",
    "characters = []\n",
    "for index in range(len(test['content'])):\n",
    "        content.append(test['content'][index])\n",
    "        characters.append(test['character'][index])\n",
    "\t\t\n",
    "testtext = test['content']\n",
    "testid = test['id']\n",
    "testcharacter = test['character']\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,character,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            #print('current_text:\\t', current_text, len(current_text))\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "            current_str_index_bak = inv_new_content_dict[current_text]\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            current_key_index_bak = dict_keys_index[current_str_index]\n",
    "            # 同一幕中所有上文拼在一起\n",
    "            lcs_lst = []\n",
    "\n",
    "            pre_current_key_index = current_key_index\n",
    "            pre_current_key_text = current_text\n",
    "            pre_content = ''\n",
    "            #前面一个语句对应的索引以及文本内容\n",
    "            num = 0\n",
    "            while True:\n",
    "                new_current_key_index = pre_current_key_index\n",
    "                new_current_key_text = pre_current_key_text\n",
    "                while new_current_key_index != -1 and new_content_dict[keys[new_current_key_index]] == new_current_key_text:\n",
    "                    new_current_key_index = new_current_key_index-1\n",
    "                if new_current_key_index == -1:\n",
    "                    pre_content = ''\n",
    "                    break\n",
    "                # 只在同一幕中找\n",
    "                new_str_index = keys[new_current_key_index]\n",
    "                current_str_indexs = current_str_index.split('_')\n",
    "                new_str_indexs = new_str_index.split('_')\n",
    "                # 跨剧本，停止寻找\n",
    "                if current_str_indexs[0] != new_str_indexs[0]:\n",
    "                    break\n",
    "                # 跨幕，停止寻找\n",
    "                if current_str_indexs[1] != new_str_indexs[1]:\n",
    "                    break\n",
    "                new_pre_content = new_content_dict[keys[new_current_key_index]]\n",
    "                if str(new_pre_content) == 'nan':\n",
    "                    new_pre_content = '无'\n",
    "                if str(current_character) == 'nan':\n",
    "                    break\n",
    "                #if num == 0 or current_character in new_pre_content:\n",
    "                lcs_lst.append(new_pre_content)\n",
    "                num = num+1\n",
    "                pre_current_key_index = new_current_key_index\n",
    "                pre_current_key_text = new_pre_content\n",
    "                if num == 5:\n",
    "                    break\n",
    "            lcs_lst.reverse()\n",
    "            if not lcs_lst:\n",
    "                pre_content = '无'\n",
    "            else:\n",
    "                pre_content = ''.join(lcs_lst)\n",
    "            # 统计有效上文长度\n",
    "            \n",
    "            #print('pre_content = ')\n",
    "            #print(pre_content)\n",
    "            #print('current_text = ')\n",
    "            #print(current_text)\n",
    "            \n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            current_character_token = tokenizer.tokenize(current_character)\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            \n",
    "            #current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #for data1 in lcs_lst:\n",
    "            #    new_token = tokenizer.tokenize(data1)\n",
    "            #    current_token = current_token+new_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"] + current_token + [\"[SEP]\"] + current_character_token + [\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+current_character_token+[\"[SEP]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #if len(current_token) > 1:\n",
    "                #print('pre_text:\\t', pre_content, len(pre_content))\n",
    "                #pre_content_lengths.append(len(current_token))\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        #self.segment_id = sequence_padding(self.segment_id,maxlen)\n",
    "        #self.mask_id = sequence_padding(self.mask_id,maxlen)\n",
    "        self.tensors = [torch.tensor(self.token_id,dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            #print('inputs = ')\n",
    "            #print(len(inputs))\n",
    "            abondon = len(inputs)-length\n",
    "            inputs = inputs[0:2]+inputs[3+abondon-1:]\n",
    "            #print('now current_token = ')\n",
    "            #print(len(inputs))\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "\t\t\n",
    "test_dataset = TestDataset(testtext,testcharacter,maxlen=250)\n",
    "test_loader = DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
    "\n",
    "for current_split in range(split_n):\n",
    "    model = torch.load('best_score='+str(bestpoint[current_split])+'split='+str(current_split)+'.pth')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    eval_predict_label = []\n",
    "    index = []\n",
    "    pred = [[],[],[],[],[],[]]\n",
    "    current_index = 0\n",
    "    for batch_token_ids in tqdm(test_loader):\n",
    "        batch_token_ids = batch_token_ids[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(batch_token_ids)\n",
    "            for index in range(len(output)):\n",
    "                #总的数据\n",
    "                for index1 in range(len(output[index])):\n",
    "                    #对应的类别概率0~6\n",
    "                    abs0 = abs(output[index][index1]-0)\n",
    "                    abs1 = abs(output[index][index1]-1)\n",
    "                    abs2 = abs(output[index][index1]-2)\n",
    "                    abs3 = abs(output[index][index1]-3)\n",
    "                    currentdata = [abs0,abs1,abs2,abs3]\n",
    "                    current_label = currentdata.index(min(currentdata))\n",
    "                    #eval_predict_label[index1].append(current_label)\n",
    "                    current_predict = output[index][index1].item()\n",
    "\n",
    "                    if current_predict < 0.00:\n",
    "                        current_predict = 0.0\n",
    "                    elif current_predict > 3:\n",
    "                        current_predict = 3.0\n",
    "\n",
    "                    pred[index1].append(current_predict)\n",
    "                    #eval_predict_label[index1].append(current_predict)\n",
    "                    #当前类别的分类结果,这里append(output[index][index1])\n",
    "                    #直接放入对应概率值时效果最好,这里直接放入0,1,2,3对应的数值\n",
    "    for index in range(len(pred[0])):\n",
    "        eval_predict_label.append(str(pred[0][index])+','+str(pred[1][index])+','+str(pred[2][index])+','+str(pred[3][index])+','+str(pred[4][index])+','+str(pred[5][index]))\n",
    "    result_data = []\n",
    "    for index in range(len(testid)):\n",
    "        result_data.append([testid[index],eval_predict_label[index]])\n",
    "    #pd.DataFrame({\"id\":testid,\"label\":eval_predict_label}).to_csv(\"/home/xiaoguzai/代码/剧本角色情感识别/数据集/crossentropy\"+str(bestpoint)+\"result.csv\",index=False)\n",
    "    import csv\n",
    "    with open(r'/home/xiaoguzai/程序/剧本角色情感识别/训练数据/多折叠_上文+同一人物说过的两句话_nezha'+str(seed)+'_best_point='+str(bestpoint[current_split])+\"result.csv\", 'w') as f:\n",
    "        tsv_w = csv.writer(f, delimiter='\\t')\n",
    "        tsv_w.writerow(['id', 'emotion'])  # 单行写入\n",
    "        tsv_w.writerows(result_data)  # 多行写入\n",
    "\n",
    "#模型融合\n",
    "import pandas as pd\n",
    "result = []\n",
    "for current_split in range(split_n):\n",
    "    current_result = pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/训练数据/多折叠_上文+同一人物说过的两句话_nezha'+str(seed)+'_best_point='+str(bestpoint[current_split])+\"result.csv\",sep='\\t')\n",
    "    result.append(current_result)\n",
    "\n",
    "final_result = [[],[],[],[],[],[]]\n",
    "test_id = []\n",
    "for index in range(len(result[0]['emotion'])):\n",
    "    test_id.append(result[0]['id'][index])\n",
    "    currentdata = [0,0,0,0,0,0]\n",
    "    for index1 in range(split_n):\n",
    "        newdata = result[index1]['emotion'][index].split(',')\n",
    "        currentdata[0] = currentdata[0]+float(newdata[0])\n",
    "        currentdata[1] = currentdata[1]+float(newdata[1])\n",
    "        currentdata[2] = currentdata[2]+float(newdata[2])\n",
    "        currentdata[3] = currentdata[3]+float(newdata[3])\n",
    "        currentdata[4] = currentdata[4]+float(newdata[4])\n",
    "        currentdata[5] = currentdata[5]+float(newdata[5])\n",
    "    currentdata[0] = currentdata[0]/split_n\n",
    "    currentdata[1] = currentdata[1]/split_n\n",
    "    currentdata[2] = currentdata[2]/split_n\n",
    "    currentdata[3] = currentdata[3]/split_n\n",
    "    currentdata[4] = currentdata[4]/split_n\n",
    "    currentdata[5] = currentdata[5]/split_n\n",
    "    #final_result.append(currentdata)\n",
    "    final_result[0].append(currentdata[0])\n",
    "    final_result[1].append(currentdata[1])\n",
    "    final_result[2].append(currentdata[2])\n",
    "    final_result[3].append(currentdata[3])\n",
    "    final_result[4].append(currentdata[4])\n",
    "    final_result[5].append(currentdata[5])\n",
    "eval_predict_label = []\n",
    "for index in range(len(pred[0])):\n",
    "    eval_predict_label.append(str(final_result[0][index])+','+str(final_result[1][index])+','+str(final_result[2][index]) \\\n",
    "                               +','+str(final_result[3][index])+','+str(final_result[4][index])+','+str(final_result[5][index]))\n",
    "result = []\n",
    "for index in range(len(test_id)):\n",
    "    result.append([test_id[index],eval_predict_label[index]])\n",
    "with open(r'/home/xiaoguzai/程序/剧本角色情感识别/训练数据/多折去掉主语加上一句'+str(split_n)+'折融合nezha+seed='+str(seed)+'.csv','w') as f:\n",
    "    csv_w = csv.writer(f,delimiter='\\t')\n",
    "    csv_w.writerow(['id','emotion'])\n",
    "    csv_w.writerows(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d410ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = \n",
      "['01171_0001_A_0001', '01171_0001_A_0002', '01171_0001_A_0003', '01171_0001_A_0004', '01171_0001_A_0005', '01171_0001_A_0006', '01171_0001_A_0007', '01171_0001_A_0008', '01171_0001_A_0009', '01171_0001_A_0010']\n",
      "---__init__ Nezha\n",
      "test_str = \n",
      "['32504', '34173', '34162']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 32722/32722 [00:10<00:00, 3081.98it/s]\n",
      "100%|█████████████████████████████████████| 4060/4060 [00:01<00:00, 2697.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2046/2046 [05:35<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 377.819611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 254/254 [00:16<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.141385\n",
      "index = 1,current_loss = 0.098432\n",
      "index = 2,current_loss = 0.116111\n",
      "index = 3,current_loss = 0.187714\n",
      "index = 4,current_loss = 0.378859\n",
      "index = 5,current_loss = 0.446120\n",
      "totalloss = \n",
      "1.3686216697096825\n",
      "current_point = \n",
      "0.6767722577868155\n",
      "eval_label_loss = \n",
      "[[0, 366, 24, 0], [493, 0, 17, 0], [503, 125, 0, 0], [264, 117, 52, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2046/2046 [05:33<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 243.034973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 254/254 [00:17<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.139037\n",
      "index = 1,current_loss = 0.093957\n",
      "index = 2,current_loss = 0.115398\n",
      "index = 3,current_loss = 0.201252\n",
      "index = 4,current_loss = 0.364113\n",
      "index = 5,current_loss = 0.365193\n",
      "totalloss = \n",
      "1.2789489552378654\n",
      "current_point = \n",
      "0.6841393368257896\n",
      "eval_label_loss = \n",
      "[[0, 649, 87, 5], [415, 0, 52, 2], [410, 160, 0, 9], [205, 104, 99, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2046/2046 [05:33<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 177.257156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 254/254 [00:21<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.141103\n",
      "index = 1,current_loss = 0.094553\n",
      "index = 2,current_loss = 0.118710\n",
      "index = 3,current_loss = 0.208142\n",
      "index = 4,current_loss = 0.358413\n",
      "index = 5,current_loss = 0.380862\n",
      "totalloss = \n",
      "1.3017822578549385\n",
      "current_point = \n",
      "0.6822242754243596\n",
      "eval_label_loss = \n",
      "[[0, 609, 87, 7], [432, 0, 52, 5], [431, 142, 0, 11], [212, 96, 84, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2046/2046 [05:39<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 141.751617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 254/254 [00:14<00:00, 17.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.144937\n",
      "index = 1,current_loss = 0.099143\n",
      "index = 2,current_loss = 0.113545\n",
      "index = 3,current_loss = 0.202014\n",
      "index = 4,current_loss = 0.383673\n",
      "index = 5,current_loss = 0.392319\n",
      "totalloss = \n",
      "1.3356294259428978\n",
      "current_point = \n",
      "0.6794354133154941\n",
      "eval_label_loss = \n",
      "[[0, 583, 108, 12], [425, 0, 51, 13], [436, 135, 0, 15], [207, 103, 80, 0]]\n",
      "test_str = \n",
      "['02721', '32812', '34311']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34864/34864 [00:11<00:00, 3100.94it/s]\n",
      "100%|█████████████████████████████████████| 1918/1918 [00:00<00:00, 3198.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2179/2179 [05:32<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 383.804016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 120/120 [00:06<00:00, 17.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.298905\n",
      "index = 1,current_loss = 0.216166\n",
      "index = 2,current_loss = 0.191896\n",
      "index = 3,current_loss = 0.583688\n",
      "index = 4,current_loss = 0.205223\n",
      "index = 5,current_loss = 0.592458\n",
      "totalloss = \n",
      "2.08833546936512\n",
      "current_point = \n",
      "0.628945597541716\n",
      "eval_label_loss = \n",
      "[[0, 381, 26, 0], [271, 0, 6, 0], [300, 150, 0, 1], [200, 120, 68, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2179/2179 [05:25<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 249.808945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 120/120 [00:06<00:00, 17.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.279303\n",
      "index = 1,current_loss = 0.189758\n",
      "index = 2,current_loss = 0.189800\n",
      "index = 3,current_loss = 0.548293\n",
      "index = 4,current_loss = 0.204134\n",
      "index = 5,current_loss = 0.586435\n",
      "totalloss = \n",
      "1.9977234601974487\n",
      "current_point = \n",
      "0.6341067295777738\n",
      "eval_label_loss = \n",
      "[[0, 391, 50, 0], [254, 0, 14, 0], [261, 157, 0, 5], [179, 110, 84, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2179/2179 [05:25<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 180.980713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 120/120 [00:06<00:00, 17.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.273950\n",
      "index = 1,current_loss = 0.190839\n",
      "index = 2,current_loss = 0.192389\n",
      "index = 3,current_loss = 0.541573\n",
      "index = 4,current_loss = 0.189314\n",
      "index = 5,current_loss = 0.614081\n",
      "totalloss = \n",
      "2.0021457821130753\n",
      "current_point = \n",
      "0.6338501713932815\n",
      "eval_label_loss = \n",
      "[[0, 289, 48, 2], [254, 0, 13, 2], [276, 144, 0, 7], [186, 92, 92, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2179/2179 [05:28<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 145.196564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 120/120 [00:07<00:00, 17.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.276387\n",
      "index = 1,current_loss = 0.200295\n",
      "index = 2,current_loss = 0.190633\n",
      "index = 3,current_loss = 0.519941\n",
      "index = 4,current_loss = 0.196795\n",
      "index = 5,current_loss = 0.608176\n",
      "totalloss = \n",
      "1.9922280013561249\n",
      "current_point = \n",
      "0.634426231243504\n",
      "eval_label_loss = \n",
      "[[0, 336, 54, 3], [252, 0, 18, 2], [264, 142, 0, 11], [182, 90, 93, 0]]\n",
      "test_str = \n",
      "['34126', '34313', '34946']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33919/33919 [00:10<00:00, 3156.84it/s]\n",
      "100%|█████████████████████████████████████| 2863/2863 [00:00<00:00, 3371.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2120/2120 [05:15<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 380.296753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 179/179 [00:10<00:00, 17.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.264694\n",
      "index = 1,current_loss = 0.174764\n",
      "index = 2,current_loss = 0.194101\n",
      "index = 3,current_loss = 0.374165\n",
      "index = 4,current_loss = 0.177801\n",
      "index = 5,current_loss = 0.393754\n",
      "totalloss = \n",
      "1.5792785733938217\n",
      "current_point = \n",
      "0.6609197594789672\n",
      "eval_label_loss = \n",
      "[[0, 599, 29, 0], [583, 0, 34, 0], [303, 155, 0, 1], [183, 145, 83, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2120/2120 [05:18<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 246.923599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 179/179 [00:10<00:00, 17.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.264060\n",
      "index = 1,current_loss = 0.161636\n",
      "index = 2,current_loss = 0.173370\n",
      "index = 3,current_loss = 0.373778\n",
      "index = 4,current_loss = 0.123110\n",
      "index = 5,current_loss = 0.386525\n",
      "totalloss = \n",
      "1.4824780225753784\n",
      "current_point = \n",
      "0.6679709503394402\n",
      "eval_label_loss = \n",
      "[[0, 490, 40, 1], [573, 0, 35, 3], [280, 149, 0, 9], [162, 136, 97, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2120/2120 [05:12<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 180.363419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 179/179 [00:10<00:00, 17.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.262499\n",
      "index = 1,current_loss = 0.160906\n",
      "index = 2,current_loss = 0.172408\n",
      "index = 3,current_loss = 0.371750\n",
      "index = 4,current_loss = 0.130770\n",
      "index = 5,current_loss = 0.391434\n",
      "totalloss = \n",
      "1.4897662103176117\n",
      "current_point = \n",
      "0.667426888478115\n",
      "eval_label_loss = \n",
      "[[0, 437, 55, 3], [577, 0, 43, 1], [285, 137, 0, 11], [165, 117, 101, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2120/2120 [05:24<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 143.358978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 179/179 [00:10<00:00, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.267175\n",
      "index = 1,current_loss = 0.165425\n",
      "index = 2,current_loss = 0.176195\n",
      "index = 3,current_loss = 0.365097\n",
      "index = 4,current_loss = 0.139369\n",
      "index = 5,current_loss = 0.395161\n",
      "totalloss = \n",
      "1.5084218680858612\n",
      "current_point = \n",
      "0.6660442793810956\n",
      "eval_label_loss = \n",
      "[[0, 654, 89, 6], [527, 0, 52, 7], [249, 148, 0, 21], [152, 116, 98, 0]]\n",
      "test_str = \n",
      "['02369', '34899', '01171']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 35335/35335 [00:11<00:00, 3086.68it/s]\n",
      "100%|█████████████████████████████████████| 1447/1447 [00:00<00:00, 4249.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2209/2209 [05:32<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 411.343109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 91/91 [00:05<00:00, 17.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.025950\n",
      "index = 1,current_loss = 0.220430\n",
      "index = 2,current_loss = 0.079891\n",
      "index = 3,current_loss = 0.142960\n",
      "index = 4,current_loss = 0.030144\n",
      "index = 5,current_loss = 0.511252\n",
      "totalloss = \n",
      "1.0106263160705566\n",
      "current_point = \n",
      "0.7090128635047135\n",
      "eval_label_loss = \n",
      "[[0, 111, 2, 0], [873, 0, 18, 0], [86, 59, 0, 5], [25, 20, 15, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2209/2209 [05:34<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 265.739136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 91/91 [00:05<00:00, 17.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.036360\n",
      "index = 1,current_loss = 0.232060\n",
      "index = 2,current_loss = 0.077180\n",
      "index = 3,current_loss = 0.158390\n",
      "index = 4,current_loss = 0.024878\n",
      "index = 5,current_loss = 0.493170\n",
      "totalloss = \n",
      "1.0220387671142817\n",
      "current_point = \n",
      "0.7078531402119429\n",
      "eval_label_loss = \n",
      "[[0, 170, 10, 0], [789, 0, 37, 1], [83, 59, 0, 6], [22, 21, 15, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2209/2209 [05:32<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 194.911621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 91/91 [00:05<00:00, 17.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.061172\n",
      "index = 1,current_loss = 0.224299\n",
      "index = 2,current_loss = 0.082259\n",
      "index = 3,current_loss = 0.165639\n",
      "index = 4,current_loss = 0.027494\n",
      "index = 5,current_loss = 0.561680\n",
      "totalloss = \n",
      "1.1225426029413939\n",
      "current_point = \n",
      "0.6980610224618516\n",
      "eval_label_loss = \n",
      "[[0, 141, 19, 0], [827, 0, 38, 2], [94, 50, 0, 6], [30, 13, 16, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2209/2209 [05:33<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 157.206924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 91/91 [00:05<00:00, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.078980\n",
      "index = 1,current_loss = 0.235247\n",
      "index = 2,current_loss = 0.086833\n",
      "index = 3,current_loss = 0.172284\n",
      "index = 4,current_loss = 0.027382\n",
      "index = 5,current_loss = 0.533044\n",
      "totalloss = \n",
      "1.133769128471613\n",
      "current_point = \n",
      "0.6970112631792665\n",
      "eval_label_loss = \n",
      "[[0, 183, 20, 2], [798, 0, 47, 4], [90, 37, 0, 7], [25, 17, 12, 0]]\n",
      "test_str = \n",
      "['34842', '32505', '34949']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33343/33343 [00:10<00:00, 3118.89it/s]\n",
      "100%|█████████████████████████████████████| 3439/3439 [00:01<00:00, 3115.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2084/2084 [05:13<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 398.936981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 215/215 [00:12<00:00, 17.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.042709\n",
      "index = 1,current_loss = 0.164020\n",
      "index = 2,current_loss = 0.061670\n",
      "index = 3,current_loss = 0.134017\n",
      "index = 4,current_loss = 0.232739\n",
      "index = 5,current_loss = 0.183000\n",
      "totalloss = \n",
      "0.8181548342108727\n",
      "current_point = \n",
      "0.7303170160558682\n",
      "eval_label_loss = \n",
      "[[0, 376, 25, 0], [1500, 0, 34, 0], [199, 74, 0, 0], [30, 20, 14, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2084/2084 [05:12<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 260.389618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 215/215 [00:12<00:00, 17.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.048917\n",
      "index = 1,current_loss = 0.159455\n",
      "index = 2,current_loss = 0.070430\n",
      "index = 3,current_loss = 0.132830\n",
      "index = 4,current_loss = 0.235593\n",
      "index = 5,current_loss = 0.184379\n",
      "totalloss = \n",
      "0.8316041491925716\n",
      "current_point = \n",
      "0.7287083419927655\n",
      "eval_label_loss = \n",
      "[[0, 409, 60, 3], [1416, 0, 85, 6], [172, 67, 0, 2], [21, 20, 17, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2084/2084 [05:13<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 190.462982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 215/215 [00:12<00:00, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.051049\n",
      "index = 1,current_loss = 0.159260\n",
      "index = 2,current_loss = 0.073998\n",
      "index = 3,current_loss = 0.145418\n",
      "index = 4,current_loss = 0.233078\n",
      "index = 5,current_loss = 0.189043\n",
      "totalloss = \n",
      "0.8518456034362316\n",
      "current_point = \n",
      "0.726324686955167\n",
      "eval_label_loss = \n",
      "[[0, 388, 72, 4], [1383, 0, 77, 13], [181, 57, 0, 11], [17, 21, 16, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2084/2084 [05:18<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 151.643326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 215/215 [00:12<00:00, 17.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.053193\n",
      "index = 1,current_loss = 0.166660\n",
      "index = 2,current_loss = 0.079220\n",
      "index = 3,current_loss = 0.139088\n",
      "index = 4,current_loss = 0.252728\n",
      "index = 5,current_loss = 0.197257\n",
      "totalloss = \n",
      "0.8881458006799221\n",
      "current_point = \n",
      "0.7221576126834106\n",
      "eval_label_loss = \n",
      "[[0, 392, 93, 7], [1368, 0, 102, 21], [178, 59, 0, 16], [21, 17, 14, 0]]\n",
      "test_str = \n",
      "['02388', '34945', '34911', '34940']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33423/33423 [00:10<00:00, 3133.12it/s]\n",
      "100%|█████████████████████████████████████| 3359/3359 [00:01<00:00, 2935.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2089/2089 [05:14<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 400.198456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 210/210 [00:12<00:00, 17.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.075722\n",
      "index = 1,current_loss = 0.134793\n",
      "index = 2,current_loss = 0.108296\n",
      "index = 3,current_loss = 0.159009\n",
      "index = 4,current_loss = 0.174239\n",
      "index = 5,current_loss = 0.246044\n",
      "totalloss = \n",
      "0.8981030359864235\n",
      "current_point = \n",
      "0.7210377395001341\n",
      "eval_label_loss = \n",
      "[[0, 819, 70, 0], [1290, 0, 119, 0], [175, 92, 0, 0], [15, 16, 10, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2089/2089 [05:15<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 259.931396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 210/210 [00:12<00:00, 17.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.086983\n",
      "index = 1,current_loss = 0.134322\n",
      "index = 2,current_loss = 0.118187\n",
      "index = 3,current_loss = 0.160245\n",
      "index = 4,current_loss = 0.189384\n",
      "index = 5,current_loss = 0.294555\n",
      "totalloss = \n",
      "0.9836760461330414\n",
      "current_point = \n",
      "0.7117931839196404\n",
      "eval_label_loss = \n",
      "[[0, 682, 121, 5], [1317, 0, 160, 12], [171, 78, 0, 11], [11, 16, 10, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2089/2089 [05:17<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 189.473892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 210/210 [00:12<00:00, 17.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.133046\n",
      "index = 1,current_loss = 0.151752\n",
      "index = 2,current_loss = 0.130428\n",
      "index = 3,current_loss = 0.173729\n",
      "index = 4,current_loss = 0.175562\n",
      "index = 5,current_loss = 0.293201\n",
      "totalloss = \n",
      "1.0577166229486465\n",
      "current_point = \n",
      "0.7042926133361697\n",
      "eval_label_loss = \n",
      "[[0, 618, 160, 14], [1279, 0, 164, 42], [170, 71, 0, 34], [14, 11, 10, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2089/2089 [05:16<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 150.816101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 210/210 [00:12<00:00, 17.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.073981\n",
      "index = 1,current_loss = 0.141610\n",
      "index = 2,current_loss = 0.125649\n",
      "index = 3,current_loss = 0.171004\n",
      "index = 4,current_loss = 0.171269\n",
      "index = 5,current_loss = 0.271184\n",
      "totalloss = \n",
      "0.9546962007880211\n",
      "current_point = \n",
      "0.7148506958005939\n",
      "eval_label_loss = \n",
      "[[0, 463, 102, 11], [1353, 0, 140, 28], [185, 68, 0, 27], [15, 10, 11, 0]]\n",
      "test_str = \n",
      "['34135', '32899', '32845']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 32988/32988 [00:10<00:00, 3139.28it/s]\n",
      "100%|█████████████████████████████████████| 3794/3794 [00:01<00:00, 3130.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2062/2062 [05:06<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 367.591553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 238/238 [00:13<00:00, 17.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.114478\n",
      "index = 1,current_loss = 0.064324\n",
      "index = 2,current_loss = 0.166245\n",
      "index = 3,current_loss = 0.366222\n",
      "index = 4,current_loss = 0.371478\n",
      "index = 5,current_loss = 0.469749\n",
      "totalloss = \n",
      "1.5524970665574074\n",
      "current_point = \n",
      "0.6628336023895293\n",
      "eval_label_loss = \n",
      "[[0, 965, 96, 1], [308, 0, 11, 0], [442, 309, 0, 2], [196, 186, 144, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2062/2062 [05:09<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 241.684296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 238/238 [00:13<00:00, 17.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.108980\n",
      "index = 1,current_loss = 0.063154\n",
      "index = 2,current_loss = 0.162330\n",
      "index = 3,current_loss = 0.351466\n",
      "index = 4,current_loss = 0.354538\n",
      "index = 5,current_loss = 0.455414\n",
      "totalloss = \n",
      "1.4958818331360817\n",
      "current_point = \n",
      "0.6669720655646242\n",
      "eval_label_loss = \n",
      "[[0, 661, 78, 1], [314, 0, 10, 0], [473, 275, 0, 7], [213, 174, 116, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2062/2062 [05:13<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 175.662888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 238/238 [00:13<00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.111923\n",
      "index = 1,current_loss = 0.069703\n",
      "index = 2,current_loss = 0.159606\n",
      "index = 3,current_loss = 0.353104\n",
      "index = 4,current_loss = 0.356855\n",
      "index = 5,current_loss = 0.448700\n",
      "totalloss = \n",
      "1.4998904392123222\n",
      "current_point = \n",
      "0.666674782527566\n",
      "eval_label_loss = \n",
      "[[0, 694, 135, 4], [319, 0, 20, 2], [439, 255, 0, 23], [211, 133, 124, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2062/2062 [05:14<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 140.850128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 238/238 [00:13<00:00, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.110336\n",
      "index = 1,current_loss = 0.065794\n",
      "index = 2,current_loss = 0.154455\n",
      "index = 3,current_loss = 0.358039\n",
      "index = 4,current_loss = 0.360274\n",
      "index = 5,current_loss = 0.462024\n",
      "totalloss = \n",
      "1.5109212398529053\n",
      "current_point = \n",
      "0.6658601300465157\n",
      "eval_label_loss = \n",
      "[[0, 730, 165, 17], [293, 0, 22, 1], [420, 246, 0, 34], [197, 124, 128, 0]]\n",
      "test_str = \n",
      "['02996', '34527', '01460']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 35400/35400 [00:11<00:00, 3117.98it/s]\n",
      "100%|█████████████████████████████████████| 1382/1382 [00:00<00:00, 3800.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2213/2213 [05:30<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 417.192444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 87/87 [00:04<00:00, 17.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.082983\n",
      "index = 1,current_loss = 0.038854\n",
      "index = 2,current_loss = 0.057436\n",
      "index = 3,current_loss = 0.125496\n",
      "index = 4,current_loss = 0.183413\n",
      "index = 5,current_loss = 0.316192\n",
      "totalloss = \n",
      "0.8043744936585426\n",
      "current_point = \n",
      "0.7319865354855194\n",
      "eval_label_loss = \n",
      "[[0, 332, 23, 2], [232, 0, 48, 4], [60, 60, 0, 10], [9, 25, 11, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2213/2213 [05:34<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 274.797485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 87/87 [00:04<00:00, 17.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.101541\n",
      "index = 1,current_loss = 0.051700\n",
      "index = 2,current_loss = 0.061930\n",
      "index = 3,current_loss = 0.121543\n",
      "index = 4,current_loss = 0.195026\n",
      "index = 5,current_loss = 0.320792\n",
      "totalloss = \n",
      "0.8525321781635284\n",
      "current_point = \n",
      "0.7262446062308978\n",
      "eval_label_loss = \n",
      "[[0, 338, 42, 4], [217, 0, 63, 8], [55, 53, 0, 18], [15, 9, 20, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2213/2213 [05:32<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 200.749985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 87/87 [00:05<00:00, 17.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.092564\n",
      "index = 1,current_loss = 0.054883\n",
      "index = 2,current_loss = 0.076157\n",
      "index = 3,current_loss = 0.130110\n",
      "index = 4,current_loss = 0.201611\n",
      "index = 5,current_loss = 0.302257\n",
      "totalloss = \n",
      "0.8575831614434719\n",
      "current_point = \n",
      "0.7256569986524706\n",
      "eval_label_loss = \n",
      "[[0, 288, 38, 4], [238, 0, 63, 6], [61, 42, 0, 24], [16, 12, 15, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2213/2213 [05:38<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 159.447662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 87/87 [00:05<00:00, 17.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.097006\n",
      "index = 1,current_loss = 0.068213\n",
      "index = 2,current_loss = 0.083513\n",
      "index = 3,current_loss = 0.139314\n",
      "index = 4,current_loss = 0.214029\n",
      "index = 5,current_loss = 0.348982\n",
      "totalloss = \n",
      "0.9510574340820312\n",
      "current_point = \n",
      "0.7152397386065187\n",
      "eval_label_loss = \n",
      "[[0, 312, 61, 7], [220, 0, 81, 11], [62, 39, 0, 32], [16, 10, 15, 0]]\n",
      "test_str = \n",
      "['02930', '32798', '34121']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 34403/34403 [00:10<00:00, 3235.33it/s]\n",
      "100%|█████████████████████████████████████| 2379/2379 [00:00<00:00, 2557.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2151/2151 [05:21<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 409.324524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 149/149 [00:08<00:00, 17.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.112777\n",
      "index = 1,current_loss = 0.061972\n",
      "index = 2,current_loss = 0.081585\n",
      "index = 3,current_loss = 0.093082\n",
      "index = 4,current_loss = 0.362021\n",
      "index = 5,current_loss = 0.398505\n",
      "totalloss = \n",
      "1.1099421828985214\n",
      "current_point = \n",
      "0.6992493258095046\n",
      "eval_label_loss = \n",
      "[[0, 1485, 139, 1], [200, 0, 117, 2], [35, 63, 0, 2], [22, 23, 32, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2151/2151 [05:23<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 267.716980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 149/149 [00:08<00:00, 17.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.152180\n",
      "index = 1,current_loss = 0.069706\n",
      "index = 2,current_loss = 0.135329\n",
      "index = 3,current_loss = 0.103090\n",
      "index = 4,current_loss = 0.358828\n",
      "index = 5,current_loss = 0.461001\n",
      "totalloss = \n",
      "1.2801335602998734\n",
      "current_point = \n",
      "0.6840392984934842\n",
      "eval_label_loss = \n",
      "[[0, 1348, 242, 9], [170, 0, 154, 18], [33, 52, 0, 19], [17, 19, 29, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2151/2151 [05:26<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 196.056732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 149/149 [00:08<00:00, 17.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.121972\n",
      "index = 1,current_loss = 0.069884\n",
      "index = 2,current_loss = 0.151904\n",
      "index = 3,current_loss = 0.123865\n",
      "index = 4,current_loss = 0.310300\n",
      "index = 5,current_loss = 0.458020\n",
      "totalloss = \n",
      "1.235944740474224\n",
      "current_point = \n",
      "0.6878231518139926\n",
      "eval_label_loss = \n",
      "[[0, 980, 239, 29], [208, 0, 139, 36], [37, 45, 0, 26], [21, 15, 25, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2151/2151 [05:32<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 156.089905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 149/149 [00:08<00:00, 17.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.148910\n",
      "index = 1,current_loss = 0.071521\n",
      "index = 2,current_loss = 0.159957\n",
      "index = 3,current_loss = 0.090465\n",
      "index = 4,current_loss = 0.343825\n",
      "index = 5,current_loss = 0.454129\n",
      "totalloss = \n",
      "1.2688079327344894\n",
      "current_point = \n",
      "0.6849988398112803\n",
      "eval_label_loss = \n",
      "[[0, 849, 278, 31], [207, 0, 133, 46], [39, 40, 0, 26], [20, 17, 22, 0]]\n",
      "test_str = \n",
      "['32795', '34314', '34913']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 32322/32322 [00:10<00:00, 3102.95it/s]\n",
      "100%|█████████████████████████████████████| 4460/4460 [00:01<00:00, 3298.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_norm.weight\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_dense1.bias\n",
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2021/2021 [05:03<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 377.218506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 279/279 [00:15<00:00, 17.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.129760\n",
      "index = 1,current_loss = 0.103403\n",
      "index = 2,current_loss = 0.145150\n",
      "index = 3,current_loss = 0.155528\n",
      "index = 4,current_loss = 0.196501\n",
      "index = 5,current_loss = 0.375236\n",
      "totalloss = \n",
      "1.105578526854515\n",
      "current_point = \n",
      "0.6996633668424702\n",
      "eval_label_loss = \n",
      "[[0, 1044, 109, 0], [793, 0, 51, 0], [376, 249, 0, 5], [109, 81, 64, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2021/2021 [05:03<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个epoch的学习率：0.000006\n",
      "Train Loss: 243.117676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 279/279 [00:16<00:00, 17.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.130933\n",
      "index = 1,current_loss = 0.111844\n",
      "index = 2,current_loss = 0.146895\n",
      "index = 3,current_loss = 0.141634\n",
      "index = 4,current_loss = 0.210658\n",
      "index = 5,current_loss = 0.326209\n",
      "totalloss = \n",
      "1.0681728422641754\n",
      "current_point = \n",
      "0.7032672245926479\n",
      "eval_label_loss = \n",
      "[[0, 751, 113, 5], [780, 0, 104, 3], [390, 199, 0, 15], [109, 82, 54, 0]]\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2021/2021 [05:07<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2个epoch的学习率：0.000005\n",
      "Train Loss: 176.692032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 279/279 [00:16<00:00, 17.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.136126\n",
      "index = 1,current_loss = 0.109792\n",
      "index = 2,current_loss = 0.155057\n",
      "index = 3,current_loss = 0.144082\n",
      "index = 4,current_loss = 0.213747\n",
      "index = 5,current_loss = 0.342036\n",
      "totalloss = \n",
      "1.1008392423391342\n",
      "current_point = \n",
      "0.700114533258899\n",
      "eval_label_loss = \n",
      "[[0, 603, 124, 10], [813, 0, 99, 5], [430, 160, 0, 19], [118, 66, 56, 0]]\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2021/2021 [05:08<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3个epoch的学习率：0.000004\n",
      "Train Loss: 141.336945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 279/279 [00:16<00:00, 17.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.137466\n",
      "index = 1,current_loss = 0.110423\n",
      "index = 2,current_loss = 0.162744\n",
      "index = 3,current_loss = 0.152548\n",
      "index = 4,current_loss = 0.229226\n",
      "index = 5,current_loss = 0.363748\n",
      "totalloss = \n",
      "1.1561551243066788\n",
      "current_point = \n",
      "0.6949426979635358\n",
      "eval_label_loss = \n",
      "[[0, 652, 134, 19], [811, 0, 85, 16], [430, 152, 0, 31], [129, 56, 52, 0]]\n",
      "keys = \n",
      "['01597_0001_A_0001', '01597_0001_A_0002', '01597_0001_A_0003', '01597_0001_A_0004', '01597_0001_A_0005', '01597_0001_A_0006', '01597_0001_A_0007', '01597_0001_A_0008', '01597_0001_A_0009', '01597_0001_A_0010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 21376/21376 [00:07<00:00, 3014.32it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_score=0.6841393368257896split=0.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4706/1137452772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcurrent_split\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_score='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbestpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_split\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'split='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_score=0.6841393368257896split=0.pth'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/数据/train_dataset_v2.tsv', sep='\\t')\n",
    "seed = 3\n",
    "split_n = 10\n",
    "#split_n = 2\n",
    "\n",
    "content_dict = {}\n",
    "play_dict = {}\n",
    "for index in range(len(train['content'])):\n",
    "    currentindex = train['id'][index]\n",
    "    currentindex = currentindex.split('_')\n",
    "    str1 = currentindex[0]\n",
    "    while len(str1) < 5:\n",
    "        str1 = '0'+str1\n",
    "    str4 = currentindex[3]\n",
    "    while len(str4) < 4:\n",
    "        str4 = '0'+str4\n",
    "    resultindex = str1+'_'+currentindex[1]+'_'+currentindex[2]+'_'+str4\n",
    "    train['id'][index] = currentindex\n",
    "    if str1 not in play_dict:\n",
    "        play_dict[str1] = 0\n",
    "    else:\n",
    "        play_dict[str1] = play_dict[str1]+1\n",
    "    content_dict[resultindex] = train['content'][index]\n",
    "    \n",
    "    \n",
    "def sortedDictValues(adict): \n",
    "    keys = adict.keys() \n",
    "    keys = sorted(keys)\n",
    "    print('keys = ')\n",
    "    print(keys[0:10])\n",
    "    new_content_dict = {}\n",
    "    for data in keys:\n",
    "        new_content_dict[data] = content_dict[data]\n",
    "    return new_content_dict \n",
    "new_content_dict = sortedDictValues(content_dict)\n",
    "\n",
    "inv_new_content_dict = {}\n",
    "for data in new_content_dict:\n",
    "    inv_new_content_dict[new_content_dict[data]] = data\n",
    "\n",
    "keys = new_content_dict.keys()\n",
    "keys = list(keys)\n",
    "dict_keys_index = {}\n",
    "for index in range(len(keys)):\n",
    "    #dict_keys_index[index] = keys[index]\n",
    "    dict_keys_index[keys[index]] = index\n",
    "\n",
    "content,emotions = [],[]\n",
    "dicts = {}\n",
    "inv_dicts = {}\n",
    "label_num = 0\n",
    "characters = []\n",
    "ids = []\n",
    "label1,label2,label3,label4,label5,label6 = [],[],[],[],[],[]\n",
    "for index in range(len(train['content'])):\n",
    "    if pd.isna(train['emotions'][index]) == False:\n",
    "        ids.append(train['id'][index])\n",
    "        content.append(train['content'][index])\n",
    "        emotions.append(train['emotions'][index])\n",
    "        characters.append(train['character'][index])\n",
    "        current_emotion = train['emotions'][index].split(',')\n",
    "        label1.append(int(current_emotion[0]))\n",
    "        label2.append(int(current_emotion[1]))\n",
    "        label3.append(int(current_emotion[2]))\n",
    "        label4.append(int(current_emotion[3]))\n",
    "        label5.append(int(current_emotion[4]))\n",
    "        label6.append(int(current_emotion[5]))\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from nezha import Config\n",
    "from nezha import Bert\n",
    "vocab_file = r'/home/xiaoguzai/模型/nezha-base/vocab.txt'\n",
    "vocab_size = len(open(vocab_file,'r').readlines()) \n",
    "with open('/home/xiaoguzai/模型/nezha-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "json_data['vocab_size'] = vocab_size\n",
    "config = Config(**json_data)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tokenization import FullTokenizer\n",
    "import numpy as np\n",
    "tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from tqdm import tqdm\n",
    "config.with_mlm = False\n",
    "#config.with_pooler = True\n",
    "bertmodel = Bert(config)\n",
    "import os\n",
    "import random\n",
    "from scipy import stats, integrate\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(seed)\n",
    "\n",
    "play_name = []\n",
    "for data in play_dict.keys():\n",
    "    if data != '34945':\n",
    "        play_name.append(data)\n",
    "        \n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text,characters,label1,label2,label3,label4,label5,label6,maxlen,flag):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        self.characters = characters\n",
    "        #self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            #print('current_text:\\t', current_text, len(current_text))\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "            current_str_index_bak = inv_new_content_dict[current_text]\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            current_key_index_bak = dict_keys_index[current_str_index]\n",
    "            # 同一幕中所有上文拼在一起\n",
    "            lcs_lst = []\n",
    "\n",
    "            pre_current_key_index = current_key_index\n",
    "            pre_current_key_text = current_text\n",
    "            pre_content = ''\n",
    "            #前面一个语句对应的索引以及文本内容\n",
    "            num = 0\n",
    "            while True:\n",
    "                new_current_key_index = pre_current_key_index\n",
    "                new_current_key_text = pre_current_key_text\n",
    "                while new_current_key_index != -1 and new_content_dict[keys[new_current_key_index]] == new_current_key_text:\n",
    "                    new_current_key_index = new_current_key_index-1\n",
    "                if new_current_key_index == -1:\n",
    "                    pre_content = ''\n",
    "                    break\n",
    "                # 只在同一幕中找\n",
    "                new_str_index = keys[new_current_key_index]\n",
    "                current_str_indexs = current_str_index.split('_')\n",
    "                new_str_indexs = new_str_index.split('_')\n",
    "                # 跨剧本，停止寻找\n",
    "                if current_str_indexs[0] != new_str_indexs[0]:\n",
    "                    break\n",
    "                # 跨幕，停止寻找\n",
    "                if current_str_indexs[1] != new_str_indexs[1]:\n",
    "                    break\n",
    "                new_pre_content = new_content_dict[keys[new_current_key_index]]\n",
    "                if str(new_pre_content) == 'nan':\n",
    "                    new_pre_content = '无'\n",
    "                if str(current_character) == 'nan':\n",
    "                    break\n",
    "\n",
    "                lcs_lst.append(new_pre_content)\n",
    "                num = num+1\n",
    "                \n",
    "                pre_current_key_index = new_current_key_index\n",
    "                pre_current_key_text = new_pre_content\n",
    "                if num == 5:\n",
    "                    break\n",
    "            lcs_lst.reverse()\n",
    "            if not lcs_lst:\n",
    "                pre_content = '无'\n",
    "            else:\n",
    "                pre_content = ''.join(lcs_lst)\n",
    "            # 统计有效上文长度\n",
    "            \n",
    "            #print('pre_content = ')\n",
    "            #print(pre_content)\n",
    "            #print('current_text = ')\n",
    "            #print(current_text)\n",
    "            \n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            current_character_token = tokenizer.tokenize(current_character)\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            \n",
    "            #current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #for data1 in lcs_lst:\n",
    "            #    new_token = tokenizer.tokenize(data1)\n",
    "            #    current_token = current_token+new_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"] + current_token + [\"[SEP]\"] + current_character_token + [\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+current_character_token+[\"[SEP]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #if len(current_token) > 1:\n",
    "                #print('pre_text:\\t', pre_content, len(pre_content))\n",
    "                #pre_content_lengths.append(len(current_token))\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        \n",
    "        self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        self.tensors = [torch.tensor(self.token_id,dtype=torch.long),\n",
    "                 torch.tensor(self.label[0],dtype=torch.long),\n",
    "                 torch.tensor(self.label[1],dtype=torch.long),\n",
    "                 torch.tensor(self.label[2],dtype=torch.long),\n",
    "                 torch.tensor(self.label[3],dtype=torch.long),\n",
    "                 torch.tensor(self.label[4],dtype=torch.long),\n",
    "                 torch.tensor(self.label[5],dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "\n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            abondon = len(inputs)-length\n",
    "            inputs = inputs[0:2]+inputs[3+abondon-1:]\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "\n",
    "text = content\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        #self.embedding = nn.Embedding(30522,768)\n",
    "        self.model = model\n",
    "        self.fc = nn.Linear(config.embedding_size,n_labels)\n",
    "        #self.fc1 = nn.Linear(config.embedding_size,128)\n",
    "        #self.activation = F.relu\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        #self.activation = F.tanh\n",
    "        #self.fc2 = nn.Linear(128,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids):\n",
    "        #outputs = self.embedding(input_ids)\n",
    "        mask_ids = torch.not_equal(input_ids,0)\n",
    "        output = self.model(input_ids)\n",
    "        #[64,128,768]\n",
    "        if mask_ids is not None:\n",
    "            mask_ids = mask_ids[:,:,None].float()\n",
    "            output -= 1e-12*(1.0-mask_ids)\n",
    "        #output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        #output = self.fc1(output)\n",
    "        #output = self.activation(output)\n",
    "        #output = self.dropout(output)\n",
    "        #output = self.fc2(output)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    #之前这里少量return outputs返回值为None\n",
    "\n",
    "def compute_multilabel_loss(x,model,label):\n",
    "    logit = model(x)\n",
    "    mseloss = 0\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    logit = torch.transpose(logit, 0, 1)\n",
    "    mseloss = loss_fn(logit,label)\n",
    "    return mseloss\n",
    "\n",
    "from loader_bert import load_bert_data\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore\n",
    "n_label = 6\n",
    "model = ClassificationModel(bertmodel,config,n_label)\n",
    "\n",
    "bestpoint = []\n",
    "for currentsplit in range(split_n):\n",
    "    bestpoint.append(0.0)\n",
    "needcount = 30//split_n\n",
    "#random.seed(seed)\n",
    "current_list = []\n",
    "for index in range(30):\n",
    "    current_list.append(index)\n",
    "test_index = random.sample(current_list,30)\n",
    "for current_split in range(split_n):\n",
    "    train_text,test_text,train_ids,test_ids = [],[],[],[]\n",
    "    train_characters,test_characters = [],[]\n",
    "    train_label1,train_label2,train_label3,train_label4,train_label5,train_label6 = [],[],[],[],[],[]\n",
    "    test_label1,test_label2,test_label3,test_label4,test_label5,test_label6 = [],[],[],[],[],[]\n",
    "    needcount = 30//split_n\n",
    "    \n",
    "    test_str = []\n",
    "    for index in test_index[needcount*current_split:min(needcount*(current_split+1),30)]:\n",
    "        test_str.append(play_name[index])\n",
    "        if play_name[index] == '02388':\n",
    "            test_str.append('34945')\n",
    "    print('test_str = ')\n",
    "    print(test_str)\n",
    "    for index in range(len(ids)):\n",
    "        current_id = ids[index]\n",
    "        current_str = ''\n",
    "        for data1 in current_id:\n",
    "            current_str = current_str+data1\n",
    "        if  current_id[0] in test_str:\n",
    "            test_ids.append(current_str)\n",
    "            test_text.append(text[index])\n",
    "            test_characters.append(characters[index])\n",
    "            test_label1.append(label1[index])\n",
    "            test_label2.append(label2[index])\n",
    "            test_label3.append(label3[index])\n",
    "            test_label4.append(label4[index])\n",
    "            test_label5.append(label5[index])\n",
    "            test_label6.append(label6[index])\n",
    "        else:\n",
    "            train_ids.append(current_str)\n",
    "            train_text.append(text[index])\n",
    "            train_characters.append(characters[index])\n",
    "            train_label1.append(label1[index])\n",
    "            train_label2.append(label2[index])\n",
    "            train_label3.append(label3[index])\n",
    "            train_label4.append(label4[index])\n",
    "            train_label5.append(label5[index])\n",
    "            train_label6.append(label6[index])\n",
    "\n",
    "    train_dataset = ClassificationDataset(train_text,train_characters,train_label1,train_label2,train_label3,train_label4,train_label5,train_label6,maxlen=250,flag=True)\n",
    "    test_dataset = ClassificationDataset(test_text,test_characters,test_label1,test_label2,test_label3,test_label4,test_label5,test_label6,maxlen=250,flag=False)\n",
    "    #到里面的classificationdataset才进行字符的切割以及划分\n",
    "    train_loader = DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=16)\n",
    "    \n",
    "    from loader_pretrain_weights import load_bert_data\n",
    "    bertmodel = load_bert_data(bertmodel,'/home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth')\n",
    "    model = ClassificationModel(bertmodel,config,n_label)\n",
    "    #初始化模型:易漏\n",
    "    \n",
    "    import torch.nn.functional as F\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model = model.to(device)\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=9e-6)\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=9e-6)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=9e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    for epoch in range(4):\n",
    "        print('epoch {}'.format(epoch))\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        model = model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        loss_fn = torch.nn.MSELoss(reduce=True,size_average=True)\n",
    "\n",
    "        for batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(train_loader):\n",
    "            torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "            #print('batch_token_ids')\n",
    "            #print(batch_token_ids)\n",
    "            batch_token_ids = batch_token_ids.to(device)\n",
    "            batch_labels = [batch_label1.numpy(),batch_label2.numpy(),batch_label3.numpy(),\\\n",
    "                            batch_label4.numpy(),batch_label5.numpy(),batch_label6.numpy()]\n",
    "            batch_labels = torch.tensor(batch_labels,dtype=torch.float)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            #for index in range(len(batch_labels)):\n",
    "            #    batch_labels[index] = batch_labels[index].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_multilabel_loss(batch_token_ids,model,batch_labels)\n",
    "            train_loss = train_loss+loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "        print('Train Loss: {:.6f}'.format(train_loss))\n",
    "\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        eval_true_label = [[],[],[],[],[],[]]\n",
    "        eval_predict_label = [[],[],[],[],[],[]]\n",
    "\n",
    "        eval_label_loss = [[0,0,0,0],\\\n",
    "                           [0,0,0,0],\\\n",
    "                           [0,0,0,0],\\\n",
    "                           [0,0,0,0]]\n",
    "        #for batch_token_ids,batch_labels in tqdm(test_loader,bar_format='{l_bar}%s{bar}%s{r_bar}' % (Fore.BLUE, Fore.RESET)):\n",
    "        for batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(test_loader):\n",
    "            batch_token_ids = batch_token_ids.to(device)\n",
    "            batch_labels = [batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6]\n",
    "            with torch.no_grad():\n",
    "                output = model(batch_token_ids)\n",
    "            for index in range(len(output)):\n",
    "                #总的数据\n",
    "                for index1 in range(len(output[index])):\n",
    "                    #对应的类别概率0~6\n",
    "                    abs0 = abs(output[index][index1]-0)\n",
    "                    abs1 = abs(output[index][index1]-1)\n",
    "                    abs2 = abs(output[index][index1]-2)\n",
    "                    abs3 = abs(output[index][index1]-3)\n",
    "                    currentdata = [abs0,abs1,abs2,abs3]\n",
    "                    current_label = currentdata.index(min(currentdata))\n",
    "                    #eval_predict_label[index1].append(current_label)\n",
    "                    current_predict = output[index][index1].item()\n",
    "                    if current_predict < 0.00:\n",
    "                        current_predict = 0\n",
    "                    elif current_predict > 3:\n",
    "                        current_predict = 3\n",
    "                    #当前类别的分类结果,这里append(output[index][index1])\n",
    "                    #直接放入对应概率值时效果最好,这里直接放入0,1,2,3对应的数值\n",
    "                    eval_predict_label[index1].append(current_predict)\n",
    "            for index in range(len(batch_labels)):\n",
    "                current_label = np.array(batch_labels[index].cpu()).tolist()\n",
    "                eval_true_label[index].extend(current_label)\n",
    "        criterion = nn.MSELoss()\n",
    "        totalloss = 0\n",
    "\n",
    "        for index in range(len(eval_true_label)):\n",
    "            inputs = torch.autograd.Variable(torch.from_numpy(np.array(eval_predict_label[index])))\n",
    "            target = torch.autograd.Variable(torch.from_numpy(np.array(eval_true_label[index])))\n",
    "            for index1 in range(len(inputs)):\n",
    "                abs0 = abs(inputs[index1]-0)\n",
    "                abs1 = abs(inputs[index1]-1)\n",
    "                abs2 = abs(inputs[index1]-2)\n",
    "                abs3 = abs(inputs[index1]-3)\n",
    "                currentdata = [abs0,abs1,abs2,abs3]\n",
    "                current_label = currentdata.index(min(currentdata))\n",
    "                true_label = target[index1].item()\n",
    "                if current_label != true_label:\n",
    "                    eval_label_loss[true_label][current_label] = eval_label_loss[true_label][current_label]+1\n",
    "                    #对的预测为错误的\n",
    "            current_loss = criterion(inputs.float(),target.float())\n",
    "            current_loss = current_loss.item()\n",
    "            print('index = %d,current_loss = %f'%(index,current_loss))\n",
    "            totalloss = totalloss+current_loss\n",
    "\n",
    "        #totalloss = totalloss/len(eval_predict_label)\n",
    "        print('totalloss = ')\n",
    "        print(totalloss)\n",
    "        totalloss = totalloss/6\n",
    "        totalloss = math.sqrt(totalloss)\n",
    "        currentpoint = 1/(1+totalloss)\n",
    "        #currentpoint = 1/(1+current_loss)\n",
    "        print('current_point = ')\n",
    "        print(currentpoint)\n",
    "        if currentpoint >= bestpoint[current_split]:\n",
    "            bestpoint[current_split] = currentpoint\n",
    "            torch.save(model,'best_score='+str(bestpoint[current_split])+'split='+str(current_split)+'.pth')\n",
    "        print('eval_label_loss = ')\n",
    "        print(eval_label_loss)\n",
    "\n",
    "test=pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/数据/test_dataset.tsv', sep='\\t')\n",
    "\n",
    "content_dict = {}\n",
    "for index in range(len(test['content'])):\n",
    "    currentindex = test['id'][index]\n",
    "    currentindex = currentindex.split('_')\n",
    "    str1 = currentindex[0]\n",
    "    while len(str1) < 5:\n",
    "        str1 = '0'+str1\n",
    "    str4 = currentindex[3]\n",
    "    while len(str4) < 4:\n",
    "        str4 = '0'+str4\n",
    "    resultindex = str1+'_'+currentindex[1]+'_'+currentindex[2]+'_'+str4\n",
    "    content_dict[resultindex] = test['content'][index]\n",
    "\t\n",
    "def sortedDictValues(adict): \n",
    "    keys = adict.keys() \n",
    "    keys = sorted(keys)\n",
    "    print('keys = ')\n",
    "    print(keys[0:10])\n",
    "    new_content_dict = {}\n",
    "    for data in keys:\n",
    "        new_content_dict[data] = content_dict[data]\n",
    "    return new_content_dict \n",
    "new_content_dict = sortedDictValues(content_dict)\n",
    "\n",
    "inv_new_content_dict = {}\n",
    "for data in new_content_dict:\n",
    "    inv_new_content_dict[new_content_dict[data]] = data\n",
    "\t\n",
    "keys = new_content_dict.keys()\n",
    "keys = list(keys)\n",
    "dict_keys_index = {}\n",
    "for index in range(len(keys)):\n",
    "    #dict_keys_index[index] = keys[index]\n",
    "    dict_keys_index[keys[index]] = index\n",
    "\t\n",
    "content,emotions = [],[]\n",
    "dicts = {}\n",
    "inv_dicts = {}\n",
    "label_num = 0\n",
    "characters = []\n",
    "for index in range(len(test['content'])):\n",
    "        content.append(test['content'][index])\n",
    "        characters.append(test['character'][index])\n",
    "\t\t\n",
    "testtext = test['content']\n",
    "testid = test['id']\n",
    "testcharacter = test['character']\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,character,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            #print('current_text:\\t', current_text, len(current_text))\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "            current_str_index_bak = inv_new_content_dict[current_text]\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            current_key_index_bak = dict_keys_index[current_str_index]\n",
    "            # 同一幕中所有上文拼在一起\n",
    "            lcs_lst = []\n",
    "\n",
    "            pre_current_key_index = current_key_index\n",
    "            pre_current_key_text = current_text\n",
    "            pre_content = ''\n",
    "            #前面一个语句对应的索引以及文本内容\n",
    "            num = 0\n",
    "            while True:\n",
    "                new_current_key_index = pre_current_key_index\n",
    "                new_current_key_text = pre_current_key_text\n",
    "                while new_current_key_index != -1 and new_content_dict[keys[new_current_key_index]] == new_current_key_text:\n",
    "                    new_current_key_index = new_current_key_index-1\n",
    "                if new_current_key_index == -1:\n",
    "                    pre_content = ''\n",
    "                    break\n",
    "                # 只在同一幕中找\n",
    "                new_str_index = keys[new_current_key_index]\n",
    "                current_str_indexs = current_str_index.split('_')\n",
    "                new_str_indexs = new_str_index.split('_')\n",
    "                # 跨剧本，停止寻找\n",
    "                if current_str_indexs[0] != new_str_indexs[0]:\n",
    "                    break\n",
    "                # 跨幕，停止寻找\n",
    "                if current_str_indexs[1] != new_str_indexs[1]:\n",
    "                    break\n",
    "                new_pre_content = new_content_dict[keys[new_current_key_index]]\n",
    "                if str(new_pre_content) == 'nan':\n",
    "                    new_pre_content = '无'\n",
    "                if str(current_character) == 'nan':\n",
    "                    break\n",
    "                #if num == 0 or current_character in new_pre_content:\n",
    "                lcs_lst.append(new_pre_content)\n",
    "                num = num+1\n",
    "                pre_current_key_index = new_current_key_index\n",
    "                pre_current_key_text = new_pre_content\n",
    "                if num == 5:\n",
    "                    break\n",
    "            lcs_lst.reverse()\n",
    "            if not lcs_lst:\n",
    "                pre_content = '无'\n",
    "            else:\n",
    "                pre_content = ''.join(lcs_lst)\n",
    "            # 统计有效上文长度\n",
    "            \n",
    "            #print('pre_content = ')\n",
    "            #print(pre_content)\n",
    "            #print('current_text = ')\n",
    "            #print(current_text)\n",
    "            \n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            current_character_token = tokenizer.tokenize(current_character)\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            \n",
    "            #current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #for data1 in lcs_lst:\n",
    "            #    new_token = tokenizer.tokenize(data1)\n",
    "            #    current_token = current_token+new_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"] + current_token + [\"[SEP]\"] + current_character_token + [\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+current_character_token+[\"[SEP]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]\n",
    "            #if len(current_token) > 1:\n",
    "                #print('pre_text:\\t', pre_content, len(pre_content))\n",
    "                #pre_content_lengths.append(len(current_token))\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        #self.segment_id = sequence_padding(self.segment_id,maxlen)\n",
    "        #self.mask_id = sequence_padding(self.mask_id,maxlen)\n",
    "        self.tensors = [torch.tensor(self.token_id,dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            #print('inputs = ')\n",
    "            #print(len(inputs))\n",
    "            abondon = len(inputs)-length\n",
    "            inputs = inputs[0:2]+inputs[3+abondon-1:]\n",
    "            #print('now current_token = ')\n",
    "            #print(len(inputs))\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x\n",
    "\t\t\n",
    "test_dataset = TestDataset(testtext,testcharacter,maxlen=250)\n",
    "test_loader = DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
    "\n",
    "for current_split in range(split_n):\n",
    "    model = torch.load('best_score='+str(bestpoint[current_split])+'split='+str(current_split)+'.pth')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    eval_predict_label = []\n",
    "    index = []\n",
    "    pred = [[],[],[],[],[],[]]\n",
    "    current_index = 0\n",
    "    for batch_token_ids in tqdm(test_loader):\n",
    "        batch_token_ids = batch_token_ids[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(batch_token_ids)\n",
    "            for index in range(len(output)):\n",
    "                #总的数据\n",
    "                for index1 in range(len(output[index])):\n",
    "                    #对应的类别概率0~6\n",
    "                    abs0 = abs(output[index][index1]-0)\n",
    "                    abs1 = abs(output[index][index1]-1)\n",
    "                    abs2 = abs(output[index][index1]-2)\n",
    "                    abs3 = abs(output[index][index1]-3)\n",
    "                    currentdata = [abs0,abs1,abs2,abs3]\n",
    "                    current_label = currentdata.index(min(currentdata))\n",
    "                    #eval_predict_label[index1].append(current_label)\n",
    "                    current_predict = output[index][index1].item()\n",
    "\n",
    "                    if current_predict < 0.00:\n",
    "                        current_predict = 0.0\n",
    "                    elif current_predict > 3:\n",
    "                        current_predict = 3.0\n",
    "\n",
    "                    pred[index1].append(current_predict)\n",
    "                    #eval_predict_label[index1].append(current_predict)\n",
    "                    #当前类别的分类结果,这里append(output[index][index1])\n",
    "                    #直接放入对应概率值时效果最好,这里直接放入0,1,2,3对应的数值\n",
    "    for index in range(len(pred[0])):\n",
    "        eval_predict_label.append(str(pred[0][index])+','+str(pred[1][index])+','+str(pred[2][index])+','+str(pred[3][index])+','+str(pred[4][index])+','+str(pred[5][index]))\n",
    "    result_data = []\n",
    "    for index in range(len(testid)):\n",
    "        result_data.append([testid[index],eval_predict_label[index]])\n",
    "    #pd.DataFrame({\"id\":testid,\"label\":eval_predict_label}).to_csv(\"/home/xiaoguzai/代码/剧本角色情感识别/数据集/crossentropy\"+str(bestpoint)+\"result.csv\",index=False)\n",
    "    import csv\n",
    "    with open(r'/home/xiaoguzai/程序/剧本角色情感识别/训练数据/多折叠_上文+同一人物说过的两句话_nezha'+str(seed)+'_best_point='+str(bestpoint[current_split])+\"result.csv\", 'w') as f:\n",
    "        tsv_w = csv.writer(f, delimiter='\\t')\n",
    "        tsv_w.writerow(['id', 'emotion'])  # 单行写入\n",
    "        tsv_w.writerows(result_data)  # 多行写入\n",
    "\n",
    "#模型融合\n",
    "import pandas as pd\n",
    "result = []\n",
    "for current_split in range(split_n):\n",
    "    current_result = pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/训练数据/多折叠_上文+同一人物说过的两句话_nezha'+str(seed)+'_best_point='+str(bestpoint[current_split])+\"result.csv\",sep='\\t')\n",
    "    result.append(current_result)\n",
    "\n",
    "final_result = [[],[],[],[],[],[]]\n",
    "test_id = []\n",
    "for index in range(len(result[0]['emotion'])):\n",
    "    test_id.append(result[0]['id'][index])\n",
    "    currentdata = [0,0,0,0,0,0]\n",
    "    for index1 in range(split_n):\n",
    "        newdata = result[index1]['emotion'][index].split(',')\n",
    "        currentdata[0] = currentdata[0]+float(newdata[0])\n",
    "        currentdata[1] = currentdata[1]+float(newdata[1])\n",
    "        currentdata[2] = currentdata[2]+float(newdata[2])\n",
    "        currentdata[3] = currentdata[3]+float(newdata[3])\n",
    "        currentdata[4] = currentdata[4]+float(newdata[4])\n",
    "        currentdata[5] = currentdata[5]+float(newdata[5])\n",
    "    currentdata[0] = currentdata[0]/split_n\n",
    "    currentdata[1] = currentdata[1]/split_n\n",
    "    currentdata[2] = currentdata[2]/split_n\n",
    "    currentdata[3] = currentdata[3]/split_n\n",
    "    currentdata[4] = currentdata[4]/split_n\n",
    "    currentdata[5] = currentdata[5]/split_n\n",
    "    #final_result.append(currentdata)\n",
    "    final_result[0].append(currentdata[0])\n",
    "    final_result[1].append(currentdata[1])\n",
    "    final_result[2].append(currentdata[2])\n",
    "    final_result[3].append(currentdata[3])\n",
    "    final_result[4].append(currentdata[4])\n",
    "    final_result[5].append(currentdata[5])\n",
    "eval_predict_label = []\n",
    "for index in range(len(pred[0])):\n",
    "    eval_predict_label.append(str(final_result[0][index])+','+str(final_result[1][index])+','+str(final_result[2][index]) \\\n",
    "                               +','+str(final_result[3][index])+','+str(final_result[4][index])+','+str(final_result[5][index]))\n",
    "result = []\n",
    "for index in range(len(test_id)):\n",
    "    result.append([test_id[index],eval_predict_label[index]])\n",
    "with open(r'/home/xiaoguzai/程序/剧本角色情感识别/训练数据/多折去掉主语加上一句'+str(split_n)+'折融合nezha+seed='+str(seed)+'.csv','w') as f:\n",
    "    csv_w = csv.writer(f,delimiter='\\t')\n",
    "    csv_w.writerow(['id','emotion'])\n",
    "    csv_w.writerows(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c3df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
