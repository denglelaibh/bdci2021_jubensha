{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "painted-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/数据/train_dataset_v2.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "built-methodology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0,0,0,0,0,0\n",
      "1        0,0,0,0,0,0\n",
      "2        0,0,0,0,0,0\n",
      "3        0,0,0,0,0,0\n",
      "4        0,0,0,0,0,0\n",
      "            ...     \n",
      "42785    0,0,0,0,0,0\n",
      "42786    0,3,0,0,0,0\n",
      "42787    2,3,0,0,0,0\n",
      "42788    2,3,0,0,0,0\n",
      "42789    0,0,0,0,0,0\n",
      "Name: emotions, Length: 42790, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train['emotions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29beb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = {}\n",
    "for index in range(len(train['content'])):\n",
    "    currentindex = train['id'][index]\n",
    "    currentindex = currentindex.split('_')\n",
    "    str1 = currentindex[0]\n",
    "    while len(str1) < 5:\n",
    "        str1 = '0'+str1\n",
    "    str4 = currentindex[3]\n",
    "    while len(str4) < 4:\n",
    "        str4 = '0'+str4\n",
    "    resultindex = str1+'_'+currentindex[1]+'_'+currentindex[2]+'_'+str4\n",
    "    train['id'][index] = currentindex\n",
    "    content_dict[resultindex] = train['content'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ffcfb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = \n",
      "['01171_0001_A_0001', '01171_0001_A_0002', '01171_0001_A_0003', '01171_0001_A_0004', '01171_0001_A_0005', '01171_0001_A_0006', '01171_0001_A_0007', '01171_0001_A_0008', '01171_0001_A_0009', '01171_0001_A_0010']\n"
     ]
    }
   ],
   "source": [
    "def sortedDictValues(adict): \n",
    "    keys = adict.keys() \n",
    "    keys = sorted(keys)\n",
    "    print('keys = ')\n",
    "    print(keys[0:10])\n",
    "    new_content_dict = {}\n",
    "    for data in keys:\n",
    "        new_content_dict[data] = content_dict[data]\n",
    "    return new_content_dict \n",
    "new_content_dict = sortedDictValues(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e938cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_new_content_dict = {}\n",
    "for data in new_content_dict:\n",
    "    inv_new_content_dict[new_content_dict[data]] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182ba1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = new_content_dict.keys()\n",
    "keys = list(keys)\n",
    "dict_keys_index = {}\n",
    "for index in range(len(keys)):\n",
    "    #dict_keys_index[index] = keys[index]\n",
    "    dict_keys_index[keys[index]] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "differential-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "content,emotions = [],[]\n",
    "dicts = {}\n",
    "inv_dicts = {}\n",
    "label_num = 0\n",
    "characters = []\n",
    "ids = []\n",
    "label1,label2,label3,label4,label5,label6 = [],[],[],[],[],[]\n",
    "for index in range(len(train['content'])):\n",
    "    if pd.isna(train['emotions'][index]) == False:\n",
    "        ids.append(train['id'][index])\n",
    "        content.append(train['content'][index])\n",
    "        emotions.append(train['emotions'][index])\n",
    "        characters.append(train['character'][index])\n",
    "        current_emotion = train['emotions'][index].split(',')\n",
    "        label1.append(int(current_emotion[0]))\n",
    "        label2.append(int(current_emotion[1]))\n",
    "        label3.append(int(current_emotion[2]))\n",
    "        label4.append(int(current_emotion[3]))\n",
    "        label5.append(int(current_emotion[4]))\n",
    "        label6.append(int(current_emotion[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ahead-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = set(label1)\n",
    "dict1,dict2,dict3,dict4,dict5,dict6 = {},{},{},{},{},{}\n",
    "for item in set1:\n",
    "    dict1.update({item:label1.count(item)})\n",
    "set2 = set(label2)\n",
    "for item in set2:\n",
    "    dict2.update({item:label2.count(item)})\n",
    "set3 = set(label3)\n",
    "for item in set3:\n",
    "    dict3.update({item:label3.count(item)})\n",
    "set4 = set(label4)\n",
    "for item in set4:\n",
    "    dict4.update({item:label4.count(item)})\n",
    "set5 = set(label5)\n",
    "for item in set5:\n",
    "    dict5.update({item:label5.count(item)})\n",
    "set6 = set(label6)\n",
    "for item in set6:\n",
    "    dict6.update({item:label6.count(item)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a3ccf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 35500, 1: 527, 2: 346, 3: 409}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4e4ee0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 34014, 1: 2057, 2: 479, 3: 232}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0667309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 34568, 1: 1335, 2: 593, 3: 286}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c109a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 33009, 1: 1998, 2: 1197, 3: 578}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b05c9546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 33821, 1: 1541, 2: 1012, 3: 408}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "079c647e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 31004, 1: 2821, 2: 1983, 3: 974}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "moving-hollywood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---__init__ Nezha\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "from bertmodels import Config\n",
    "from nezha import Bert\n",
    "vocab_file = r'/home/xiaoguzai/模型/nezha-base/vocab.txt'\n",
    "vocab_size = len(open(vocab_file,'r').readlines()) \n",
    "with open('/home/xiaoguzai/模型/nezha-base/config.json','r',encoding='utf8')as fp:\n",
    "    json_data = json.load(fp)\n",
    "json_data['vocab_size'] = vocab_size\n",
    "config = Config(**json_data)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tokenization import FullTokenizer\n",
    "import numpy as np\n",
    "tokenizer = FullTokenizer(vocab_file=vocab_file)\n",
    "from tqdm import tqdm\n",
    "config.with_mlm = False\n",
    "#config.with_pooler = True\n",
    "bertmodel = Bert(config)\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b6724eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e83ee32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555load Pytorch model555\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tmodule.mlm_dense1.bias\n",
      "\tmodule.mlm_dense1.weight\n",
      "\tmodule.mlm_dense0.weight\n",
      "\tmodule.mlm_norm.bias\n",
      "\tmodule.mlm_dense0.bias\n",
      "\tmodule.mlm_norm.weight\n"
     ]
    }
   ],
   "source": [
    "from loader_pretrain_weights import load_bert_data\n",
    "bertmodel = load_bert_data(bertmodel,'/home/xiaoguzai/程序/剧本角色情感识别/预训练文件/labeled_data+model_epoch=60.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e79ee3",
   "metadata": {},
   "source": [
    "from loader_nezha import load_bert_data\n",
    "bertmodel = load_bert_data(bertmodel,'/home/xiaoguzai/模型/bert/nezha-base/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d658a",
   "metadata": {},
   "source": [
    "from loader_pretrain_weights import load_bert_data\n",
    "bertmodel = load_bert_data(bertmodel,'/home/xiaoguzai/数据集/剧本角色识别/labeled_data+model_epoch=60.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "descending-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self,text,characters,label1,label2,label3,label4,label5,label6,maxlen,flag):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        self.characters = characters\n",
    "        #self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            #print('current_text:\\t', current_text, len(current_text))\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "            current_str_index_bak = inv_new_content_dict[current_text]\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            current_key_index_bak = dict_keys_index[current_str_index]\n",
    "            # 同一幕中所有上文拼在一起\n",
    "            lcs_lst = []\n",
    "\n",
    "            pre_current_key_index = current_key_index\n",
    "            pre_current_key_text = current_text\n",
    "            pre_content = ''\n",
    "            #前面一个语句对应的索引以及文本内容\n",
    "            num = 0\n",
    "            while True:\n",
    "                new_current_key_index = pre_current_key_index\n",
    "                new_current_key_text = pre_current_key_text\n",
    "                while new_current_key_index != -1 and new_content_dict[keys[new_current_key_index]] == new_current_key_text:\n",
    "                    new_current_key_index = new_current_key_index-1\n",
    "                if new_current_key_index == -1:\n",
    "                    pre_content = ''\n",
    "                    break\n",
    "                # 只在同一幕中找\n",
    "                new_str_index = keys[new_current_key_index]\n",
    "                current_str_indexs = current_str_index.split('_')\n",
    "                new_str_indexs = new_str_index.split('_')\n",
    "                # 跨剧本，停止寻找\n",
    "                if current_str_indexs[0] != new_str_indexs[0]:\n",
    "                    break\n",
    "                # 跨幕，停止寻找\n",
    "                if current_str_indexs[1] != new_str_indexs[1]:\n",
    "                    break\n",
    "                new_pre_content = new_content_dict[keys[new_current_key_index]]\n",
    "                if str(new_pre_content) == 'nan':\n",
    "                    new_pre_content = '无'\n",
    "                if str(current_character) == 'nan':\n",
    "                    break\n",
    "                if num == 0 or current_character in new_pre_content:\n",
    "                    lcs_lst.append(new_pre_content)\n",
    "                    num = num+1\n",
    "                pre_current_key_index = new_current_key_index\n",
    "                pre_current_key_text = new_pre_content\n",
    "                if num == 3:\n",
    "                    break\n",
    "            lcs_lst.reverse()\n",
    "            if not lcs_lst:\n",
    "                pre_content = '无'\n",
    "            else:\n",
    "                pre_content = ''.join(lcs_lst)\n",
    "            # 统计有效上文长度\n",
    "            \n",
    "            #print('pre_content = ')\n",
    "            #print(pre_content)\n",
    "            #print('current_text = ')\n",
    "            #print(current_text)\n",
    "            \n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            current_character_token = tokenizer.tokenize(current_character)\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            \n",
    "            #current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #for data1 in lcs_lst:\n",
    "            #    new_token = tokenizer.tokenize(data1)\n",
    "            #    current_token = current_token+new_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"] + current_token + [\"[SEP]\"] + current_character_token + [\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #if len(current_token) > 1:\n",
    "                #print('pre_text:\\t', pre_content, len(pre_content))\n",
    "                #pre_content_lengths.append(len(current_token))\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        self.tensors = [torch.tensor(self.token_id,dtype=torch.long),\n",
    "                 torch.tensor(self.label[0],dtype=torch.long),\n",
    "                 torch.tensor(self.label[1],dtype=torch.long),\n",
    "                 torch.tensor(self.label[2],dtype=torch.long),\n",
    "                 torch.tensor(self.label[3],dtype=torch.long),\n",
    "                 torch.tensor(self.label[4],dtype=torch.long),\n",
    "                 torch.tensor(self.label[5],dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "\n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length]\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sealed-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = content\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "r\"\"\"\n",
    "ss = StratifiedKFold(n_splits=5,shuffle=True,random_state=2)\n",
    "#建立4折交叉验证方法 查一下KFold函数的参数\n",
    "text = np.array(text)\n",
    "label1 = np.array(label1)\n",
    "label2 = np.array(label2)\n",
    "label3 = np.array(label3)\n",
    "label4 = np.array(label4)\n",
    "label5 = np.array(label5)\n",
    "label6 = np.array(label6)\n",
    "characters = np.array(characters)\n",
    "for train_index,test_index in ss.split(text,label1):\n",
    "    train_text = text[np.array(train_index)]\n",
    "    test_text = text[test_index]\n",
    "    train_ids = np.array(ids)[train_index]\n",
    "    test_ids = np.array(ids)[test_index]\n",
    "    train_characters = characters[train_index]\n",
    "    test_characters = characters[test_index]\n",
    "    train_label1 = label1[train_index]\n",
    "    test_label1 = label1[test_index]\n",
    "    train_label2 = label2[train_index]\n",
    "    test_label2 = label2[test_index]\n",
    "    train_label3 = label3[train_index]\n",
    "    test_label3 = label3[test_index]\n",
    "    train_label4 = label4[train_index]\n",
    "    test_label4 = label4[test_index]\n",
    "    train_label5 = label5[train_index]\n",
    "    test_label5 = label5[test_index]\n",
    "    train_label6 = label6[train_index]\n",
    "    test_label6 = label6[test_index]\n",
    "\"\"\"\n",
    "train_text,test_text,train_ids,test_ids = [],[],[],[]\n",
    "train_characters,test_characters = [],[]\n",
    "train_label1,train_label2,train_label3,train_label4,train_label5,train_label6 = [],[],[],[],[],[]\n",
    "test_label1,test_label2,test_label3,test_label4,test_label5,test_label6 = [],[],[],[],[],[]\n",
    "for index in range(len(ids)):\n",
    "    current_id = ids[index]\n",
    "    current_str = ''\n",
    "    for data1 in current_id:\n",
    "        current_str = current_str+data1\n",
    "    if current_id[0] == '34940' or current_id[0] == '34945' or current_id[0] == '34946' or current_id[0] == '34949':\n",
    "        test_ids.append(current_str)\n",
    "        test_text.append(text[index])\n",
    "        test_characters.append(characters[index])\n",
    "        test_label1.append(label1[index])\n",
    "        test_label2.append(label2[index])\n",
    "        test_label3.append(label3[index])\n",
    "        test_label4.append(label4[index])\n",
    "        test_label5.append(label5[index])\n",
    "        test_label6.append(label6[index])\n",
    "    else:\n",
    "        train_ids.append(current_str)\n",
    "        train_text.append(text[index])\n",
    "        train_characters.append(characters[index])\n",
    "        train_label1.append(label1[index])\n",
    "        train_label2.append(label2[index])\n",
    "        train_label3.append(label3[index])\n",
    "        train_label4.append(label4[index])\n",
    "        train_label5.append(label5[index])\n",
    "        train_label6.append(label6[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "264eed51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3468"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "121c7ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 33314/33314 [00:07<00:00, 4277.56it/s]\n",
      "100%|█████████████████████████████████████| 3468/3468 [00:00<00:00, 3958.44it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ClassificationDataset(train_text,train_characters,train_label1,train_label2,train_label3,train_label4,train_label5,train_label6,maxlen=200,flag=True)\n",
    "test_dataset = ClassificationDataset(test_text,test_characters,test_label1,test_label2,test_label3,test_label4,test_label5,test_label6,maxlen=200,flag=False)\n",
    "#到里面的classificationdataset才进行字符的切割以及划分\n",
    "train_loader = DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "portuguese-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model,config,n_labels):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        #self.embedding = nn.Embedding(30522,768)\n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(config.embedding_size,128)\n",
    "        self.activation = F.relu\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #self.activation = F.tanh\n",
    "        self.fc2 = nn.Linear(128,n_labels)\n",
    "        \n",
    "    def forward(self,input_ids,segment_ids,input_mask):\n",
    "        #outputs = self.embedding(input_ids)\n",
    "        output = self.model(input_ids)\n",
    "        #[64,128,768]\n",
    "        output = self.dropout(output)\n",
    "        output = output[:,0]\n",
    "        output = self.fc1(output)\n",
    "        output = self.activation(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    #之前这里少量return outputs返回值为None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f34fb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multilabel_loss(x,model,label):\n",
    "    logit = model(x,None,None)\n",
    "    mseloss = 0\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    logit = torch.transpose(logit, 0, 1)\n",
    "    mseloss = loss_fn(logit,label)\n",
    "    return mseloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02293b0",
   "metadata": {},
   "source": [
    "def compute_multilabel_loss(x,model,label):\n",
    "    logit = model(x,None,None)\n",
    "    mseloss = 0\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True, size_average=True)\n",
    "    logit = torch.transpose(logit, 0, 1)\n",
    "    mseloss = 0\n",
    "    totaltimes = logit.shape[1]\n",
    "    for index in range(len(logit)):\n",
    "        for index1 in range(len(logit[index])):\n",
    "            mseloss = mseloss+(logit[index][index1]-label[index][index1])**2\n",
    "            if index == 5 and (label[index][index1] == 2 or label[index][index1] == 3):\n",
    "                mseloss = mseloss+(logit[index][index1]-label[index][index1])**2\n",
    "        #currentloss = loss_fn(logit,label)\n",
    "        #if index == 5:\n",
    "            #mseloss = mseloss+0.6*currentloss\n",
    "    #mseloss = loss_fn(logit,label)\n",
    "    return mseloss/totaltimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "seventh-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader_bert import load_bert_data\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore\n",
    "n_label = 6\n",
    "model = ClassificationModel(bertmodel,config,n_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "prepared-layout",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 9e-06\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoguzai/.local/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "100%|███████████████████████████████████████| 2083/2083 [04:23<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个epoch的学习率：0.000009\n",
      "Train Loss: 409.431610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 217/217 [00:16<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0,current_loss = 0.075547\n",
      "index = 1,current_loss = 0.161768\n",
      "index = 2,current_loss = 0.122997\n",
      "index = 3,current_loss = 0.231182\n",
      "index = 4,current_loss = 0.300620\n",
      "index = 5,current_loss = 0.285307\n",
      "totalloss = \n",
      "1.1774215325713158\n",
      "current_point = \n",
      "0.6930072399051398\n",
      "eval_label_loss = \n",
      "[[0, 1148, 58, 0], [1361, 0, 60, 0], [279, 204, 0, 0], [34, 47, 31, 0]]\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████████████████████▏                 | 1133/2083 [02:26<02:02,  7.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14142/4289352210.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#    batch_labels[index] = batch_labels[index].to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_multilabel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_token_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14142/3796390048.py\u001b[0m in \u001b[0;36mcompute_multilabel_loss\u001b[0;34m(x, model, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_multilabel_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmseloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14142/1229484056.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, segment_ids, input_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#outputs = self.embedding(input_ids)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#[64,128,768]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/程序/剧本角色情感识别/程序/nezha.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, segment_ids, mask_ids)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m#outputs = self.bert_encoder_layer[0](outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_ndx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_encoder_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_ndx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;31m#print('outputs1 = ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m#print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/程序/剧本角色情感识别/程序/nezha.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, masks, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/程序/剧本角色情感识别/程序/nezha.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, mask, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_per_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = model.to(device)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=9e-6)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=9e-6)\n",
    "def lr_lambda(epoch):\n",
    "    if epoch > 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2/(epoch+1)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "bestpoint = 0.0\n",
    "for epoch in range(5):\n",
    "    print('epoch {}'.format(epoch))\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    loss_fn = torch.nn.MSELoss(reduce=True,size_average=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(train_loader):\n",
    "        torch.set_printoptions(edgeitems=768)# 设置输出矩阵维度为768\n",
    "        #print('batch_token_ids')\n",
    "        #print(batch_token_ids)\n",
    "        batch_token_ids = batch_token_ids.to(device)\n",
    "        batch_labels = [batch_label1.numpy(),batch_label2.numpy(),batch_label3.numpy(),\\\n",
    "                        batch_label4.numpy(),batch_label5.numpy(),batch_label6.numpy()]\n",
    "        batch_labels = torch.tensor(batch_labels,dtype=torch.float)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        #for index in range(len(batch_labels)):\n",
    "        #    batch_labels[index] = batch_labels[index].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_multilabel_loss(batch_token_ids,model,batch_labels)\n",
    "        train_loss = train_loss+loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "    #注意必须从epoch=1开始，否则第0个没有学习率\n",
    "    print('Train Loss: {:.6f}'.format(train_loss))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    eval_true_label = [[],[],[],[],[],[]]\n",
    "    eval_predict_label = [[],[],[],[],[],[]]\n",
    "    \n",
    "    eval_label_loss = [[0,0,0,0],\\\n",
    "                       [0,0,0,0],\\\n",
    "                       [0,0,0,0],\\\n",
    "                       [0,0,0,0]]\n",
    "    #for batch_token_ids,batch_labels in tqdm(test_loader,bar_format='{l_bar}%s{bar}%s{r_bar}' % (Fore.BLUE, Fore.RESET)):\n",
    "    for batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(test_loader):\n",
    "        batch_token_ids = batch_token_ids.to(device)\n",
    "        batch_labels = [batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6]\n",
    "        with torch.no_grad():\n",
    "            output = model(batch_token_ids,None,None)\n",
    "        for index in range(len(output)):\n",
    "            #总的数据\n",
    "            for index1 in range(len(output[index])):\n",
    "                #对应的类别概率0~6\n",
    "                abs0 = abs(output[index][index1]-0)\n",
    "                abs1 = abs(output[index][index1]-1)\n",
    "                abs2 = abs(output[index][index1]-2)\n",
    "                abs3 = abs(output[index][index1]-3)\n",
    "                currentdata = [abs0,abs1,abs2,abs3]\n",
    "                current_label = currentdata.index(min(currentdata))\n",
    "                #eval_predict_label[index1].append(current_label)\n",
    "                current_predict = output[index][index1].item()\n",
    "                if current_predict < 0.00:\n",
    "                    current_predict = 0\n",
    "                elif current_predict > 3:\n",
    "                    current_predict = 3\n",
    "                #当前类别的分类结果,这里append(output[index][index1])\n",
    "                #直接放入对应概率值时效果最好,这里直接放入0,1,2,3对应的数值\n",
    "                eval_predict_label[index1].append(current_predict)\n",
    "        for index in range(len(batch_labels)):\n",
    "            current_label = np.array(batch_labels[index].cpu()).tolist()\n",
    "            eval_true_label[index].extend(current_label)\n",
    "    criterion = nn.MSELoss()\n",
    "    totalloss = 0\n",
    "\n",
    "    for index in range(len(eval_true_label)):\n",
    "        inputs = torch.autograd.Variable(torch.from_numpy(np.array(eval_predict_label[index])))\n",
    "        target = torch.autograd.Variable(torch.from_numpy(np.array(eval_true_label[index])))\n",
    "        for index1 in range(len(inputs)):\n",
    "            abs0 = abs(inputs[index1]-0)\n",
    "            abs1 = abs(inputs[index1]-1)\n",
    "            abs2 = abs(inputs[index1]-2)\n",
    "            abs3 = abs(inputs[index1]-3)\n",
    "            currentdata = [abs0,abs1,abs2,abs3]\n",
    "            current_label = currentdata.index(min(currentdata))\n",
    "            true_label = target[index1].item()\n",
    "            if current_label != true_label:\n",
    "                eval_label_loss[true_label][current_label] = eval_label_loss[true_label][current_label]+1\n",
    "                #对的预测为错误的\n",
    "        current_loss = criterion(inputs.float(),target.float())\n",
    "        current_loss = current_loss.item()\n",
    "        print('index = %d,current_loss = %f'%(index,current_loss))\n",
    "        totalloss = totalloss+current_loss\n",
    "    \n",
    "    #totalloss = totalloss/len(eval_predict_label)\n",
    "    print('totalloss = ')\n",
    "    print(totalloss)\n",
    "    totalloss = totalloss/6\n",
    "    totalloss = math.sqrt(totalloss)\n",
    "    currentpoint = 1/(1+totalloss)\n",
    "    #currentpoint = 1/(1+current_loss)\n",
    "    print('current_point = ')\n",
    "    print(currentpoint)\n",
    "    if currentpoint > bestpoint:\n",
    "        bestpoint = currentpoint\n",
    "        torch.save(model,'best_score'+str(bestpoint)+'.pth')\n",
    "    print('eval_label_loss = ')\n",
    "    print(eval_label_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d312a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestpoint = 0.6987036931675447\n",
    "model = torch.load('best_score'+str(bestpoint)+'.pth')\n",
    "test=pd.read_csv('/home/xiaoguzai/程序/剧本角色情感识别/数据/test_dataset.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199b6d3",
   "metadata": {},
   "source": [
    "##预测完成之后将结果写入文件之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestClassificationDataset(Dataset):\n",
    "    def __init__(self,ids,text,characters,label1,label2,label3,label4,label5,label6,maxlen,flag):\n",
    "        self.ids = ids\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        self.characters = characters\n",
    "        #self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            #print('current_text:\\t', current_text, len(current_text))\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "            current_str_index_bak = inv_new_content_dict[current_text]\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            current_key_index_bak = dict_keys_index[current_str_index]\n",
    "            # 同一幕中所有上文拼在一起\n",
    "            lcs_lst = []\n",
    "\n",
    "            pre_current_key_index = current_key_index\n",
    "            pre_current_key_text = current_text\n",
    "            pre_content = ''\n",
    "            #前面一个语句对应的索引以及文本内容\n",
    "            num = 0\n",
    "            while True:\n",
    "                new_current_key_index = pre_current_key_index\n",
    "                new_current_key_text = pre_current_key_text\n",
    "                while new_current_key_index != -1 and new_content_dict[keys[new_current_key_index]] == new_current_key_text:\n",
    "                    new_current_key_index = new_current_key_index-1\n",
    "                if new_current_key_index == -1:\n",
    "                    pre_content = ''\n",
    "                    break\n",
    "                # 只在同一幕中找\n",
    "                new_str_index = keys[new_current_key_index]\n",
    "                current_str_indexs = current_str_index.split('_')\n",
    "                new_str_indexs = new_str_index.split('_')\n",
    "                # 跨剧本，停止寻找\n",
    "                if current_str_indexs[0] != new_str_indexs[0]:\n",
    "                    break\n",
    "                # 跨幕，停止寻找\n",
    "                if current_str_indexs[1] != new_str_indexs[1]:\n",
    "                    break\n",
    "                new_pre_content = new_content_dict[keys[new_current_key_index]]\n",
    "                if str(new_pre_content) == 'nan':\n",
    "                    new_pre_content = '无'\n",
    "                if str(current_character) == 'nan':\n",
    "                    break\n",
    "                if num == 0 or current_character in new_pre_content:\n",
    "                    lcs_lst.append(new_pre_content)\n",
    "                    num = num+1\n",
    "                pre_current_key_index = new_current_key_index\n",
    "                pre_current_key_text = new_pre_content\n",
    "                if num == 3:\n",
    "                    break\n",
    "            lcs_lst.reverse()\n",
    "            if not lcs_lst:\n",
    "                pre_content = '无'\n",
    "            else:\n",
    "                pre_content = ''.join(lcs_lst)\n",
    "            # 统计有效上文长度\n",
    "            \n",
    "            #print('pre_content = ')\n",
    "            #print(pre_content)\n",
    "            #print('current_text = ')\n",
    "            #print(current_text)\n",
    "            \n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            current_character_token = tokenizer.tokenize(current_character)\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            \n",
    "            #current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #for data1 in lcs_lst:\n",
    "            #    new_token = tokenizer.tokenize(data1)\n",
    "            #    current_token = current_token+new_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"] + current_token + [\"[SEP]\"] + current_character_token + [\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #if len(current_token) > 1:\n",
    "                #print('pre_text:\\t', pre_content, len(pre_content))\n",
    "                #pre_content_lengths.append(len(current_token))\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        self.label = [label1,label2,label3,label4,label5,label6]\n",
    "        self.tensors = [self.ids,\n",
    "                        self.text,\n",
    "                torch.tensor(self.token_id,dtype=torch.long),\n",
    "                 torch.tensor(self.label[0],dtype=torch.long),\n",
    "                 torch.tensor(self.label[1],dtype=torch.long),\n",
    "                 torch.tensor(self.label[2],dtype=torch.long),\n",
    "                 torch.tensor(self.label[3],dtype=torch.long),\n",
    "                 torch.tensor(self.label[4],dtype=torch.long),\n",
    "                 torch.tensor(self.label[5],dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "\n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length]\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2264cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestClassificationDataset(train_ids,train_text,train_characters,train_label1,train_label2,train_label3,train_label4,train_label5,train_label6,maxlen=200,flag=False)\n",
    "#到里面的classificationdataset才进行字符的切割以及划分\n",
    "test_loader = DataLoader(test_dataset,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e755d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "ids = []\n",
    "text = []\n",
    "predict_label1,predict_label2,predict_label3,predict_label4,predict_label5,predict_label6 = [],[],[],[],[],[]\n",
    "true_label1,true_label2,true_label3,true_label4,true_label5,true_label6 = [],[],[],[],[],[]\n",
    "for batch_test_id,batch_test_text,batch_token_ids,batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6 in tqdm(test_loader):\n",
    "    for data1 in batch_test_id:\n",
    "        ids.append(data1)\n",
    "    #batch_test_text = batch_test_text.tolist()\n",
    "    for text1 in batch_test_text:\n",
    "        text.append(text1)\n",
    "    batch_token_ids = batch_token_ids.to(device)\n",
    "    batch_labels = [batch_label1,batch_label2,batch_label3,batch_label4,batch_label5,batch_label6]\n",
    "    with torch.no_grad():\n",
    "        output = model(batch_token_ids,None,None)\n",
    "    for index in range(len(output)):\n",
    "        predict_label1.append(output[index][0].item())\n",
    "        predict_label2.append(output[index][1].item())\n",
    "        predict_label3.append(output[index][2].item())\n",
    "        predict_label4.append(output[index][3].item())\n",
    "        predict_label5.append(output[index][4].item())\n",
    "        predict_label6.append(output[index][5].item())\n",
    "        true_label1.append(batch_label1[index].item())\n",
    "        true_label2.append(batch_label2[index].item())\n",
    "        true_label3.append(batch_label3[index].item())\n",
    "        true_label4.append(batch_label4[index].item())\n",
    "        true_label5.append(batch_label5[index].item())\n",
    "        true_label6.append(batch_label6[index].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = []\n",
    "for index in range(len(ids)):\n",
    "    predict_data.append([ids[index],text[index],predict_label1[index],\\\n",
    "                        predict_label2[index],predict_label3[index],\\\n",
    "                        predict_label4[index],predict_label5[index],\\\n",
    "                        predict_label6[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbade334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(r'/home/xiaoguzai/程序/剧本角色情感识别/模型融合/对比nezha_base训练集数据'+str(bestpoint)+\"predict_data.csv\", 'w') as f:\n",
    "    tsv_w = csv.writer(f, delimiter='\\t')\n",
    "    tsv_w.writerow(['id', 'text','emotion1',\\\n",
    "                   'emotion2','emotion3','emotion4',\\\n",
    "                   'emotion5','emotion6'])  # 单行写入\n",
    "    tsv_w.writerows(predict_data)  # 多行写入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data = []\n",
    "for index in range(len(ids)):\n",
    "    true_data.append([ids[index],text[index],true_label1[index],\\\n",
    "                     true_label2[index],true_label3[index],\\\n",
    "                     true_label4[index],true_label5[index],\\\n",
    "                     true_label6[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(r'/home/xiaoguzai/程序/剧本角色情感识别/模型融合/对比nezha_base训练集数据'+str(bestpoint)+\"true_data.csv\", 'w') as f:\n",
    "    tsv_w = csv.writer(f, delimiter='\\t')\n",
    "    tsv_w.writerow(['id', 'text','emotion1',\\\n",
    "                   'emotion2','emotion3','emotion4',\\\n",
    "                   'emotion5','emotion6'])  # 单行写入\n",
    "    tsv_w.writerows(true_data)  # 多行写入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e21d7",
   "metadata": {},
   "source": [
    "## 开始新的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eabde2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = {}\n",
    "for index in range(len(test['content'])):\n",
    "    currentindex = test['id'][index]\n",
    "    currentindex = currentindex.split('_')\n",
    "    str1 = currentindex[0]\n",
    "    while len(str1) < 5:\n",
    "        str1 = '0'+str1\n",
    "    str4 = currentindex[3]\n",
    "    while len(str4) < 4:\n",
    "        str4 = '0'+str4\n",
    "    resultindex = str1+'_'+currentindex[1]+'_'+currentindex[2]+'_'+str4\n",
    "    content_dict[resultindex] = test['content'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd00144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = \n",
      "['01597_0001_A_0001', '01597_0001_A_0002', '01597_0001_A_0003', '01597_0001_A_0004', '01597_0001_A_0005', '01597_0001_A_0006', '01597_0001_A_0007', '01597_0001_A_0008', '01597_0001_A_0009', '01597_0001_A_0010']\n"
     ]
    }
   ],
   "source": [
    "def sortedDictValues(adict): \n",
    "    keys = adict.keys() \n",
    "    keys = sorted(keys)\n",
    "    print('keys = ')\n",
    "    print(keys[0:10])\n",
    "    new_content_dict = {}\n",
    "    for data in keys:\n",
    "        new_content_dict[data] = content_dict[data]\n",
    "    return new_content_dict \n",
    "new_content_dict = sortedDictValues(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9499dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_new_content_dict = {}\n",
    "for data in new_content_dict:\n",
    "    inv_new_content_dict[new_content_dict[data]] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a9af0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = new_content_dict.keys()\n",
    "keys = list(keys)\n",
    "dict_keys_index = {}\n",
    "for index in range(len(keys)):\n",
    "    #dict_keys_index[index] = keys[index]\n",
    "    dict_keys_index[keys[index]] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "215426e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "content,emotions = [],[]\n",
    "dicts = {}\n",
    "inv_dicts = {}\n",
    "label_num = 0\n",
    "characters = []\n",
    "for index in range(len(test['content'])):\n",
    "        content.append(test['content'][index])\n",
    "        characters.append(test['character'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "federal-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext = test['content']\n",
    "testid = test['id']\n",
    "testcharacter = test['character']\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,character,maxlen):\n",
    "        self.text = text\n",
    "        self.maxlen = maxlen\n",
    "        token_data,token_id,segment_id,mask_id = [],[],[],[]\n",
    "        #sequence填充可以最后统一实现\n",
    "        for index in tqdm(range(len(self.text))):\n",
    "            current_text = text[index]\n",
    "            #print('current_text:\\t', current_text, len(current_text))\n",
    "            current_character = characters[index]\n",
    "            current_token = tokenizer.tokenize(current_text)\n",
    "            current_str_index = inv_new_content_dict[current_text]\n",
    "            current_str_index_bak = inv_new_content_dict[current_text]\n",
    "            current_key_index = dict_keys_index[current_str_index]\n",
    "            current_key_index_bak = dict_keys_index[current_str_index]\n",
    "            # 同一幕中所有上文拼在一起\n",
    "            lcs_lst = []\n",
    "\n",
    "            pre_current_key_index = current_key_index\n",
    "            pre_current_key_text = current_text\n",
    "            pre_content = ''\n",
    "            #前面一个语句对应的索引以及文本内容\n",
    "            num = 0\n",
    "            while True:\n",
    "                new_current_key_index = pre_current_key_index\n",
    "                new_current_key_text = pre_current_key_text\n",
    "                while new_current_key_index != -1 and new_content_dict[keys[new_current_key_index]] == new_current_key_text:\n",
    "                    new_current_key_index = new_current_key_index-1\n",
    "                if new_current_key_index == -1:\n",
    "                    pre_content = ''\n",
    "                    break\n",
    "                # 只在同一幕中找\n",
    "                new_str_index = keys[new_current_key_index]\n",
    "                current_str_indexs = current_str_index.split('_')\n",
    "                new_str_indexs = new_str_index.split('_')\n",
    "                # 跨剧本，停止寻找\n",
    "                if current_str_indexs[0] != new_str_indexs[0]:\n",
    "                    break\n",
    "                # 跨幕，停止寻找\n",
    "                if current_str_indexs[1] != new_str_indexs[1]:\n",
    "                    break\n",
    "                new_pre_content = new_content_dict[keys[new_current_key_index]]\n",
    "                if str(new_pre_content) == 'nan':\n",
    "                    new_pre_content = '无'\n",
    "                if str(current_character) == 'nan':\n",
    "                    break\n",
    "                if num == 0 or current_character in new_pre_content:\n",
    "                    lcs_lst.append(new_pre_content)\n",
    "                    num = num+1\n",
    "                pre_current_key_index = new_current_key_index\n",
    "                pre_current_key_text = new_pre_content\n",
    "                if num == 3:\n",
    "                    break\n",
    "            lcs_lst.reverse()\n",
    "            if not lcs_lst:\n",
    "                pre_content = '无'\n",
    "            else:\n",
    "                pre_content = ''.join(lcs_lst)\n",
    "            # 统计有效上文长度\n",
    "            \n",
    "            #print('pre_content = ')\n",
    "            #print(pre_content)\n",
    "            #print('current_text = ')\n",
    "            #print(current_text)\n",
    "            \n",
    "            if str(current_character) == 'nan':\n",
    "                current_character = '无'\n",
    "            current_character_token = tokenizer.tokenize(current_character)\n",
    "            pre_token = tokenizer.tokenize(pre_content)\n",
    "            \n",
    "            #current_token = [\"[CLS]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #for data1 in lcs_lst:\n",
    "            #    new_token = tokenizer.tokenize(data1)\n",
    "            #    current_token = current_token+new_token+[\"[SEP]\"]\n",
    "            #current_token = [\"[CLS]\"] + current_token + [\"[SEP]\"] + current_character_token + [\"[SEP]\"]\n",
    "            current_token = [\"[CLS]\"]+pre_token+[\"[SEP]\"]+current_token+[\"[SEP]\"]+current_character_token+[\"[SEP]\"]\n",
    "            #if len(current_token) > 1:\n",
    "                #print('pre_text:\\t', pre_content, len(pre_content))\n",
    "                #pre_content_lengths.append(len(current_token))\n",
    "            current_id = tokenizer.convert_tokens_to_ids(current_token)            \n",
    "            current_id = self.sequence_padding(current_id)\n",
    "            token_data.append(current_token)\n",
    "            token_id.append(current_id)\n",
    "        self.token_data = token_data\n",
    "        self.token_id = token_id\n",
    "        #self.segment_id = sequence_padding(self.segment_id,maxlen)\n",
    "        #self.mask_id = sequence_padding(self.mask_id,maxlen)\n",
    "        self.tensors = [torch.tensor(self.token_id,dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.token_id)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "    def sequence_padding(self,inputs,padding = 0):\n",
    "        length = self.maxlen\n",
    "        if len(inputs) > length:\n",
    "            inputs = inputs[:length]\n",
    "        outputs = []\n",
    "        pad_width = (0,length-len(inputs))\n",
    "        x = np.pad(inputs,pad_width,'constant',constant_values=padding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "devoted-jenny",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 21376/21376 [00:05<00:00, 3941.61it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(testtext,testcharacter,maxlen=200)\n",
    "test_loader = DataLoader(test_dataset,batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81860705",
   "metadata": {},
   "source": [
    "## 修改测试集的评分标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "atlantic-dispatch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1336/1336 [01:09<00:00, 19.16it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "eval_loss = 0.\n",
    "eval_acc = 0.\n",
    "eval_predict_label = []\n",
    "index = []\n",
    "pred = [[],[],[],[],[],[]]\n",
    "current_index = 0\n",
    "for batch_token_ids in tqdm(test_loader):\n",
    "    batch_token_ids = batch_token_ids[0].to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(batch_token_ids,None,None)\n",
    "        for index in range(len(output)):\n",
    "            #总的数据\n",
    "            for index1 in range(len(output[index])):\n",
    "                #对应的类别概率0~6\n",
    "                abs0 = abs(output[index][index1]-0)\n",
    "                abs1 = abs(output[index][index1]-1)\n",
    "                abs2 = abs(output[index][index1]-2)\n",
    "                abs3 = abs(output[index][index1]-3)\n",
    "                currentdata = [abs0,abs1,abs2,abs3]\n",
    "                current_label = currentdata.index(min(currentdata))\n",
    "                #eval_predict_label[index1].append(current_label)\n",
    "                current_predict = output[index][index1].item()\n",
    "                \n",
    "                if current_predict < 0.00:\n",
    "                    current_predict = 0.0\n",
    "                elif current_predict > 3:\n",
    "                    current_predict = 3.0\n",
    "                \n",
    "                pred[index1].append(current_predict)\n",
    "                #eval_predict_label[index1].append(current_predict)\n",
    "                #当前类别的分类结果,这里append(output[index][index1])\n",
    "                #直接放入对应概率值时效果最好,这里直接放入0,1,2,3对应的数值\n",
    "for index in range(len(pred[0])):\n",
    "    eval_predict_label.append(str(pred[0][index])+','+str(pred[1][index])+','+str(pred[2][index])+','+str(pred[3][index])+','+str(pred[4][index])+','+str(pred[5][index]))\n",
    "result_data = []\n",
    "for index in range(len(testid)):\n",
    "    result_data.append([testid[index],eval_predict_label[index]])\n",
    "#pd.DataFrame({\"id\":testid,\"label\":eval_predict_label}).to_csv(\"/home/xiaoguzai/代码/剧本角色情感识别/数据集/crossentropy\"+str(bestpoint)+\"result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-major",
   "metadata": {},
   "source": [
    "a=np.array([[1,2],[3,4]])\n",
    "b=np.array([[2,3],[4,5]])\n",
    "loss_fn = torch.nn.MSELoss(reduce=False, size_average=False)\n",
    "input = torch.autograd.Variable(torch.from_numpy(a))\n",
    "target = torch.autograd.Variable(torch.from_numpy(b))\n",
    "loss = loss_fn(input.float(),target.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "completed-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(r'/home/xiaoguzai/程序/剧本角色情感识别/数据/nezha_base'+str(bestpoint)+\"result.csv\", 'w') as f:\n",
    "    tsv_w = csv.writer(f, delimiter='\\t')\n",
    "    tsv_w.writerow(['id', 'emotion'])  # 单行写入\n",
    "    tsv_w.writerows(result_data)  # 多行写入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9853f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
