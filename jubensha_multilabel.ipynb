{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42790, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>character</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1171_0001_A_1</td>\n",
       "      <td>天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。</td>\n",
       "      <td>o2</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1171_0001_A_2</td>\n",
       "      <td>天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。</td>\n",
       "      <td>c1</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1171_0001_A_3</td>\n",
       "      <td>o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。</td>\n",
       "      <td>o2</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1171_0001_A_4</td>\n",
       "      <td>o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。</td>\n",
       "      <td>c1</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1171_0001_A_5</td>\n",
       "      <td>o2停下来接过c1手里的行李：你妈妈交待我了，等领了军装一定要照张相寄回去，让街坊邻居都知道...</td>\n",
       "      <td>o2</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                            content character  \\\n",
       "0  1171_0001_A_1          天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。        o2   \n",
       "1  1171_0001_A_2          天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。        c1   \n",
       "2  1171_0001_A_3                       o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。        o2   \n",
       "3  1171_0001_A_4                       o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。        c1   \n",
       "4  1171_0001_A_5  o2停下来接过c1手里的行李：你妈妈交待我了，等领了军装一定要照张相寄回去，让街坊邻居都知道...        o2   \n",
       "\n",
       "      emotions  \n",
       "0  0,0,0,0,0,0  \n",
       "1  0,0,0,0,0,0  \n",
       "2  0,0,0,0,0,0  \n",
       "3  0,0,0,0,0,0  \n",
       "4  0,0,0,0,0,0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练集\n",
    "train = pd.read_csv('train_dataset_v2.tsv', sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21376, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34170_0002_A_12</td>\n",
       "      <td>穿着背心的b1醒来，看看手机，三点了。</td>\n",
       "      <td>b1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34170_0002_A_14</td>\n",
       "      <td>b1走出卧室。</td>\n",
       "      <td>b1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34170_0003_A_16</td>\n",
       "      <td>b1拿着手机，点开计时功能。</td>\n",
       "      <td>b1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34170_0003_A_17</td>\n",
       "      <td>b1站在淋浴头下面，水从b1的头和脸上冲刷而过。</td>\n",
       "      <td>b1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34170_0003_A_18</td>\n",
       "      <td>b1摈着呼吸。</td>\n",
       "      <td>b1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                   content character\n",
       "0  34170_0002_A_12       穿着背心的b1醒来，看看手机，三点了。        b1\n",
       "1  34170_0002_A_14                   b1走出卧室。        b1\n",
       "2  34170_0003_A_16            b1拿着手机，点开计时功能。        b1\n",
       "3  34170_0003_A_17  b1站在淋浴头下面，水从b1的头和脸上冲刷而过。        b1\n",
       "4  34170_0003_A_18                   b1摈着呼吸。        b1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载测试集\n",
    "test = pd.read_csv('test_dataset.tsv', sep='\\t')\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21376, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34170_0002_A_12</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34170_0002_A_14</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34170_0003_A_16</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34170_0003_A_17</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34170_0003_A_18</td>\n",
       "      <td>0,0,0,0,0,0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id      emotion\n",
       "0  34170_0002_A_12  0,0,0,0,0,0\n",
       "1  34170_0002_A_14  0,0,0,0,0,0\n",
       "2  34170_0003_A_16  0,0,0,0,0,0\n",
       "3  34170_0003_A_17  0,0,0,0,0,0\n",
       "4  34170_0003_A_18  0,0,0,0,0,0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.read_csv('submit_example.tsv', sep='\\t')\n",
    "print(submit.shape)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36782, 4)\n"
     ]
    }
   ],
   "source": [
    "train = train[train['emotions'].notna()]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无上文\n"
     ]
    }
   ],
   "source": [
    "def get_context(df, cur_id):\n",
    "    pos = cur_id.split('_')[-1]\n",
    "    # print(pos)\n",
    "    pos_str = str(int(pos) - 1)\n",
    "\n",
    "    location = train['id'][0][:-2] + '_' + pos_str\n",
    "    #print(location)\n",
    "    ret = train.loc[train['id'] == location]['content'].values.tolist()\n",
    "    if len(ret) == 1:\n",
    "        return ret[0]\n",
    "    else:\n",
    "        return \"无上文\"\n",
    "print(get_context(train, '1171_0001_A_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['character'].fillna('无角色', inplace=True)\n",
    "test['character'].fillna('无角色', inplace=True)\n",
    "\n",
    "train['text'] = train['content'].astype(str) + ' 角色: ' + train['character'].astype(str)\n",
    "test['text'] = test['content'].astype(str) + ' 角色: ' + test['character'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                       id                                            content  \\\n",
       "0          1171_0001_A_1          天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。   \n",
       "1          1171_0001_A_2          天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。   \n",
       "2          1171_0001_A_3                       o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。   \n",
       "3          1171_0001_A_4                       o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。   \n",
       "4          1171_0001_A_5  o2停下来接过c1手里的行李：你妈妈交待我了，等领了军装一定要照张相寄回去，让街坊邻居都知道...   \n",
       "...                  ...                                                ...   \n",
       "42785  34949_0180_A_1450                                x2的未婚夫手捧大束玫瑰花在寻找x2。   \n",
       "42786  34949_0180_A_1458                f1看看表：过了五分钟。他从口袋里掏出药盒，倒出药：一片，两片，半片。   \n",
       "42787  34949_0180_A_1459  b1转脸朝前走。突然站住了，他看见自己的前妻，c1领着女儿g3站在前面。他过去，三人紧紧拥在...   \n",
       "42788  34949_0180_A_1460  b1转脸朝前走。突然站住了，他看见自己的前妻，c1领着女儿g3站在前面。他过去，三人紧紧拥在...   \n",
       "42789  34949_0180_A_1463  b1转脸朝前走。突然站住了，他看见自己的前妻，c1领着女儿g3站在前面。他过去，三人紧紧拥在...   \n",
       "\n",
       "      character     emotions  \\\n",
       "0            o2  0,0,0,0,0,0   \n",
       "1            c1  0,0,0,0,0,0   \n",
       "2            o2  0,0,0,0,0,0   \n",
       "3            c1  0,0,0,0,0,0   \n",
       "4            o2  0,0,0,0,0,0   \n",
       "...         ...          ...   \n",
       "42785        x2  0,0,0,0,0,0   \n",
       "42786        f1  0,3,0,0,0,0   \n",
       "42787        b1  2,3,0,0,0,0   \n",
       "42788        c1  2,3,0,0,0,0   \n",
       "42789        n1  0,0,0,0,0,0   \n",
       "\n",
       "                                                    text  \n",
       "0      天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。 角色: ...  \n",
       "1      天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。 角色: ...  \n",
       "2      o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。 角色: o2 上文: 天空下着暴雨...  \n",
       "3      o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。 角色: c1 上文: o2一手拿着...  \n",
       "4      o2停下来接过c1手里的行李：你妈妈交待我了，等领了军装一定要照张相寄回去，让街坊邻居都知道...  \n",
       "...                                                  ...  \n",
       "42785                         x2的未婚夫手捧大束玫瑰花在寻找x2。 角色: x2  \n",
       "42786         f1看看表：过了五分钟。他从口袋里掏出药盒，倒出药：一片，两片，半片。 角色: f1  \n",
       "42787  b1转脸朝前走。突然站住了，他看见自己的前妻，c1领着女儿g3站在前面。他过去，三人紧紧拥在...  \n",
       "42788  b1转脸朝前走。突然站住了，他看见自己的前妻，c1领着女儿g3站在前面。他过去，三人紧紧拥在...  \n",
       "42789  b1转脸朝前走。突然站住了，他看见自己的前妻，c1领着女儿g3站在前面。他过去，三人紧紧拥在...  \n",
       "\n",
       "[36782 rows x 5 columns]>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add train context info\n",
    "for i in range(len(train['text'])):\n",
    "    try:\n",
    "        lcw = get_context(train, train['id'][i])\n",
    "    except:\n",
    "        continue\n",
    "    train['text'][i] += ' 上文: ' + lcw\n",
    "train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                       id                   content character  \\\n",
       "0        34170_0002_A_12       穿着背心的b1醒来，看看手机，三点了。        b1   \n",
       "1        34170_0002_A_14                   b1走出卧室。        b1   \n",
       "2        34170_0003_A_16            b1拿着手机，点开计时功能。        b1   \n",
       "3        34170_0003_A_17  b1站在淋浴头下面，水从b1的头和脸上冲刷而过。        b1   \n",
       "4        34170_0003_A_18                   b1摈着呼吸。        b1   \n",
       "...                  ...                       ...       ...   \n",
       "21371  34122_0130_A_1184               w2：正版药进医保了。        w2   \n",
       "21372  34122_0130_A_1185                   u2：那挺好。        u2   \n",
       "21373  34122_0130_A_1186      w2：你还是接着卖你的壮阳药吧，适合你。        w2   \n",
       "21374  34122_0130_A_1187           w2开着车，带着u2驶向远方。        u2   \n",
       "21375  34122_0130_A_1188           w2开着车，带着u2驶向远方。        w2   \n",
       "\n",
       "                                                    text  \n",
       "0      穿着背心的b1醒来，看看手机，三点了。 角色: b1 上文: o2笑了笑：军礼不是这么敬的。...  \n",
       "1                     b1走出卧室。 角色: b1 上文: o2示范了一个动作，c1照做。  \n",
       "2                        b1拿着手机，点开计时功能。 角色: b1 上文: c1照做。  \n",
       "3      b1站在淋浴头下面，水从b1的头和脸上冲刷而过。 角色: b1 上文: b1画外音：我叫b1...  \n",
       "4                                 b1摈着呼吸。 角色: b1 上文: 无上文  \n",
       "...                                                  ...  \n",
       "21371                         w2：正版药进医保了。 角色: w2 上文: 无上文  \n",
       "21372                             u2：那挺好。 角色: u2 上文: 无上文  \n",
       "21373                w2：你还是接着卖你的壮阳药吧，适合你。 角色: w2 上文: 无上文  \n",
       "21374                     w2开着车，带着u2驶向远方。 角色: u2 上文: 无上文  \n",
       "21375                     w2开着车，带着u2驶向远方。 角色: w2 上文: 无上文  \n",
       "\n",
       "[21376 rows x 4 columns]>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add test context info\n",
    "for i in range(len(test['id'])):\n",
    "    try:\n",
    "        lcw = get_context(test, test['id'][i])\n",
    "    except:\n",
    "        continue\n",
    "    test['text'][i] += ' 上文: ' + lcw\n",
    "test.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import (\n",
    "    MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36782, 5)\n"
     ]
    }
   ],
   "source": [
    "# 失败的尝试，直接拼 0.69 -> 0.68\n",
    "# train['text'] = train['context'].astype(str) + train['text'].astype(str)\n",
    "# test['text'] = test['context'].astype(str) + test['text'].astype(str)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['labels'] = train['emotions'].apply(lambda x: [int(i) for i in x.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。 角色: ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。 角色: ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。 角色: o2 上文: 天空下着暴雨...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。 角色: c1 上文: o2一手拿着...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o2停下来接过c1手里的行李：你妈妈交待我了，等领了军装一定要照张相寄回去，让街坊邻居都知道...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text              labels\n",
       "0  天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。 角色: ...  [0, 0, 0, 0, 0, 0]\n",
       "1  天空下着暴雨，o2正在给c1穿雨衣，他自己却只穿着单薄的军装，完全暴露在大雨之中。 角色: ...  [0, 0, 0, 0, 0, 0]\n",
       "2  o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。 角色: o2 上文: 天空下着暴雨...  [0, 0, 0, 0, 0, 0]\n",
       "3  o2一手拿着一个行李，一路小跑着把c1带到了文工团门口。 角色: c1 上文: o2一手拿着...  [0, 0, 0, 0, 0, 0]\n",
       "4  o2停下来接过c1手里的行李：你妈妈交待我了，等领了军装一定要照张相寄回去，让街坊邻居都知道...  [0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train[['text', 'labels']].copy()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sample(frac=1.0, random_state=42)\n",
    "dev_data = train_data.iloc[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36782\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import (\n",
    "    MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = MultiLabelClassificationArgs()\n",
    "model_args.max_seq_length = 128\n",
    "model_args.num_train_epochs = 5\n",
    "model_args.learning_rate = 2e-5\n",
    "model_args.do_lower_case = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.no_save = True\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.save_steps = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelClassificationArgs(adafactor_beta1=None, adafactor_clip_threshold=1.0, adafactor_decay_rate=-0.8, adafactor_eps=(1e-30, 0.001), adafactor_relative_step=True, adafactor_scale_parameter=True, adafactor_warmup_init=True, adam_epsilon=1e-08, best_model_dir='outputs/best_model', cache_dir='cache_dir/', config={}, cosine_schedule_num_cycles=0.5, custom_layer_parameters=[], custom_parameter_groups=[], dataloader_num_workers=0, do_lower_case=True, dynamic_quantize=False, early_stopping_consider_epochs=False, early_stopping_delta=0, early_stopping_metric='eval_loss', early_stopping_metric_minimize=True, early_stopping_patience=3, encoding=None, eval_batch_size=8, evaluate_during_training=False, evaluate_during_training_silent=True, evaluate_during_training_steps=2000, evaluate_during_training_verbose=False, evaluate_each_epoch=True, fp16=True, gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, manual_seed=None, max_grad_norm=1.0, max_seq_length=128, model_name=None, model_type=None, multiprocessing_chunksize=-1, n_gpu=1, no_cache=False, no_save=True, not_saved_args=[], num_train_epochs=5, optimizer='AdamW', output_dir='outputs/', overwrite_output_dir=True, polynomial_decay_schedule_lr_end=1e-07, polynomial_decay_schedule_power=1.0, process_count=94, quantized_model=False, reprocess_input_data=True, save_best_model=True, save_eval_checkpoints=True, save_model_every_epoch=False, save_optimizer_and_scheduler=True, save_steps=-1, scheduler='linear_schedule_with_warmup', silent=False, skip_special_tokens=True, tensorboard_dir=None, thread_count=None, tokenizer_name=None, tokenizer_type=None, train_batch_size=8, train_custom_parameters_only=False, use_cached_eval_features=False, use_early_stopping=False, use_hf_datasets=False, use_multiprocessing=True, use_multiprocessing_for_evaluation=True, wandb_kwargs={}, wandb_project=None, warmup_ratio=0.06, warmup_steps=0, weight_decay=0.0, model_class='MultiLabelClassificationModel', sliding_window=False, stride=0.8, threshold=0.5, tie_value=1, labels_list=[], labels_map={}, lazy_loading=False, special_tokens_list=[])\n"
     ]
    }
   ],
   "source": [
    "print(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForMultiLabelSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#model = MultiLabelClassificationModel('bert', 'hfl/chinese-bert-wwm-ext', num_labels=6)\n",
    "model = MultiLabelClassificationModel('bert', 'hfl/chinese-roberta-wwm-ext', num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d3f53a88e546e582819e9691bc5115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6ceb3ca17f4335953ca28c88d8fb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1e83549ecf466c9e2296c77d0e535e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/4598 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4598, 0.2584208245440497)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b929ddc7ea3d4a1db82a564b0e335b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b6498b5055487d9d9aa836fe28b536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "得分：0.7046656332279972\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "predictions, raw_outputs = model.predict([text for text in dev_data['text'].values])\n",
    "raw_outputs[raw_outputs<=0.01] = 0\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "result = raw_outputs.tolist()\n",
    "# for i in range(len(result)):\n",
    "#     for j in range(6):\n",
    "#         if result[i][j] > 0.99:\n",
    "#             result[i][j] += 0.5\n",
    "test_y = dev_data['labels'].values.tolist()\n",
    "rmse = np.sqrt(mean_squared_error(result, test_y))\n",
    "score = 1.0 / (1+rmse)\n",
    "print(f\"得分：{score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # submit tag\n",
    "# sub = submit.copy()\n",
    "# sub['emotion'] = predictions\n",
    "# sub['emotion'] = sub['emotion'].apply(lambda x: ','.join([str(i) for i in x]))\n",
    "# sub.head()\n",
    "# sub.to_csv('4epoch_2e05_tags_roberta.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0340ddb567c6412d9849d67508b31873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e05056b4b54f018af336dc7c092b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "predictions, raw_outputs = model.predict([text for text in test['text'].values])\n",
    "raw_outputs[raw_outputs<=0.01] = 0\n",
    "# sub probs\n",
    "prob_submit = submit.copy()\n",
    "to_submit = []\n",
    "# prob_submit['emotion'] = raw_outputs\n",
    "for i in range(len(raw_outputs)):\n",
    "    lst = raw_outputs[i].tolist()\n",
    "#     for j in range(6):\n",
    "#         if lst[j] > 0.99:\n",
    "#             lst[j] += 0.5\n",
    "    to_submit.append(','.join([str(i) for i in lst]))\n",
    "    # break\n",
    "prob_submit['emotion'] = to_submit #prob_submit['emotion'].apply(lambda x: ','.join([str(i) for i in x]))\n",
    "prob_submit.head()\n",
    "prob_submit.to_csv('0924_s7046_roberta_structured.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sub probs\n",
    "# prob_submit = sub.copy()\n",
    "# to_submit = []\n",
    "# # prob_submit['emotion'] = raw_outputs\n",
    "# for i in range(len(raw_outputs)):\n",
    "#     lst = raw_outputs[i].tolist()\n",
    "#     to_submit.append(','.join([str(i * 3) for i in lst]))\n",
    "#     # break\n",
    "# prob_submit['emotion'] = to_submit #prob_submit['emotion'].apply(lambda x: ','.join([str(i) for i in x]))\n",
    "# prob_submit.head()\n",
    "# prob_submit.to_csv('4epoch_2e05_probs_emplified_roberta.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLabelClassificationModel('bert', 'outputs', num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "train_data_0 = []\n",
    "train_data_1 = []\n",
    "train_data_2 = []\n",
    "train_data_3 = []\n",
    "train_data_4 = []\n",
    "train_data_5 = []\n",
    "\n",
    "for i in range(len(train_data['text'])):\n",
    "    try:\n",
    "        ## train data\n",
    "        train_data_0.append([train_data['text'][i], train_data['labels'][i][0]])\n",
    "        train_data_1.append([train_data['text'][i], train_data['labels'][i][1]])\n",
    "        train_data_2.append([train_data['text'][i], train_data['labels'][i][2]])\n",
    "        train_data_3.append([train_data['text'][i], train_data['labels'][i][3]])\n",
    "        train_data_4.append([train_data['text'][i], train_data['labels'][i][4]])\n",
    "        train_data_5.append([train_data['text'][i], train_data['labels'][i][5]])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "dev_data_0 = []\n",
    "dev_data_1 = []\n",
    "dev_data_2 = []\n",
    "dev_data_3 = []\n",
    "dev_data_4 = []\n",
    "dev_data_5 = []\n",
    "\n",
    "for i in range(len(dev_data['text'])):\n",
    "    try:\n",
    "        ## train data\n",
    "        dev_data_0.append([dev_data['text'][i], dev_data['labels'][i][0]])\n",
    "        dev_data_1.append([dev_data['text'][i], dev_data['labels'][i][1]])\n",
    "        dev_data_2.append([dev_data['text'][i], dev_data['labels'][i][2]])\n",
    "        dev_data_3.append([dev_data['text'][i], dev_data['labels'][i][3]])\n",
    "        dev_data_4.append([dev_data['text'][i], dev_data['labels'][i][4]])\n",
    "        dev_data_5.append([dev_data['text'][i], dev_data['labels'][i][5]])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_0 = pd.DataFrame(train_data_0)\n",
    "train_df_0.columns = [\"text\", \"labels\"]\n",
    "train_df_1 = pd.DataFrame(train_data_1)\n",
    "train_df_1.columns = [\"text\", \"labels\"]\n",
    "train_df_2 = pd.DataFrame(train_data_2)\n",
    "train_df_2.columns = [\"text\", \"labels\"]\n",
    "train_df_3 = pd.DataFrame(train_data_3)\n",
    "train_df_3.columns = [\"text\", \"labels\"]\n",
    "train_df_4 = pd.DataFrame(train_data_4)\n",
    "train_df_4.columns = [\"text\", \"labels\"]\n",
    "train_df_5 = pd.DataFrame(train_data_5)\n",
    "train_df_5.columns = [\"text\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_0 = pd.DataFrame(dev_data_0)\n",
    "dev_df_0.columns = [\"text\", \"labels\"]\n",
    "dev_df_1 = pd.DataFrame(dev_data_1)\n",
    "dev_df_1.columns = [\"text\", \"labels\"]\n",
    "dev_df_2 = pd.DataFrame(dev_data_2)\n",
    "dev_df_2.columns = [\"text\", \"labels\"]\n",
    "dev_df_3 = pd.DataFrame(dev_data_3)\n",
    "dev_df_3.columns = [\"text\", \"labels\"]\n",
    "dev_df_4 = pd.DataFrame(dev_data_4)\n",
    "dev_df_4.columns = [\"text\", \"labels\"]\n",
    "dev_df_5 = pd.DataFrame(dev_data_5)\n",
    "dev_df_5.columns = [\"text\", \"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a941a02a8ada43909f24d4b835781246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_4_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7be78ff3de6485d970a3199e2eae64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07d44b2650241b2a911174db6c3fec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b918eb27fef4d32be061a3af3262969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional model configuration\n",
    "model_args_0 = ClassificationArgs(num_train_epochs=4, output_dir='outputs_0')\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model_0 = ClassificationModel(\n",
    "    'bert',\n",
    "    'hfl/chinese-roberta-wwm-ext',\n",
    "    num_labels=4,\n",
    "    args=model_args_0\n",
    ") \n",
    "# Train the model\n",
    "model_0.train_model(train_df_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ffdb956d8b4308886b874d5c79d531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_4_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd01de0629e44318b7a905d1db2ccd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bba3d8ff4ca46aeb4ff2ef0725f5ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87317cb727a49c1a41ea9622a42012d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional model configuration\n",
    "model_args_1 = ClassificationArgs(num_train_epochs=4, output_dir='outputs_1')\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model_1 = ClassificationModel(\n",
    "    'bert',\n",
    "    'hfl/chinese-roberta-wwm-ext',\n",
    "    num_labels=4,\n",
    "    args=model_args_1\n",
    ") \n",
    "# Train the model\n",
    "model_1.train_model(train_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to outputs_2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15896, 0.22567643049037953)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional model configuration\n",
    "model_args_2 = ClassificationArgs(num_train_epochs=4, output_dir='outputs_2')\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model_2 = ClassificationModel(\n",
    "    'bert',\n",
    "    'hfl/chinese-roberta-wwm-ext',\n",
    "    num_labels=4,\n",
    "    args=model_args_2\n",
    ") \n",
    "# Train the model\n",
    "model_2.train_model(train_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a986df643454aba8fd792c72fee7802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_4_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa040716ffc40b7b5740d7677337358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652b358f638648ca90022ff2a410c5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f08b829f3940468a5f9ad52f36d3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76bf33f9d2a4d6a9a5acd65160eaaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2692d02a608f403f8c9dc1e28d741726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_model: Training of bert model complete. Saved to outputs_3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15896, 0.32029314788462837)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional model configuration\n",
    "model_args_3 = ClassificationArgs(num_train_epochs=4, output_dir='outputs_3')\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model_3 = ClassificationModel(\n",
    "    'bert',\n",
    "    'hfl/chinese-roberta-wwm-ext',\n",
    "    num_labels=4,\n",
    "    args=model_args_3\n",
    ") \n",
    "# Train the model\n",
    "model_3.train_model(train_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b183aa9d484fe896bd808015ef8415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Saving features into cached file cache_dir/cached_train_bert_128_4_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de78a2ce07b54d6aa77a86e29ffc6b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037aca31b29645cb9ece1301cf22e419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dc3a22fef94807b37716220f3d0de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eab561ebce04e83a103e0a5b8632321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e49a0bf26c94b35929fc00e58a47424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 4:   0%|          | 0/3974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optional model configuration\n",
    "model_args_4 = ClassificationArgs(num_train_epochs=4, output_dir='outputs_4')\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model_4 = ClassificationModel(\n",
    "    'bert',\n",
    "    'hfl/chinese-roberta-wwm-ext',\n",
    "    num_labels=4,\n",
    "    args=model_args_4\n",
    ") \n",
    "# Train the model\n",
    "model_4.train_model(train_df_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional model configuration\n",
    "model_args_5 = ClassificationArgs(num_train_epochs=4, output_dir='outputs_5')\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model_5 = ClassificationModel(\n",
    "    'bert',\n",
    "    'hfl/chinese-roberta-wwm-ext',\n",
    "    num_labels=4,\n",
    "    args=model_args_5\n",
    ") \n",
    "# Train the model\n",
    "model_5.train_model(train_df_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = ClassificationModel('bert', 'outputs_0', num_labels=4)\n",
    "model_1 = ClassificationModel('bert', 'outputs_1', num_labels=4)\n",
    "model_2 = ClassificationModel('bert', 'outputs_2', num_labels=4)\n",
    "model_3 = ClassificationModel('bert', 'outputs_3', num_labels=4)\n",
    "model_4 = ClassificationModel('bert', 'outputs_4', num_labels=4)\n",
    "model_5 = ClassificationModel('bert', 'outputs_5', num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8620b45a9b4c3db1bf417a49f717b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa708be91414df69c3cf8663c619615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca41df06e17475a9a11faa8fea8aee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63a79cac1624066a3e0ecf86a51d93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c7bcbbbeac4f83a9e820943c884d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7366fb3d7c944d8a2e98430dcfa2412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dca4775db834462a3f5541f72f2463b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbaf70ab13a4822a6aa6c1f0426616c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0887d42f4ef94167aa64969e9e3cd263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0e6a371d8446c8b50b983fa859acc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3d6195ff894727866afedb4e98b0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d0d8457d3640bcb0d1b007ca7043e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# multi-classification\n",
    "_, raw_output_0 = model_0.predict([text for text in dev_data['text'].values])\n",
    "_, raw_output_1 = model_1.predict([text for text in dev_data['text'].values])\n",
    "_, raw_output_2 = model_2.predict([text for text in dev_data['text'].values])\n",
    "_, raw_output_3 = model_3.predict([text for text in dev_data['text'].values])\n",
    "_, raw_output_4 = model_4.predict([text for text in dev_data['text'].values])\n",
    "_, raw_output_5 = model_5.predict([text for text in dev_data['text'].values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# custom function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# define vectorized sigmoid\n",
    "sigmoid_v = np.vectorize(sigmoid)\n",
    "\n",
    "raw_output_0 = sigmoid_v(raw_output_0).tolist()\n",
    "raw_output_1 = sigmoid_v(raw_output_1).tolist()\n",
    "raw_output_2 = sigmoid_v(raw_output_2).tolist()\n",
    "raw_output_3 = sigmoid_v(raw_output_3).tolist()\n",
    "raw_output_4 = sigmoid_v(raw_output_4).tolist()\n",
    "raw_output_5 = sigmoid_v(raw_output_5).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fffc8a058c404a8e525729dae3466c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434012894a6c4ce89fb4864a7857715e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_173112/805991875.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mmax_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output_5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_output_5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "\n",
    "# eval on joint model\n",
    "predictions, raw_outputs = model.predict([text for text in dev_data['text'].values])\n",
    "raw_outputs[raw_outputs<=0.01] = 0\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "result = raw_outputs.tolist()\n",
    "for i in range(len(result)):\n",
    "    for j in range(6):\n",
    "        if result[i][j] > 0.90:\n",
    "            if j == 0:\n",
    "                max_prob = max(raw_output_0[i])\n",
    "                max_indx = raw_output_0[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "            elif j == 1:\n",
    "                max_prob = max(raw_output_1[i])\n",
    "                max_indx = raw_output_1[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "            elif j == 2:\n",
    "                max_prob = max(raw_output_2[i])\n",
    "                max_indx = raw_output_2[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "            elif j == 3:\n",
    "                max_prob = max(raw_output_3[i])\n",
    "                max_indx = raw_output_3[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "            elif j == 4:\n",
    "                max_prob = max(raw_output_4[i])\n",
    "                max_indx = raw_output_4[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "            elif j == 5:\n",
    "                max_prob = max(raw_output_5[i])\n",
    "                max_indx = raw_output_5[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "test_y = dev_data['labels'].values.tolist()\n",
    "rmse = np.sqrt(mean_squared_error(result, test_y))\n",
    "score = 1.0 / (1+rmse)\n",
    "print(f\"得分：{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45134504ddde410287819e8968e39b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eab90f4dbc04f358c1f5f40218ec98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdbc051f6b943a9bd4bec61ae4c53a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2380f89187e145768cf70ebff6d0863e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556590ab66d3437d9961ed59efc727c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7126d20abd4548e2ab6d148db421ff2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e3a44a048e49189bfba7af84f5b030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb786f91edd44c589a98f7b8891feb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4057099fe3fc4ef98b5398cb50f13594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b50c28a26149e08424e4ef90a05c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9b46e245a64de799f0f312975fe7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543b254f0b9d4599b54e45560776a128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# multi-classification\n",
    "_, raw_output_0 = model_0.predict([text for text in test['text'].values])\n",
    "_, raw_output_1 = model_1.predict([text for text in test['text'].values])\n",
    "_, raw_output_2 = model_2.predict([text for text in test['text'].values])\n",
    "_, raw_output_3 = model_3.predict([text for text in test['text'].values])\n",
    "_, raw_output_4 = model_4.predict([text for text in test['text'].values])\n",
    "_, raw_output_5 = model_5.predict([text for text in test['text'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_output_0 = sigmoid_v(raw_output_0).tolist()\n",
    "raw_output_1 = sigmoid_v(raw_output_1).tolist()\n",
    "raw_output_2 = sigmoid_v(raw_output_2).tolist()\n",
    "raw_output_3 = sigmoid_v(raw_output_3).tolist()\n",
    "raw_output_4 = sigmoid_v(raw_output_4).tolist()\n",
    "raw_output_5 = sigmoid_v(raw_output_5).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d347a2685444a93bdb042be558cb8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9786367f714175aa7025e8aed64d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# test\n",
    "predictions, raw_outputs = model.predict([text for text in test['text'].values])\n",
    "raw_outputs[raw_outputs<=0.01] = 0\n",
    "# sub probs\n",
    "prob_submit = submit.copy()\n",
    "to_submit = []\n",
    "result = raw_outputs.tolist()\n",
    "for i in range(len(result)):\n",
    "    for j in range(6):\n",
    "        if result[i][j] > 0.95:\n",
    "            if j == 0:\n",
    "                max_prob = max(raw_output_0[i])\n",
    "                max_indx = raw_output_0[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "            elif j == 1:\n",
    "                max_prob = max(raw_output_1[i])\n",
    "                max_indx = raw_output_1[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "            elif j == 2:\n",
    "                max_prob = max(raw_output_2[i])\n",
    "                max_indx = raw_output_2[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "            elif j == 3:\n",
    "                max_prob = max(raw_output_3[i])\n",
    "                max_indx = raw_output_3[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "            elif j == 4:\n",
    "                max_prob = max(raw_output_4[i])\n",
    "                max_indx = raw_output_4[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "            elif j == 5:\n",
    "                max_prob = max(raw_output_5[i])\n",
    "                max_indx = raw_output_5[i].index(max_prob)\n",
    "                if max_indx == 0:\n",
    "                    continue\n",
    "                result[i][j] = max_prob * max_indx\n",
    "# prob_submit['emotion'] = raw_outputs\n",
    "for i in range(len(result)):\n",
    "    lst = result[i]\n",
    "    to_submit.append(','.join([str(i) for i in lst]))\n",
    "    # break\n",
    "prob_submit['emotion'] = to_submit #prob_submit['emotion'].apply(lambda x: ','.join([str(i) for i in x]))\n",
    "prob_submit.head()\n",
    "prob_submit.to_csv('0922_s6905_bert_fusion.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
